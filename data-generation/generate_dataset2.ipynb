{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: sources/tourism/data_sentence.csv -> DataFrame: tourism_data_sentence\n",
      "Loaded: sources/education/data_sentence.csv -> DataFrame: education_data_sentence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_763581/2386564630.py:35: DtypeWarning: Columns (8,32) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: sources/healthcare/data_sentence.csv -> DataFrame: healthcare_data_sentence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_763581/2386564630.py:35: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: sources/ecommerce/data_sentence.csv -> DataFrame: ecommerce_data_sentence\n",
      "Loaded: sources/finance/data_sentence.csv -> DataFrame: finance_data_sentence\n",
      "DataFrame Name: tourism_data_sentence\n",
      "   Trip ID       Destination Start date   End date  Duration           name  \\\n",
      "0        1        London, UK   5/1/2023   5/8/2023       7.0     John Smith   \n",
      "1        2  Phuket, Thailand  6/15/2023  6/20/2023       5.0       Jane Doe   \n",
      "2        3   Bali, Indonesia   7/1/2023   7/8/2023       7.0      David Lee   \n",
      "3        4     New York, USA  8/15/2023  8/29/2023      14.0  Sarah Johnson   \n",
      "4        5      Tokyo, Japan  9/10/2023  9/17/2023       7.0     Kim Nguyen   \n",
      "\n",
      "    age  gender nationality Accommodation type Accommodation cost  \\\n",
      "0  35.0    Male    American              Hotel               1200   \n",
      "1  28.0  Female    Canadian             Resort                800   \n",
      "2  45.0    Male      Korean              Villa               1000   \n",
      "3  29.0  Female     British              Hotel               2000   \n",
      "4  26.0  Female  Vietnamese             Airbnb                700   \n",
      "\n",
      "  Transportation type Transportation cost  \\\n",
      "0              Flight                 600   \n",
      "1              Flight                 500   \n",
      "2              Flight                 700   \n",
      "3              Flight                1000   \n",
      "4               Train                 200   \n",
      "\n",
      "                                            sentence  \\\n",
      "0  destination is london, uk, start date is 5/1/2...   \n",
      "1  destination is phuket, thailand, start date is...   \n",
      "2  destination is bali, indonesia, start date is ...   \n",
      "3  destination is new york, usa, start date is 8/...   \n",
      "4  destination is tokyo, japan, start date is 9/1...   \n",
      "\n",
      "                                   creative_sentence  \n",
      "0  As I sat in my small apartment in New York, Jo...  \n",
      "1  It was June 15th, 2023, when Jane Doe, a 28-ye...  \n",
      "2  As the sun began to set on July 1st, 2023, 45-...  \n",
      "3  Imagine Sarah Johnson, a 29-year-old British w...  \n",
      "4  Kim Nguyen, a 26-year-old Vietnamese woman, ha...  \n",
      "DataFrame Name: education_data_sentence\n",
      "   index  age         workclass  education      marital-status  \\\n",
      "0      1   39         State-gov  Bachelors       Never-married   \n",
      "1      2   50  Self-emp-not-inc  Bachelors  Married-civ-spouse   \n",
      "2      3   38           Private    HS-grad            Divorced   \n",
      "3      4   53           Private       11th  Married-civ-spouse   \n",
      "4      5   28           Private  Bachelors  Married-civ-spouse   \n",
      "\n",
      "          occupation   relationship   race     sex  capital-gain  \\\n",
      "0       Adm-clerical  Not-in-family  White    Male          2174   \n",
      "1    Exec-managerial        Husband  White    Male             0   \n",
      "2  Handlers-cleaners  Not-in-family  White    Male             0   \n",
      "3  Handlers-cleaners        Husband  Black    Male             0   \n",
      "4     Prof-specialty           Wife  Black  Female             0   \n",
      "\n",
      "   capital-loss  hours-per-week native-country salary  \\\n",
      "0             0              40  United-States  <=50K   \n",
      "1             0              13  United-States  <=50K   \n",
      "2             0              40  United-States  <=50K   \n",
      "3             0              40  United-States  <=50K   \n",
      "4             0              40           Cuba  <=50K   \n",
      "\n",
      "                                            sentence  \\\n",
      "0  age is 39, workclass is state-gov, education i...   \n",
      "1  age is 50, workclass is self-emp-not-inc, educ...   \n",
      "2  age is 38, workclass is private, education is ...   \n",
      "3  age is 53, workclass is private, education is ...   \n",
      "4  age is 28, workclass is private, education is ...   \n",
      "\n",
      "                                   creative_sentence  \n",
      "0  Meet John, a 39-year-old man who works as an a...  \n",
      "1  Meet John, a 50-year-old man from the United S...  \n",
      "2  Meet John, a 38-year-old man from the United S...  \n",
      "3  Meet John, a 53-year-old man from the United S...  \n",
      "4  As I sat down to talk to her, I learned that m...  \n",
      "DataFrame Name: healthcare_data_sentence\n",
      "   Index           start_date             end_date drug_type  \\\n",
      "0      0  2138-07-18 00:00:00  2138-07-20 00:00:00      MAIN   \n",
      "1      1  2138-07-18 00:00:00  2138-07-20 00:00:00      BASE   \n",
      "2      2  2138-07-18 00:00:00  2138-07-21 00:00:00      MAIN   \n",
      "3      3  2138-07-18 00:00:00  2138-07-21 00:00:00      BASE   \n",
      "4      4                  NaN                  NaN       NaN   \n",
      "\n",
      "                  drug_name drug_name_poe drug_name_generic formulary_code  \\\n",
      "0         NEO*IV*Gentamicin           NaN               NaN        GENT10I   \n",
      "1  Syringe (Neonatal) *D5W*           NaN               NaN      NEOSYRD5W   \n",
      "2         Ampicillin Sodium           NaN               NaN        AMP500I   \n",
      "3           Send 500mg Vial           NaN               NaN          AMPVL   \n",
      "4                       NaN           NaN               NaN            NaN   \n",
      "\n",
      "      gsn           ndc  ...       religion marital_status ethnicity  \\\n",
      "0  009298  6.332302e+10  ...  NOT SPECIFIED            NaN     ASIAN   \n",
      "1     NaN  0.000000e+00  ...  NOT SPECIFIED            NaN     ASIAN   \n",
      "2  008937  6.332304e+10  ...  NOT SPECIFIED            NaN     ASIAN   \n",
      "3     NaN  0.000000e+00  ...  NOT SPECIFIED            NaN     ASIAN   \n",
      "4     NaN           NaN  ...       BUDDHIST            NaN     ASIAN   \n",
      "\n",
      "  ed_registration_time ed_exit_time diagnosis expired_in_hospital  \\\n",
      "0                  NaN          NaN   NEWBORN                 0.0   \n",
      "1                  NaN          NaN   NEWBORN                 0.0   \n",
      "2                  NaN          NaN   NEWBORN                 0.0   \n",
      "3                  NaN          NaN   NEWBORN                 0.0   \n",
      "4                  NaN          NaN   NEWBORN                 0.0   \n",
      "\n",
      "  has_chartevents_data                                           sentence  \\\n",
      "0                  1.0  start_date is 2138-07-18 00:00:00, end_date is...   \n",
      "1                  1.0  start_date is 2138-07-18 00:00:00, end_date is...   \n",
      "2                  1.0  start_date is 2138-07-18 00:00:00, end_date is...   \n",
      "3                  1.0  start_date is 2138-07-18 00:00:00, end_date is...   \n",
      "4                  1.0  admit_time is 2103-02-02 04:31:00, discharge_t...   \n",
      "\n",
      "                                   creative_sentence  \n",
      "0  It was July 18, 2138, when a newborn's medical...  \n",
      "1  It was the 18th of July, 2138, when a newborn ...  \n",
      "2  It was a sweltering summer in the year 2138, a...  \n",
      "3  It was a hot summer day on July 18, 2138, when...  \n",
      "4  It was a chilly winter morning on February 2nd...  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "DataFrame Name: ecommerce_data_sentence\n",
      "   index    order_status order_date  \\\n",
      "0      1        complete   7/1/2016   \n",
      "1      2        canceled   7/1/2016   \n",
      "2      3        canceled   7/1/2016   \n",
      "3      4        complete   7/1/2016   \n",
      "4      5  order_refunded   7/1/2016   \n",
      "\n",
      "                                         product_sku  product_price  \\\n",
      "0                                  kreations_YI 06-L         1950.0   \n",
      "1  kcc_Buy 2 Frey Air Freshener & Get 1 Kasual Bo...          240.0   \n",
      "2                                 Ego_UP0017-999-MR0         2450.0   \n",
      "3                                     kcc_krone deal          360.0   \n",
      "4                                        BK7010400AG          555.0   \n",
      "\n",
      "   quantity_ordered  order_total   product_category commission_code  \\\n",
      "0               1.0       1950.0    Women's Fashion              \\N   \n",
      "1               1.0        240.0  Beauty & Grooming              \\N   \n",
      "2               1.0       2450.0    Women's Fashion              \\N   \n",
      "3               1.0         60.0  Beauty & Grooming     R-FSD-52352   \n",
      "4               2.0       1110.0            Soghaat              \\N   \n",
      "\n",
      "   order_discount  ... processing_date business_status order_value_display  \\\n",
      "0             0.0  ...        7/1/2016           #REF!              1,950    \n",
      "1             0.0  ...        7/1/2016           Gross                240    \n",
      "2             0.0  ...        7/1/2016           Gross              2,450    \n",
      "3           300.0  ...        7/1/2016             Net                360    \n",
      "4             0.0  ...        7/1/2016           Valid              1,110    \n",
      "\n",
      "  order_year  order_month  customer_since month_year fiscal_year  \\\n",
      "0     2016.0          7.0          2016-7     7-2016        FY17   \n",
      "1     2016.0          7.0          2016-7     7-2016        FY17   \n",
      "2     2016.0          7.0          2016-7     7-2016        FY17   \n",
      "3     2016.0          7.0          2016-7     7-2016        FY17   \n",
      "4     2016.0          7.0          2016-7     7-2016        FY17   \n",
      "\n",
      "                                            sentence  \\\n",
      "0  order_status is complete, order_date is 7/1/20...   \n",
      "1  order_status is canceled, order_date is 7/1/20...   \n",
      "2  order_status is canceled, order_date is 7/1/20...   \n",
      "3  order_status is complete, order_date is 7/1/20...   \n",
      "4  order_status is order_refunded, order_date is ...   \n",
      "\n",
      "                                   creative_sentence  \n",
      "0  It was a sunny July 1st, 2016, when Emma made ...  \n",
      "1  It was July 1st, 2016, when a customer placed ...  \n",
      "2  It was July 1st, 2016, when the order for the ...  \n",
      "3  It was a beautiful day on July 1st, 2016, when...  \n",
      "4  It was July 1st, 2016, when a customer placed ...  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "DataFrame Name: finance_data_sentence\n",
      "  Customer_ID  Age  Location Income_Level  Total_Transactions  \\\n",
      "0   cust_0000   54     Urban          Low                 192   \n",
      "1   cust_0001   67  Suburban         High                 979   \n",
      "2   cust_0002   44     Urban         High                 329   \n",
      "3   cust_0003   30     Rural         High                  71   \n",
      "4   cust_0004   58     Urban       Middle                 878   \n",
      "\n",
      "   Avg_Transaction_Value  Max_Transaction_Value  Min_Transaction_Value  \\\n",
      "0           16736.384023           60216.834510            6525.814861   \n",
      "1           14536.734683           48350.100272            2186.742245   \n",
      "2            7061.372800           32521.157187            2743.406808   \n",
      "3           16426.876453           17827.896720            4360.784994   \n",
      "4           10800.092660           17497.634534            4532.872520   \n",
      "\n",
      "    Total_Spent  Active_Days  Last_Transaction_Days_Ago  \\\n",
      "0  3.213386e+06          140                        209   \n",
      "1  1.423146e+07          229                        240   \n",
      "2  2.323192e+06           73                         21   \n",
      "3  1.166308e+06          299                        285   \n",
      "4  9.482481e+06          236                        329   \n",
      "\n",
      "   Loyalty_Points_Earned  Referral_Count  Cashback_Received  \\\n",
      "0                   2114              25        2224.012140   \n",
      "1                   2960              20        4026.823518   \n",
      "2                   3170               0        1441.011395   \n",
      "3                   4756              35        4365.855580   \n",
      "4                   1992              18        4161.523827   \n",
      "\n",
      "  App_Usage_Frequency Preferred_Payment_Method  Support_Tickets_Raised  \\\n",
      "0             Monthly               Debit Card                       3   \n",
      "1             Monthly                      UPI                      17   \n",
      "2             Monthly               Debit Card                      11   \n",
      "3              Weekly           Wallet Balance                       6   \n",
      "4               Daily                      UPI                      18   \n",
      "\n",
      "   Issue_Resolution_Time  Customer_Satisfaction_Score  \\\n",
      "0              61.568590                            1   \n",
      "1              60.392889                            8   \n",
      "2              45.305579                            4   \n",
      "3              22.030191                            1   \n",
      "4              20.634723                            5   \n",
      "\n",
      "                                            sentence  \n",
      "0  customer_id is cust_0000, age is 54, location ...  \n",
      "1  customer_id is cust_0001, age is 67, location ...  \n",
      "2  customer_id is cust_0002, age is 44, location ...  \n",
      "3  customer_id is cust_0003, age is 30, location ...  \n",
      "4  customer_id is cust_0004, age is 58, location ...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_csv_files(folder_path, skip_folder=None):\n",
    "    \"\"\"\n",
    "    Recursively load all CSV files from the specified folder and its subfolders into Pandas DataFrames,\n",
    "    while skipping a specified folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing CSV files.\n",
    "        skip_folder (str, optional): Name of the folder to skip (relative to folder_path).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are CSV file paths (relative to folder_path)\n",
    "              and values are the corresponding Pandas DataFrames.\n",
    "    \"\"\"\n",
    "    dataframes = {}\n",
    "\n",
    "    # Walk through all directories and files\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        # Skip the specified folder\n",
    "        if skip_folder and skip_folder in dirs:\n",
    "            dirs.remove(skip_folder)  # This prevents os.walk() from descending into the folder\n",
    "\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(\"data_sentence.csv\"):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "\n",
    "                # Create a unique name for the DataFrame (relative path without extension)\n",
    "                rel_path = os.path.relpath(file_path, folder_path)\n",
    "                dataframe_name = os.path.splitext(rel_path)[0].replace(os.sep, \"_\")\n",
    "\n",
    "                try:\n",
    "                    # Read the CSV file into a Pandas DataFrame\n",
    "                    df = pd.read_csv(file_path)\n",
    "\n",
    "                    # Store DataFrame in the dictionary\n",
    "                    dataframes[dataframe_name] = df\n",
    "\n",
    "                    print(f\"Loaded: {file_path} -> DataFrame: {dataframe_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not load: {file_path} -> DataFrame: {dataframe_name}. Error: {e}\")\n",
    "\n",
    "    return dataframes\n",
    "\n",
    "# Specify the folder containing your CSV files\n",
    "folder_path = \"sources\"\n",
    "\n",
    "# Load all CSV files into DataFrames\n",
    "dataframes_dict = load_csv_files(folder_path)\n",
    "\n",
    "# Example: Access a specific DataFrame\n",
    "for name, df in dataframes_dict.items():\n",
    "    print(f\"DataFrame Name: {name}\")\n",
    "    print(df.head(5))  # Display the first few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Name: tourism_data_sentence\n",
      "Columns:\n",
      "  - Trip ID\n",
      "  - Destination\n",
      "  - Start date\n",
      "  - End date\n",
      "  - Duration\n",
      "  - name\n",
      "  - age\n",
      "  - gender\n",
      "  - nationality\n",
      "  - Accommodation type\n",
      "  - Accommodation cost\n",
      "  - Transportation type\n",
      "  - Transportation cost\n",
      "  - sentence\n",
      "  - creative_sentence\n",
      "\n",
      "DataFrame Name: education_data_sentence\n",
      "Columns:\n",
      "  - index\n",
      "  - age\n",
      "  - workclass\n",
      "  - education\n",
      "  - marital-status\n",
      "  - occupation\n",
      "  - relationship\n",
      "  - race\n",
      "  - sex\n",
      "  - capital-gain\n",
      "  - capital-loss\n",
      "  - hours-per-week\n",
      "  - native-country\n",
      "  - salary\n",
      "  - sentence\n",
      "  - creative_sentence\n",
      "\n",
      "DataFrame Name: healthcare_data_sentence\n",
      "Columns:\n",
      "  - Index\n",
      "  - start_date\n",
      "  - end_date\n",
      "  - drug_type\n",
      "  - drug_name\n",
      "  - drug_name_poe\n",
      "  - drug_name_generic\n",
      "  - formulary_code\n",
      "  - gsn\n",
      "  - ndc\n",
      "  - product_strength\n",
      "  - dose_value\n",
      "  - dose_unit\n",
      "  - form_value_dispensed\n",
      "  - form_unit_dispensed\n",
      "  - route\n",
      "  - admit_time\n",
      "  - discharge_time\n",
      "  - admission_type\n",
      "  - admission_location\n",
      "  - discharge_location\n",
      "  - insurance\n",
      "  - language\n",
      "  - religion\n",
      "  - marital_status\n",
      "  - ethnicity\n",
      "  - ed_registration_time\n",
      "  - ed_exit_time\n",
      "  - diagnosis\n",
      "  - expired_in_hospital\n",
      "  - has_chartevents_data\n",
      "  - sentence\n",
      "  - creative_sentence\n",
      "\n",
      "DataFrame Name: ecommerce_data_sentence\n",
      "Columns:\n",
      "  - index\n",
      "  - order_status\n",
      "  - order_date\n",
      "  - product_sku\n",
      "  - product_price\n",
      "  - quantity_ordered\n",
      "  - order_total\n",
      "  - product_category\n",
      "  - commission_code\n",
      "  - order_discount\n",
      "  - payment_type\n",
      "  - processing_date\n",
      "  - business_status\n",
      "  - order_value_display\n",
      "  - order_year\n",
      "  - order_month\n",
      "  - customer_since\n",
      "  - month_year\n",
      "  - fiscal_year\n",
      "  - sentence\n",
      "  - creative_sentence\n",
      "\n",
      "DataFrame Name: finance_data_sentence\n",
      "Columns:\n",
      "  - Customer_ID\n",
      "  - Age\n",
      "  - Location\n",
      "  - Income_Level\n",
      "  - Total_Transactions\n",
      "  - Avg_Transaction_Value\n",
      "  - Max_Transaction_Value\n",
      "  - Min_Transaction_Value\n",
      "  - Total_Spent\n",
      "  - Active_Days\n",
      "  - Last_Transaction_Days_Ago\n",
      "  - Loyalty_Points_Earned\n",
      "  - Referral_Count\n",
      "  - Cashback_Received\n",
      "  - App_Usage_Frequency\n",
      "  - Preferred_Payment_Method\n",
      "  - Support_Tickets_Raised\n",
      "  - Issue_Resolution_Time\n",
      "  - Customer_Satisfaction_Score\n",
      "  - sentence\n"
     ]
    }
   ],
   "source": [
    "for name, df in dataframes_dict.items():\n",
    "    print(f\"\\nDataFrame Name: {name}\")\n",
    "    print(\"Columns:\")\n",
    "    for column in df.columns:\n",
    "        print(f\"  - {column}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - tourism_data_sentence\n",
      "\n",
      " - education_data_sentence\n",
      "\n",
      " - healthcare_data_sentence\n",
      "\n",
      " - ecommerce_data_sentence\n",
      "\n",
      " - finance_data_sentence\n"
     ]
    }
   ],
   "source": [
    "for name, df in dataframes_dict.items():\n",
    "    print(f\"\\n - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_dict['tourism_data_sentence'].drop([\"sentence\",\"creative_sentence\"],axis=1,inplace=True)\n",
    "dataframes_dict['education_data_sentence'].drop([\"sentence\",\"creative_sentence\",\"workclass\",\"relationship\",\"sex\",\"capital-gain\",\"capital-loss\"],axis=1,inplace=True)\n",
    "dataframes_dict['healthcare_data_sentence'].drop([\"sentence\",\"creative_sentence\",\"drug_name_poe\",\"form_value_dispensed\",\"form_unit_dispensed\",\"ed_registration_time\",\"ed_exit_time\",\"has_chartevents_data\",\"expired_in_hospital\"],axis=1,inplace=True)\n",
    "dataframes_dict['ecommerce_data_sentence'].drop([\"sentence\",\"creative_sentence\",\"commission_code\",\"order_discount\",\"order_value_display\",\"fiscal_year\",\"business_status\"],axis=1,inplace=True)\n",
    "dataframes_dict['finance_data_sentence'].drop([\"sentence\",\"Avg_Transaction_Value\",\"Max_Transaction_Value\",\"Min_Transaction_Value\",\"Last_Transaction_Days_Ago\",\"Referral_Count\",\"App_Usage_Frequency\",\"Support_Tickets_Raised\",\"Issue_Resolution_Time\"],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trip ID</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Start date</th>\n",
       "      <th>End date</th>\n",
       "      <th>Duration</th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>nationality</th>\n",
       "      <th>Accommodation type</th>\n",
       "      <th>Accommodation cost</th>\n",
       "      <th>Transportation type</th>\n",
       "      <th>Transportation cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>London, UK</td>\n",
       "      <td>5/1/2023</td>\n",
       "      <td>5/8/2023</td>\n",
       "      <td>7.0</td>\n",
       "      <td>John Smith</td>\n",
       "      <td>35.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>American</td>\n",
       "      <td>Hotel</td>\n",
       "      <td>1200</td>\n",
       "      <td>Flight</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Phuket, Thailand</td>\n",
       "      <td>6/15/2023</td>\n",
       "      <td>6/20/2023</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Jane Doe</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Canadian</td>\n",
       "      <td>Resort</td>\n",
       "      <td>800</td>\n",
       "      <td>Flight</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Bali, Indonesia</td>\n",
       "      <td>7/1/2023</td>\n",
       "      <td>7/8/2023</td>\n",
       "      <td>7.0</td>\n",
       "      <td>David Lee</td>\n",
       "      <td>45.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Korean</td>\n",
       "      <td>Villa</td>\n",
       "      <td>1000</td>\n",
       "      <td>Flight</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>New York, USA</td>\n",
       "      <td>8/15/2023</td>\n",
       "      <td>8/29/2023</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Sarah Johnson</td>\n",
       "      <td>29.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>British</td>\n",
       "      <td>Hotel</td>\n",
       "      <td>2000</td>\n",
       "      <td>Flight</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Tokyo, Japan</td>\n",
       "      <td>9/10/2023</td>\n",
       "      <td>9/17/2023</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Kim Nguyen</td>\n",
       "      <td>26.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>Airbnb</td>\n",
       "      <td>700</td>\n",
       "      <td>Train</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Trip ID       Destination Start date   End date  Duration           name  \\\n",
       "0        1        London, UK   5/1/2023   5/8/2023       7.0     John Smith   \n",
       "1        2  Phuket, Thailand  6/15/2023  6/20/2023       5.0       Jane Doe   \n",
       "2        3   Bali, Indonesia   7/1/2023   7/8/2023       7.0      David Lee   \n",
       "3        4     New York, USA  8/15/2023  8/29/2023      14.0  Sarah Johnson   \n",
       "4        5      Tokyo, Japan  9/10/2023  9/17/2023       7.0     Kim Nguyen   \n",
       "\n",
       "    age  gender nationality Accommodation type Accommodation cost  \\\n",
       "0  35.0    Male    American              Hotel               1200   \n",
       "1  28.0  Female    Canadian             Resort                800   \n",
       "2  45.0    Male      Korean              Villa               1000   \n",
       "3  29.0  Female     British              Hotel               2000   \n",
       "4  26.0  Female  Vietnamese             Airbnb                700   \n",
       "\n",
       "  Transportation type Transportation cost  \n",
       "0              Flight                 600  \n",
       "1              Flight                 500  \n",
       "2              Flight                 700  \n",
       "3              Flight                1000  \n",
       "4               Train                 200  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dataframes_dict['tourism_data_sentence']))\n",
    "dataframes_dict['tourism_data_sentence'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32561\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>race</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>White</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>White</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>White</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>11th</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Black</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Black</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  age  education      marital-status         occupation   race  \\\n",
       "0      1   39  Bachelors       Never-married       Adm-clerical  White   \n",
       "1      2   50  Bachelors  Married-civ-spouse    Exec-managerial  White   \n",
       "2      3   38    HS-grad            Divorced  Handlers-cleaners  White   \n",
       "3      4   53       11th  Married-civ-spouse  Handlers-cleaners  Black   \n",
       "4      5   28  Bachelors  Married-civ-spouse     Prof-specialty  Black   \n",
       "\n",
       "   hours-per-week native-country salary  \n",
       "0              40  United-States  <=50K  \n",
       "1              13  United-States  <=50K  \n",
       "2              40  United-States  <=50K  \n",
       "3              40  United-States  <=50K  \n",
       "4              40           Cuba  <=50K  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dataframes_dict['education_data_sentence']))\n",
    "dataframes_dict['education_data_sentence'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1249121\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>drug_type</th>\n",
       "      <th>drug_name</th>\n",
       "      <th>drug_name_generic</th>\n",
       "      <th>formulary_code</th>\n",
       "      <th>gsn</th>\n",
       "      <th>ndc</th>\n",
       "      <th>product_strength</th>\n",
       "      <th>...</th>\n",
       "      <th>discharge_time</th>\n",
       "      <th>admission_type</th>\n",
       "      <th>admission_location</th>\n",
       "      <th>discharge_location</th>\n",
       "      <th>insurance</th>\n",
       "      <th>language</th>\n",
       "      <th>religion</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2138-07-18 00:00:00</td>\n",
       "      <td>2138-07-20 00:00:00</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>NEO*IV*Gentamicin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GENT10I</td>\n",
       "      <td>009298</td>\n",
       "      <td>6.332302e+10</td>\n",
       "      <td>10mg/mL-2mL</td>\n",
       "      <td>...</td>\n",
       "      <td>2138-07-21 15:48:00</td>\n",
       "      <td>NEWBORN</td>\n",
       "      <td>PHYS REFERRAL/NORMAL DELI</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Private</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOT SPECIFIED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASIAN</td>\n",
       "      <td>NEWBORN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2138-07-18 00:00:00</td>\n",
       "      <td>2138-07-20 00:00:00</td>\n",
       "      <td>BASE</td>\n",
       "      <td>Syringe (Neonatal) *D5W*</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEOSYRD5W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1 Syringe</td>\n",
       "      <td>...</td>\n",
       "      <td>2138-07-21 15:48:00</td>\n",
       "      <td>NEWBORN</td>\n",
       "      <td>PHYS REFERRAL/NORMAL DELI</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Private</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOT SPECIFIED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASIAN</td>\n",
       "      <td>NEWBORN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2138-07-18 00:00:00</td>\n",
       "      <td>2138-07-21 00:00:00</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>Ampicillin Sodium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AMP500I</td>\n",
       "      <td>008937</td>\n",
       "      <td>6.332304e+10</td>\n",
       "      <td>500mg Vial</td>\n",
       "      <td>...</td>\n",
       "      <td>2138-07-21 15:48:00</td>\n",
       "      <td>NEWBORN</td>\n",
       "      <td>PHYS REFERRAL/NORMAL DELI</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Private</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOT SPECIFIED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASIAN</td>\n",
       "      <td>NEWBORN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2138-07-18 00:00:00</td>\n",
       "      <td>2138-07-21 00:00:00</td>\n",
       "      <td>BASE</td>\n",
       "      <td>Send 500mg Vial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AMPVL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>Send 500mg Vial</td>\n",
       "      <td>...</td>\n",
       "      <td>2138-07-21 15:48:00</td>\n",
       "      <td>NEWBORN</td>\n",
       "      <td>PHYS REFERRAL/NORMAL DELI</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Private</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOT SPECIFIED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASIAN</td>\n",
       "      <td>NEWBORN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2103-02-04 12:15:00</td>\n",
       "      <td>NEWBORN</td>\n",
       "      <td>PHYS REFERRAL/NORMAL DELI</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Private</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BUDDHIST</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASIAN</td>\n",
       "      <td>NEWBORN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index           start_date             end_date drug_type  \\\n",
       "0      0  2138-07-18 00:00:00  2138-07-20 00:00:00      MAIN   \n",
       "1      1  2138-07-18 00:00:00  2138-07-20 00:00:00      BASE   \n",
       "2      2  2138-07-18 00:00:00  2138-07-21 00:00:00      MAIN   \n",
       "3      3  2138-07-18 00:00:00  2138-07-21 00:00:00      BASE   \n",
       "4      4                  NaN                  NaN       NaN   \n",
       "\n",
       "                  drug_name drug_name_generic formulary_code     gsn  \\\n",
       "0         NEO*IV*Gentamicin               NaN        GENT10I  009298   \n",
       "1  Syringe (Neonatal) *D5W*               NaN      NEOSYRD5W     NaN   \n",
       "2         Ampicillin Sodium               NaN        AMP500I  008937   \n",
       "3           Send 500mg Vial               NaN          AMPVL     NaN   \n",
       "4                       NaN               NaN            NaN     NaN   \n",
       "\n",
       "            ndc product_strength  ...       discharge_time admission_type  \\\n",
       "0  6.332302e+10      10mg/mL-2mL  ...  2138-07-21 15:48:00        NEWBORN   \n",
       "1  0.000000e+00        1 Syringe  ...  2138-07-21 15:48:00        NEWBORN   \n",
       "2  6.332304e+10       500mg Vial  ...  2138-07-21 15:48:00        NEWBORN   \n",
       "3  0.000000e+00  Send 500mg Vial  ...  2138-07-21 15:48:00        NEWBORN   \n",
       "4           NaN              NaN  ...  2103-02-04 12:15:00        NEWBORN   \n",
       "\n",
       "          admission_location discharge_location insurance language  \\\n",
       "0  PHYS REFERRAL/NORMAL DELI               HOME   Private      NaN   \n",
       "1  PHYS REFERRAL/NORMAL DELI               HOME   Private      NaN   \n",
       "2  PHYS REFERRAL/NORMAL DELI               HOME   Private      NaN   \n",
       "3  PHYS REFERRAL/NORMAL DELI               HOME   Private      NaN   \n",
       "4  PHYS REFERRAL/NORMAL DELI               HOME   Private      NaN   \n",
       "\n",
       "        religion marital_status ethnicity diagnosis  \n",
       "0  NOT SPECIFIED            NaN     ASIAN   NEWBORN  \n",
       "1  NOT SPECIFIED            NaN     ASIAN   NEWBORN  \n",
       "2  NOT SPECIFIED            NaN     ASIAN   NEWBORN  \n",
       "3  NOT SPECIFIED            NaN     ASIAN   NEWBORN  \n",
       "4       BUDDHIST            NaN     ASIAN   NEWBORN  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dataframes_dict['healthcare_data_sentence']))\n",
    "dataframes_dict['healthcare_data_sentence'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447326\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>order_status</th>\n",
       "      <th>order_date</th>\n",
       "      <th>product_sku</th>\n",
       "      <th>product_price</th>\n",
       "      <th>quantity_ordered</th>\n",
       "      <th>order_total</th>\n",
       "      <th>product_category</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>processing_date</th>\n",
       "      <th>order_year</th>\n",
       "      <th>order_month</th>\n",
       "      <th>customer_since</th>\n",
       "      <th>month_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>complete</td>\n",
       "      <td>7/1/2016</td>\n",
       "      <td>kreations_YI 06-L</td>\n",
       "      <td>1950.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1950.0</td>\n",
       "      <td>Women's Fashion</td>\n",
       "      <td>cod</td>\n",
       "      <td>7/1/2016</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2016-7</td>\n",
       "      <td>7-2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>canceled</td>\n",
       "      <td>7/1/2016</td>\n",
       "      <td>kcc_Buy 2 Frey Air Freshener &amp; Get 1 Kasual Bo...</td>\n",
       "      <td>240.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>Beauty &amp; Grooming</td>\n",
       "      <td>cod</td>\n",
       "      <td>7/1/2016</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2016-7</td>\n",
       "      <td>7-2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>canceled</td>\n",
       "      <td>7/1/2016</td>\n",
       "      <td>Ego_UP0017-999-MR0</td>\n",
       "      <td>2450.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2450.0</td>\n",
       "      <td>Women's Fashion</td>\n",
       "      <td>cod</td>\n",
       "      <td>7/1/2016</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2016-7</td>\n",
       "      <td>7-2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>complete</td>\n",
       "      <td>7/1/2016</td>\n",
       "      <td>kcc_krone deal</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>Beauty &amp; Grooming</td>\n",
       "      <td>cod</td>\n",
       "      <td>7/1/2016</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2016-7</td>\n",
       "      <td>7-2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>order_refunded</td>\n",
       "      <td>7/1/2016</td>\n",
       "      <td>BK7010400AG</td>\n",
       "      <td>555.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>Soghaat</td>\n",
       "      <td>cod</td>\n",
       "      <td>7/1/2016</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2016-7</td>\n",
       "      <td>7-2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    order_status order_date  \\\n",
       "0      1        complete   7/1/2016   \n",
       "1      2        canceled   7/1/2016   \n",
       "2      3        canceled   7/1/2016   \n",
       "3      4        complete   7/1/2016   \n",
       "4      5  order_refunded   7/1/2016   \n",
       "\n",
       "                                         product_sku  product_price  \\\n",
       "0                                  kreations_YI 06-L         1950.0   \n",
       "1  kcc_Buy 2 Frey Air Freshener & Get 1 Kasual Bo...          240.0   \n",
       "2                                 Ego_UP0017-999-MR0         2450.0   \n",
       "3                                     kcc_krone deal          360.0   \n",
       "4                                        BK7010400AG          555.0   \n",
       "\n",
       "   quantity_ordered  order_total   product_category payment_type  \\\n",
       "0               1.0       1950.0    Women's Fashion          cod   \n",
       "1               1.0        240.0  Beauty & Grooming          cod   \n",
       "2               1.0       2450.0    Women's Fashion          cod   \n",
       "3               1.0         60.0  Beauty & Grooming          cod   \n",
       "4               2.0       1110.0            Soghaat          cod   \n",
       "\n",
       "  processing_date  order_year  order_month customer_since month_year  \n",
       "0        7/1/2016      2016.0          7.0         2016-7     7-2016  \n",
       "1        7/1/2016      2016.0          7.0         2016-7     7-2016  \n",
       "2        7/1/2016      2016.0          7.0         2016-7     7-2016  \n",
       "3        7/1/2016      2016.0          7.0         2016-7     7-2016  \n",
       "4        7/1/2016      2016.0          7.0         2016-7     7-2016  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dataframes_dict['ecommerce_data_sentence']))\n",
    "dataframes_dict['ecommerce_data_sentence'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer_ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Location</th>\n",
       "      <th>Income_Level</th>\n",
       "      <th>Total_Transactions</th>\n",
       "      <th>Total_Spent</th>\n",
       "      <th>Active_Days</th>\n",
       "      <th>Loyalty_Points_Earned</th>\n",
       "      <th>Cashback_Received</th>\n",
       "      <th>Preferred_Payment_Method</th>\n",
       "      <th>Customer_Satisfaction_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cust_0000</td>\n",
       "      <td>54</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Low</td>\n",
       "      <td>192</td>\n",
       "      <td>3.213386e+06</td>\n",
       "      <td>140</td>\n",
       "      <td>2114</td>\n",
       "      <td>2224.012140</td>\n",
       "      <td>Debit Card</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cust_0001</td>\n",
       "      <td>67</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>High</td>\n",
       "      <td>979</td>\n",
       "      <td>1.423146e+07</td>\n",
       "      <td>229</td>\n",
       "      <td>2960</td>\n",
       "      <td>4026.823518</td>\n",
       "      <td>UPI</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cust_0002</td>\n",
       "      <td>44</td>\n",
       "      <td>Urban</td>\n",
       "      <td>High</td>\n",
       "      <td>329</td>\n",
       "      <td>2.323192e+06</td>\n",
       "      <td>73</td>\n",
       "      <td>3170</td>\n",
       "      <td>1441.011395</td>\n",
       "      <td>Debit Card</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cust_0003</td>\n",
       "      <td>30</td>\n",
       "      <td>Rural</td>\n",
       "      <td>High</td>\n",
       "      <td>71</td>\n",
       "      <td>1.166308e+06</td>\n",
       "      <td>299</td>\n",
       "      <td>4756</td>\n",
       "      <td>4365.855580</td>\n",
       "      <td>Wallet Balance</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cust_0004</td>\n",
       "      <td>58</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Middle</td>\n",
       "      <td>878</td>\n",
       "      <td>9.482481e+06</td>\n",
       "      <td>236</td>\n",
       "      <td>1992</td>\n",
       "      <td>4161.523827</td>\n",
       "      <td>UPI</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Customer_ID  Age  Location Income_Level  Total_Transactions   Total_Spent  \\\n",
       "0   cust_0000   54     Urban          Low                 192  3.213386e+06   \n",
       "1   cust_0001   67  Suburban         High                 979  1.423146e+07   \n",
       "2   cust_0002   44     Urban         High                 329  2.323192e+06   \n",
       "3   cust_0003   30     Rural         High                  71  1.166308e+06   \n",
       "4   cust_0004   58     Urban       Middle                 878  9.482481e+06   \n",
       "\n",
       "   Active_Days  Loyalty_Points_Earned  Cashback_Received  \\\n",
       "0          140                   2114        2224.012140   \n",
       "1          229                   2960        4026.823518   \n",
       "2           73                   3170        1441.011395   \n",
       "3          299                   4756        4365.855580   \n",
       "4          236                   1992        4161.523827   \n",
       "\n",
       "  Preferred_Payment_Method  Customer_Satisfaction_Score  \n",
       "0               Debit Card                            1  \n",
       "1                      UPI                            8  \n",
       "2               Debit Card                            4  \n",
       "3           Wallet Balance                            1  \n",
       "4                      UPI                            5  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dataframes_dict['finance_data_sentence']))\n",
    "dataframes_dict['finance_data_sentence'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_763581/493792340.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected.drop([\"Index\"],axis=1,inplace=True)\n",
      "/tmp/ipykernel_763581/493792340.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['index'] = range(1, len(df_selected) + 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>drug_type</th>\n",
       "      <th>drug_name</th>\n",
       "      <th>drug_name_generic</th>\n",
       "      <th>formulary_code</th>\n",
       "      <th>gsn</th>\n",
       "      <th>ndc</th>\n",
       "      <th>product_strength</th>\n",
       "      <th>...</th>\n",
       "      <th>discharge_time</th>\n",
       "      <th>admission_type</th>\n",
       "      <th>admission_location</th>\n",
       "      <th>discharge_location</th>\n",
       "      <th>insurance</th>\n",
       "      <th>language</th>\n",
       "      <th>religion</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>1</td>\n",
       "      <td>2157-10-21 00:00:00</td>\n",
       "      <td>2157-10-23 00:00:00</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>Dexamethasone</td>\n",
       "      <td>Dexamethasone</td>\n",
       "      <td>DEXA4</td>\n",
       "      <td>006789</td>\n",
       "      <td>5.481752e+07</td>\n",
       "      <td>4mg Tab</td>\n",
       "      <td>...</td>\n",
       "      <td>2157-10-25 14:00:00</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>TRANSFER FROM HOSP/EXTRAM</td>\n",
       "      <td>HOME HEALTH CARE</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>BRAIN MASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>2</td>\n",
       "      <td>2157-10-21 00:00:00</td>\n",
       "      <td>2157-10-25 00:00:00</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>HydrALAzine</td>\n",
       "      <td>HydrALAzine</td>\n",
       "      <td>HYDZ20I</td>\n",
       "      <td>000283</td>\n",
       "      <td>5.170901e+08</td>\n",
       "      <td>20mg/mL Vial</td>\n",
       "      <td>...</td>\n",
       "      <td>2157-10-25 14:00:00</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>TRANSFER FROM HOSP/EXTRAM</td>\n",
       "      <td>HOME HEALTH CARE</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>BRAIN MASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>3</td>\n",
       "      <td>2157-10-22 00:00:00</td>\n",
       "      <td>2157-10-22 00:00:00</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>Heparin</td>\n",
       "      <td>Heparin Sodium</td>\n",
       "      <td>HEPA5I</td>\n",
       "      <td>006549</td>\n",
       "      <td>6.332303e+10</td>\n",
       "      <td>5000 Units / mL- 1mL Vial</td>\n",
       "      <td>...</td>\n",
       "      <td>2157-10-25 14:00:00</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>TRANSFER FROM HOSP/EXTRAM</td>\n",
       "      <td>HOME HEALTH CARE</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>BRAIN MASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>4</td>\n",
       "      <td>2157-10-23 00:00:00</td>\n",
       "      <td>2157-10-25 00:00:00</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>Dexamethasone</td>\n",
       "      <td>Dexamethasone</td>\n",
       "      <td>DEXA2</td>\n",
       "      <td>006788</td>\n",
       "      <td>5.481762e+07</td>\n",
       "      <td>2mg Tab</td>\n",
       "      <td>...</td>\n",
       "      <td>2157-10-25 14:00:00</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>TRANSFER FROM HOSP/EXTRAM</td>\n",
       "      <td>HOME HEALTH CARE</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>BRAIN MASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>5</td>\n",
       "      <td>2157-10-25 00:00:00</td>\n",
       "      <td>2157-10-24 00:00:00</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>Dexamethasone</td>\n",
       "      <td>Dexamethasone</td>\n",
       "      <td>DEXA2</td>\n",
       "      <td>006788</td>\n",
       "      <td>5.481762e+07</td>\n",
       "      <td>2mg Tab</td>\n",
       "      <td>...</td>\n",
       "      <td>2157-10-25 14:00:00</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>TRANSFER FROM HOSP/EXTRAM</td>\n",
       "      <td>HOME HEALTH CARE</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>BRAIN MASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7016</th>\n",
       "      <td>116</td>\n",
       "      <td>2153-09-26 00:00:00</td>\n",
       "      <td>2153-09-28 00:00:00</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>Morphine Sulfate</td>\n",
       "      <td>Morphine Sulfate (Syringe)</td>\n",
       "      <td>MORP2I</td>\n",
       "      <td>004070</td>\n",
       "      <td>4.091762e+08</td>\n",
       "      <td>2mg Syringe</td>\n",
       "      <td>...</td>\n",
       "      <td>2153-09-28 18:48:00</td>\n",
       "      <td>ELECTIVE</td>\n",
       "      <td>PHYS REFERRAL/NORMAL DELI</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Private</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>UNKNOWN/NOT SPECIFIED</td>\n",
       "      <td>HYPERTROPHIC CARDIOMYOPATHY\\ETHANOL SEPTAL ABL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7017</th>\n",
       "      <td>117</td>\n",
       "      <td>2153-09-26 00:00:00</td>\n",
       "      <td>2153-09-28 00:00:00</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>Metoprolol XL</td>\n",
       "      <td>Metoprolol XL</td>\n",
       "      <td>TOPR100</td>\n",
       "      <td>016600</td>\n",
       "      <td>1.861092e+08</td>\n",
       "      <td>100mg XL Tab</td>\n",
       "      <td>...</td>\n",
       "      <td>2153-09-28 18:48:00</td>\n",
       "      <td>ELECTIVE</td>\n",
       "      <td>PHYS REFERRAL/NORMAL DELI</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Private</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>UNKNOWN/NOT SPECIFIED</td>\n",
       "      <td>HYPERTROPHIC CARDIOMYOPATHY\\ETHANOL SEPTAL ABL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7018</th>\n",
       "      <td>118</td>\n",
       "      <td>2153-09-26 00:00:00</td>\n",
       "      <td>2153-09-28 00:00:00</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>Oxycodone-Acetaminophen</td>\n",
       "      <td>Oxycodone-Acetaminophen</td>\n",
       "      <td>PERC</td>\n",
       "      <td>004222</td>\n",
       "      <td>4.060513e+08</td>\n",
       "      <td>5mg/325mg Tablet</td>\n",
       "      <td>...</td>\n",
       "      <td>2153-09-28 18:48:00</td>\n",
       "      <td>ELECTIVE</td>\n",
       "      <td>PHYS REFERRAL/NORMAL DELI</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Private</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>UNKNOWN/NOT SPECIFIED</td>\n",
       "      <td>HYPERTROPHIC CARDIOMYOPATHY\\ETHANOL SEPTAL ABL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7019</th>\n",
       "      <td>119</td>\n",
       "      <td>2153-09-26 00:00:00</td>\n",
       "      <td>2153-09-28 00:00:00</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>Verapamil HCl</td>\n",
       "      <td>Verapamil HCl</td>\n",
       "      <td>VERA40</td>\n",
       "      <td>000565</td>\n",
       "      <td>5.910404e+08</td>\n",
       "      <td>40mg Tab</td>\n",
       "      <td>...</td>\n",
       "      <td>2153-09-28 18:48:00</td>\n",
       "      <td>ELECTIVE</td>\n",
       "      <td>PHYS REFERRAL/NORMAL DELI</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Private</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>UNKNOWN/NOT SPECIFIED</td>\n",
       "      <td>HYPERTROPHIC CARDIOMYOPATHY\\ETHANOL SEPTAL ABL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7020</th>\n",
       "      <td>120</td>\n",
       "      <td>2153-09-27 00:00:00</td>\n",
       "      <td>2153-09-27 00:00:00</td>\n",
       "      <td>MAIN</td>\n",
       "      <td>Magnesium Oxide</td>\n",
       "      <td>Magnesium Oxide</td>\n",
       "      <td>MAGN400</td>\n",
       "      <td>001408</td>\n",
       "      <td>1.650022e+08</td>\n",
       "      <td>400 mg Tab</td>\n",
       "      <td>...</td>\n",
       "      <td>2153-09-28 18:48:00</td>\n",
       "      <td>ELECTIVE</td>\n",
       "      <td>PHYS REFERRAL/NORMAL DELI</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Private</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>UNKNOWN/NOT SPECIFIED</td>\n",
       "      <td>HYPERTROPHIC CARDIOMYOPATHY\\ETHANOL SEPTAL ABL...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index           start_date             end_date drug_type  \\\n",
       "340       1  2157-10-21 00:00:00  2157-10-23 00:00:00      MAIN   \n",
       "350       2  2157-10-21 00:00:00  2157-10-25 00:00:00      MAIN   \n",
       "352       3  2157-10-22 00:00:00  2157-10-22 00:00:00      MAIN   \n",
       "360       4  2157-10-23 00:00:00  2157-10-25 00:00:00      MAIN   \n",
       "362       5  2157-10-25 00:00:00  2157-10-24 00:00:00      MAIN   \n",
       "...     ...                  ...                  ...       ...   \n",
       "7016    116  2153-09-26 00:00:00  2153-09-28 00:00:00      MAIN   \n",
       "7017    117  2153-09-26 00:00:00  2153-09-28 00:00:00      MAIN   \n",
       "7018    118  2153-09-26 00:00:00  2153-09-28 00:00:00      MAIN   \n",
       "7019    119  2153-09-26 00:00:00  2153-09-28 00:00:00      MAIN   \n",
       "7020    120  2153-09-27 00:00:00  2153-09-27 00:00:00      MAIN   \n",
       "\n",
       "                    drug_name           drug_name_generic formulary_code  \\\n",
       "340             Dexamethasone               Dexamethasone          DEXA4   \n",
       "350               HydrALAzine                 HydrALAzine        HYDZ20I   \n",
       "352                   Heparin              Heparin Sodium         HEPA5I   \n",
       "360             Dexamethasone               Dexamethasone          DEXA2   \n",
       "362             Dexamethasone               Dexamethasone          DEXA2   \n",
       "...                       ...                         ...            ...   \n",
       "7016         Morphine Sulfate  Morphine Sulfate (Syringe)         MORP2I   \n",
       "7017            Metoprolol XL               Metoprolol XL        TOPR100   \n",
       "7018  Oxycodone-Acetaminophen     Oxycodone-Acetaminophen           PERC   \n",
       "7019            Verapamil HCl               Verapamil HCl         VERA40   \n",
       "7020          Magnesium Oxide             Magnesium Oxide        MAGN400   \n",
       "\n",
       "         gsn           ndc           product_strength  ...  \\\n",
       "340   006789  5.481752e+07                    4mg Tab  ...   \n",
       "350   000283  5.170901e+08               20mg/mL Vial  ...   \n",
       "352   006549  6.332303e+10  5000 Units / mL- 1mL Vial  ...   \n",
       "360   006788  5.481762e+07                    2mg Tab  ...   \n",
       "362   006788  5.481762e+07                    2mg Tab  ...   \n",
       "...      ...           ...                        ...  ...   \n",
       "7016  004070  4.091762e+08                2mg Syringe  ...   \n",
       "7017  016600  1.861092e+08               100mg XL Tab  ...   \n",
       "7018  004222  4.060513e+08           5mg/325mg Tablet  ...   \n",
       "7019  000565  5.910404e+08                   40mg Tab  ...   \n",
       "7020  001408  1.650022e+08                 400 mg Tab  ...   \n",
       "\n",
       "           discharge_time admission_type         admission_location  \\\n",
       "340   2157-10-25 14:00:00      EMERGENCY  TRANSFER FROM HOSP/EXTRAM   \n",
       "350   2157-10-25 14:00:00      EMERGENCY  TRANSFER FROM HOSP/EXTRAM   \n",
       "352   2157-10-25 14:00:00      EMERGENCY  TRANSFER FROM HOSP/EXTRAM   \n",
       "360   2157-10-25 14:00:00      EMERGENCY  TRANSFER FROM HOSP/EXTRAM   \n",
       "362   2157-10-25 14:00:00      EMERGENCY  TRANSFER FROM HOSP/EXTRAM   \n",
       "...                   ...            ...                        ...   \n",
       "7016  2153-09-28 18:48:00       ELECTIVE  PHYS REFERRAL/NORMAL DELI   \n",
       "7017  2153-09-28 18:48:00       ELECTIVE  PHYS REFERRAL/NORMAL DELI   \n",
       "7018  2153-09-28 18:48:00       ELECTIVE  PHYS REFERRAL/NORMAL DELI   \n",
       "7019  2153-09-28 18:48:00       ELECTIVE  PHYS REFERRAL/NORMAL DELI   \n",
       "7020  2153-09-28 18:48:00       ELECTIVE  PHYS REFERRAL/NORMAL DELI   \n",
       "\n",
       "     discharge_location insurance language  religion marital_status  \\\n",
       "340    HOME HEALTH CARE  Medicare     ENGL  CATHOLIC        MARRIED   \n",
       "350    HOME HEALTH CARE  Medicare     ENGL  CATHOLIC        MARRIED   \n",
       "352    HOME HEALTH CARE  Medicare     ENGL  CATHOLIC        MARRIED   \n",
       "360    HOME HEALTH CARE  Medicare     ENGL  CATHOLIC        MARRIED   \n",
       "362    HOME HEALTH CARE  Medicare     ENGL  CATHOLIC        MARRIED   \n",
       "...                 ...       ...      ...       ...            ...   \n",
       "7016               HOME   Private     ENGL  CATHOLIC         SINGLE   \n",
       "7017               HOME   Private     ENGL  CATHOLIC         SINGLE   \n",
       "7018               HOME   Private     ENGL  CATHOLIC         SINGLE   \n",
       "7019               HOME   Private     ENGL  CATHOLIC         SINGLE   \n",
       "7020               HOME   Private     ENGL  CATHOLIC         SINGLE   \n",
       "\n",
       "                  ethnicity                                          diagnosis  \n",
       "340                   WHITE                                         BRAIN MASS  \n",
       "350                   WHITE                                         BRAIN MASS  \n",
       "352                   WHITE                                         BRAIN MASS  \n",
       "360                   WHITE                                         BRAIN MASS  \n",
       "362                   WHITE                                         BRAIN MASS  \n",
       "...                     ...                                                ...  \n",
       "7016  UNKNOWN/NOT SPECIFIED  HYPERTROPHIC CARDIOMYOPATHY\\ETHANOL SEPTAL ABL...  \n",
       "7017  UNKNOWN/NOT SPECIFIED  HYPERTROPHIC CARDIOMYOPATHY\\ETHANOL SEPTAL ABL...  \n",
       "7018  UNKNOWN/NOT SPECIFIED  HYPERTROPHIC CARDIOMYOPATHY\\ETHANOL SEPTAL ABL...  \n",
       "7019  UNKNOWN/NOT SPECIFIED  HYPERTROPHIC CARDIOMYOPATHY\\ETHANOL SEPTAL ABL...  \n",
       "7020  UNKNOWN/NOT SPECIFIED  HYPERTROPHIC CARDIOMYOPATHY\\ETHANOL SEPTAL ABL...  \n",
       "\n",
       "[120 rows x 24 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = dataframes_dict['healthcare_data_sentence'].dropna()\n",
    "\n",
    "# Step 2: Select the first 120 rows from the cleaned DataFrame\n",
    "df_selected = df_clean.head(120)\n",
    "df_selected.drop([\"Index\"],axis=1,inplace=True)\n",
    "df_selected['index'] = range(1, len(df_selected) + 1)\n",
    "dataframes_dict['healthcare_data_sentence'] = df_selected\n",
    "# Move 'index' column to the beginning\n",
    "dataframes_dict['healthcare_data_sentence'] = dataframes_dict['healthcare_data_sentence'][['index'] + [col for col in dataframes_dict['healthcare_data_sentence'].columns if col != 'index']]\n",
    "\n",
    "dataframes_dict['healthcare_data_sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_763581/3225794751.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected.drop([\"Trip ID\"],axis=1,inplace=True)\n",
      "/tmp/ipykernel_763581/3225794751.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['index'] = range(1, len(df_selected) + 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Start date</th>\n",
       "      <th>End date</th>\n",
       "      <th>Duration</th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>nationality</th>\n",
       "      <th>Accommodation type</th>\n",
       "      <th>Accommodation cost</th>\n",
       "      <th>Transportation type</th>\n",
       "      <th>Transportation cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>London, UK</td>\n",
       "      <td>5/1/2023</td>\n",
       "      <td>5/8/2023</td>\n",
       "      <td>7.0</td>\n",
       "      <td>John Smith</td>\n",
       "      <td>35.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>American</td>\n",
       "      <td>Hotel</td>\n",
       "      <td>1200</td>\n",
       "      <td>Flight</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Phuket, Thailand</td>\n",
       "      <td>6/15/2023</td>\n",
       "      <td>6/20/2023</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Jane Doe</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Canadian</td>\n",
       "      <td>Resort</td>\n",
       "      <td>800</td>\n",
       "      <td>Flight</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Bali, Indonesia</td>\n",
       "      <td>7/1/2023</td>\n",
       "      <td>7/8/2023</td>\n",
       "      <td>7.0</td>\n",
       "      <td>David Lee</td>\n",
       "      <td>45.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Korean</td>\n",
       "      <td>Villa</td>\n",
       "      <td>1000</td>\n",
       "      <td>Flight</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>New York, USA</td>\n",
       "      <td>8/15/2023</td>\n",
       "      <td>8/29/2023</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Sarah Johnson</td>\n",
       "      <td>29.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>British</td>\n",
       "      <td>Hotel</td>\n",
       "      <td>2000</td>\n",
       "      <td>Flight</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Tokyo, Japan</td>\n",
       "      <td>9/10/2023</td>\n",
       "      <td>9/17/2023</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Kim Nguyen</td>\n",
       "      <td>26.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>Airbnb</td>\n",
       "      <td>700</td>\n",
       "      <td>Train</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>116</td>\n",
       "      <td>Paris, France</td>\n",
       "      <td>3/15/2022</td>\n",
       "      <td>3/22/2022</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Bob Johnson</td>\n",
       "      <td>47.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Canadian</td>\n",
       "      <td>Hotel</td>\n",
       "      <td>1200</td>\n",
       "      <td>Train</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>117</td>\n",
       "      <td>Sydney, Aus</td>\n",
       "      <td>5/1/2022</td>\n",
       "      <td>5/12/2022</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Cindy Chen</td>\n",
       "      <td>26.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Airbnb</td>\n",
       "      <td>800</td>\n",
       "      <td>Plane</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>118</td>\n",
       "      <td>Rome, Italy</td>\n",
       "      <td>6/10/2022</td>\n",
       "      <td>6/17/2022</td>\n",
       "      <td>7.0</td>\n",
       "      <td>David Lee</td>\n",
       "      <td>38.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Korean</td>\n",
       "      <td>Hotel</td>\n",
       "      <td>900</td>\n",
       "      <td>Train</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>119</td>\n",
       "      <td>Bali, Indonesia</td>\n",
       "      <td>7/20/2022</td>\n",
       "      <td>7/30/2022</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Emily Kim</td>\n",
       "      <td>29.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Korean</td>\n",
       "      <td>Hostel</td>\n",
       "      <td>500</td>\n",
       "      <td>Plane</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>120</td>\n",
       "      <td>Cancun, Mexico</td>\n",
       "      <td>8/8/2022</td>\n",
       "      <td>8/16/2022</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Frank Li</td>\n",
       "      <td>41.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>American</td>\n",
       "      <td>Hotel</td>\n",
       "      <td>1300</td>\n",
       "      <td>Plane</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index       Destination Start date   End date  Duration           name  \\\n",
       "0        1        London, UK   5/1/2023   5/8/2023       7.0     John Smith   \n",
       "1        2  Phuket, Thailand  6/15/2023  6/20/2023       5.0       Jane Doe   \n",
       "2        3   Bali, Indonesia   7/1/2023   7/8/2023       7.0      David Lee   \n",
       "3        4     New York, USA  8/15/2023  8/29/2023      14.0  Sarah Johnson   \n",
       "4        5      Tokyo, Japan  9/10/2023  9/17/2023       7.0     Kim Nguyen   \n",
       "..     ...               ...        ...        ...       ...            ...   \n",
       "115    116     Paris, France  3/15/2022  3/22/2022       7.0    Bob Johnson   \n",
       "116    117       Sydney, Aus   5/1/2022  5/12/2022      11.0     Cindy Chen   \n",
       "117    118       Rome, Italy  6/10/2022  6/17/2022       7.0      David Lee   \n",
       "118    119   Bali, Indonesia  7/20/2022  7/30/2022      10.0      Emily Kim   \n",
       "119    120    Cancun, Mexico   8/8/2022  8/16/2022       8.0       Frank Li   \n",
       "\n",
       "      age  gender nationality Accommodation type Accommodation cost  \\\n",
       "0    35.0    Male    American              Hotel               1200   \n",
       "1    28.0  Female    Canadian             Resort                800   \n",
       "2    45.0    Male      Korean              Villa               1000   \n",
       "3    29.0  Female     British              Hotel               2000   \n",
       "4    26.0  Female  Vietnamese             Airbnb                700   \n",
       "..    ...     ...         ...                ...                ...   \n",
       "115  47.0    Male    Canadian              Hotel               1200   \n",
       "116  26.0  Female     Chinese             Airbnb                800   \n",
       "117  38.0    Male      Korean              Hotel                900   \n",
       "118  29.0  Female      Korean             Hostel                500   \n",
       "119  41.0    Male    American              Hotel               1300   \n",
       "\n",
       "    Transportation type Transportation cost  \n",
       "0                Flight                 600  \n",
       "1                Flight                 500  \n",
       "2                Flight                 700  \n",
       "3                Flight                1000  \n",
       "4                 Train                 200  \n",
       "..                  ...                 ...  \n",
       "115               Train                 500  \n",
       "116               Plane                1000  \n",
       "117               Train                 400  \n",
       "118               Plane                 800  \n",
       "119               Plane                 600  \n",
       "\n",
       "[120 rows x 13 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = dataframes_dict['tourism_data_sentence'].dropna()\n",
    "\n",
    "# Step 2: Select the first 120 rows from the cleaned DataFrame\n",
    "df_selected = df_clean.head(120)\n",
    "df_selected.drop([\"Trip ID\"],axis=1,inplace=True)\n",
    "df_selected['index'] = range(1, len(df_selected) + 1)\n",
    "dataframes_dict['tourism_data_sentence'] = df_selected\n",
    "# Move 'index' column to the beginning\n",
    "dataframes_dict['tourism_data_sentence'] = dataframes_dict['tourism_data_sentence'][['index'] + [col for col in dataframes_dict['tourism_data_sentence'].columns if col != 'index']]\n",
    "\n",
    "dataframes_dict['tourism_data_sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_763581/3051563475.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected.drop([\"index\"],axis=1,inplace=True)\n",
      "/tmp/ipykernel_763581/3051563475.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['index'] = range(1, len(df_selected) + 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>race</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>White</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>White</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>White</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>11th</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Black</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Black</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>116</td>\n",
       "      <td>53</td>\n",
       "      <td>9th</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>White</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>117</td>\n",
       "      <td>56</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>White</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>118</td>\n",
       "      <td>49</td>\n",
       "      <td>Assoc-voc</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Black</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>119</td>\n",
       "      <td>55</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>White</td>\n",
       "      <td>56</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>120</td>\n",
       "      <td>22</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>White</td>\n",
       "      <td>41</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  age     education      marital-status         occupation   race  \\\n",
       "0        1   39     Bachelors       Never-married       Adm-clerical  White   \n",
       "1        2   50     Bachelors  Married-civ-spouse    Exec-managerial  White   \n",
       "2        3   38       HS-grad            Divorced  Handlers-cleaners  White   \n",
       "3        4   53          11th  Married-civ-spouse  Handlers-cleaners  Black   \n",
       "4        5   28     Bachelors  Married-civ-spouse     Prof-specialty  Black   \n",
       "..     ...  ...           ...                 ...                ...    ...   \n",
       "115    116   53           9th  Married-civ-spouse  Handlers-cleaners  White   \n",
       "116    117   56  Some-college  Married-civ-spouse              Sales  White   \n",
       "117    118   49     Assoc-voc  Married-civ-spouse       Craft-repair  Black   \n",
       "118    119   55  Some-college  Married-civ-spouse              Sales  White   \n",
       "119    120   22       HS-grad       Never-married       Craft-repair  White   \n",
       "\n",
       "     hours-per-week native-country salary  \n",
       "0                40  United-States  <=50K  \n",
       "1                13  United-States  <=50K  \n",
       "2                40  United-States  <=50K  \n",
       "3                40  United-States  <=50K  \n",
       "4                40           Cuba  <=50K  \n",
       "..              ...            ...    ...  \n",
       "115              50  United-States  <=50K  \n",
       "116              50  United-States  <=50K  \n",
       "117              40  United-States   >50K  \n",
       "118              56  United-States  <=50K  \n",
       "119              41  United-States  <=50K  \n",
       "\n",
       "[120 rows x 9 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = dataframes_dict['education_data_sentence'].dropna()\n",
    "\n",
    "# Step 2: Select the first 120 rows from the cleaned DataFrame\n",
    "df_selected = df_clean.head(120)\n",
    "df_selected.drop([\"index\"],axis=1,inplace=True)\n",
    "df_selected['index'] = range(1, len(df_selected) + 1)\n",
    "dataframes_dict['education_data_sentence'] = df_selected\n",
    "# Move 'index' column to the beginning\n",
    "dataframes_dict['education_data_sentence'] = dataframes_dict['education_data_sentence'][['index'] + [col for col in dataframes_dict['education_data_sentence'].columns if col != 'index']]\n",
    "\n",
    "dataframes_dict['education_data_sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_dict['education_data_sentence'].rename(columns={'native-country': 'country'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_763581/2464527115.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected.drop([\"index\"],axis=1,inplace=True)\n",
      "/tmp/ipykernel_763581/2464527115.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['index'] = range(1, len(df_selected) + 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>order_status</th>\n",
       "      <th>order_date</th>\n",
       "      <th>product_sku</th>\n",
       "      <th>product_price</th>\n",
       "      <th>quantity_ordered</th>\n",
       "      <th>order_total</th>\n",
       "      <th>product_category</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>processing_date</th>\n",
       "      <th>order_year</th>\n",
       "      <th>order_month</th>\n",
       "      <th>customer_since</th>\n",
       "      <th>month_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>complete</td>\n",
       "      <td>7/1/2016</td>\n",
       "      <td>kreations_YI 06-L</td>\n",
       "      <td>1950.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1950.0</td>\n",
       "      <td>Women's Fashion</td>\n",
       "      <td>cod</td>\n",
       "      <td>7/1/2016</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2016-7</td>\n",
       "      <td>7-2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>canceled</td>\n",
       "      <td>7/1/2016</td>\n",
       "      <td>kcc_Buy 2 Frey Air Freshener &amp; Get 1 Kasual Bo...</td>\n",
       "      <td>240.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>Beauty &amp; Grooming</td>\n",
       "      <td>cod</td>\n",
       "      <td>7/1/2016</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2016-7</td>\n",
       "      <td>7-2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>canceled</td>\n",
       "      <td>7/1/2016</td>\n",
       "      <td>Ego_UP0017-999-MR0</td>\n",
       "      <td>2450.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2450.0</td>\n",
       "      <td>Women's Fashion</td>\n",
       "      <td>cod</td>\n",
       "      <td>7/1/2016</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2016-7</td>\n",
       "      <td>7-2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>complete</td>\n",
       "      <td>7/1/2016</td>\n",
       "      <td>kcc_krone deal</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>Beauty &amp; Grooming</td>\n",
       "      <td>cod</td>\n",
       "      <td>7/1/2016</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2016-7</td>\n",
       "      <td>7-2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>order_refunded</td>\n",
       "      <td>7/1/2016</td>\n",
       "      <td>BK7010400AG</td>\n",
       "      <td>555.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>Soghaat</td>\n",
       "      <td>cod</td>\n",
       "      <td>7/1/2016</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2016-7</td>\n",
       "      <td>7-2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>116</td>\n",
       "      <td>canceled</td>\n",
       "      <td>7/1/2016</td>\n",
       "      <td>Al Muhafiz Sohan Halwa Almond</td>\n",
       "      <td>350.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>Soghaat</td>\n",
       "      <td>cod</td>\n",
       "      <td>7/1/2016</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2016-7</td>\n",
       "      <td>7-2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>117</td>\n",
       "      <td>canceled</td>\n",
       "      <td>7/1/2016</td>\n",
       "      <td>sst_Lyquin-Regular fit-Large</td>\n",
       "      <td>1950.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2745.0</td>\n",
       "      <td>Men's Fashion</td>\n",
       "      <td>ublcreditcard</td>\n",
       "      <td>7/1/2016</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2016-7</td>\n",
       "      <td>7-2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>118</td>\n",
       "      <td>canceled</td>\n",
       "      <td>7/1/2016</td>\n",
       "      <td>Fcafe_11777-L</td>\n",
       "      <td>795.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2745.0</td>\n",
       "      <td>Men's Fashion</td>\n",
       "      <td>ublcreditcard</td>\n",
       "      <td>7/1/2016</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2016-7</td>\n",
       "      <td>7-2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>119</td>\n",
       "      <td>complete</td>\n",
       "      <td>7/1/2016</td>\n",
       "      <td>LC_359547105042</td>\n",
       "      <td>4750.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12150.0</td>\n",
       "      <td>Beauty &amp; Grooming</td>\n",
       "      <td>cod</td>\n",
       "      <td>7/1/2016</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2016-7</td>\n",
       "      <td>7-2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>120</td>\n",
       "      <td>complete</td>\n",
       "      <td>7/1/2016</td>\n",
       "      <td>LC_3349668508587</td>\n",
       "      <td>7400.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12150.0</td>\n",
       "      <td>Beauty &amp; Grooming</td>\n",
       "      <td>cod</td>\n",
       "      <td>7/1/2016</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2016-7</td>\n",
       "      <td>7-2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index    order_status order_date  \\\n",
       "0        1        complete   7/1/2016   \n",
       "1        2        canceled   7/1/2016   \n",
       "2        3        canceled   7/1/2016   \n",
       "3        4        complete   7/1/2016   \n",
       "4        5  order_refunded   7/1/2016   \n",
       "..     ...             ...        ...   \n",
       "115    116        canceled   7/1/2016   \n",
       "116    117        canceled   7/1/2016   \n",
       "117    118        canceled   7/1/2016   \n",
       "118    119        complete   7/1/2016   \n",
       "119    120        complete   7/1/2016   \n",
       "\n",
       "                                           product_sku  product_price  \\\n",
       "0                                    kreations_YI 06-L         1950.0   \n",
       "1    kcc_Buy 2 Frey Air Freshener & Get 1 Kasual Bo...          240.0   \n",
       "2                                   Ego_UP0017-999-MR0         2450.0   \n",
       "3                                       kcc_krone deal          360.0   \n",
       "4                                          BK7010400AG          555.0   \n",
       "..                                                 ...            ...   \n",
       "115                      Al Muhafiz Sohan Halwa Almond          350.0   \n",
       "116                       sst_Lyquin-Regular fit-Large         1950.0   \n",
       "117                                      Fcafe_11777-L          795.0   \n",
       "118                                    LC_359547105042         4750.0   \n",
       "119                                   LC_3349668508587         7400.0   \n",
       "\n",
       "     quantity_ordered  order_total   product_category   payment_type  \\\n",
       "0                 1.0       1950.0    Women's Fashion            cod   \n",
       "1                 1.0        240.0  Beauty & Grooming            cod   \n",
       "2                 1.0       2450.0    Women's Fashion            cod   \n",
       "3                 1.0         60.0  Beauty & Grooming            cod   \n",
       "4                 2.0       1110.0            Soghaat            cod   \n",
       "..                ...          ...                ...            ...   \n",
       "115               1.0        350.0            Soghaat            cod   \n",
       "116               1.0       2745.0      Men's Fashion  ublcreditcard   \n",
       "117               1.0       2745.0      Men's Fashion  ublcreditcard   \n",
       "118               1.0      12150.0  Beauty & Grooming            cod   \n",
       "119               1.0      12150.0  Beauty & Grooming            cod   \n",
       "\n",
       "    processing_date  order_year  order_month customer_since month_year  \n",
       "0          7/1/2016      2016.0          7.0         2016-7     7-2016  \n",
       "1          7/1/2016      2016.0          7.0         2016-7     7-2016  \n",
       "2          7/1/2016      2016.0          7.0         2016-7     7-2016  \n",
       "3          7/1/2016      2016.0          7.0         2016-7     7-2016  \n",
       "4          7/1/2016      2016.0          7.0         2016-7     7-2016  \n",
       "..              ...         ...          ...            ...        ...  \n",
       "115        7/1/2016      2016.0          7.0         2016-7     7-2016  \n",
       "116        7/1/2016      2016.0          7.0         2016-7     7-2016  \n",
       "117        7/1/2016      2016.0          7.0         2016-7     7-2016  \n",
       "118        7/1/2016      2016.0          7.0         2016-7     7-2016  \n",
       "119        7/1/2016      2016.0          7.0         2016-7     7-2016  \n",
       "\n",
       "[120 rows x 14 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = dataframes_dict['ecommerce_data_sentence'].dropna()\n",
    "\n",
    "# Step 2: Select the first 120 rows from the cleaned DataFrame\n",
    "df_selected = df_clean.head(120)\n",
    "df_selected.drop([\"index\"],axis=1,inplace=True)\n",
    "df_selected['index'] = range(1, len(df_selected) + 1)\n",
    "dataframes_dict['ecommerce_data_sentence'] = df_selected\n",
    "# Move 'index' column to the beginning\n",
    "dataframes_dict['ecommerce_data_sentence'] = dataframes_dict['ecommerce_data_sentence'][['index'] + [col for col in dataframes_dict['ecommerce_data_sentence'].columns if col != 'index']]\n",
    "\n",
    "dataframes_dict['ecommerce_data_sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_763581/3463404056.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected.drop([\"Customer_ID\"],axis=1,inplace=True)\n",
      "/tmp/ipykernel_763581/3463404056.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['index'] = range(1, len(df_selected) + 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Age</th>\n",
       "      <th>Location</th>\n",
       "      <th>Income_Level</th>\n",
       "      <th>Total_Transactions</th>\n",
       "      <th>Total_Spent</th>\n",
       "      <th>Active_Days</th>\n",
       "      <th>Loyalty_Points_Earned</th>\n",
       "      <th>Cashback_Received</th>\n",
       "      <th>Preferred_Payment_Method</th>\n",
       "      <th>Customer_Satisfaction_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Low</td>\n",
       "      <td>192</td>\n",
       "      <td>3.213386e+06</td>\n",
       "      <td>140</td>\n",
       "      <td>2114</td>\n",
       "      <td>2224.012140</td>\n",
       "      <td>Debit Card</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>67</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>High</td>\n",
       "      <td>979</td>\n",
       "      <td>1.423146e+07</td>\n",
       "      <td>229</td>\n",
       "      <td>2960</td>\n",
       "      <td>4026.823518</td>\n",
       "      <td>UPI</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>Urban</td>\n",
       "      <td>High</td>\n",
       "      <td>329</td>\n",
       "      <td>2.323192e+06</td>\n",
       "      <td>73</td>\n",
       "      <td>3170</td>\n",
       "      <td>1441.011395</td>\n",
       "      <td>Debit Card</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>Rural</td>\n",
       "      <td>High</td>\n",
       "      <td>71</td>\n",
       "      <td>1.166308e+06</td>\n",
       "      <td>299</td>\n",
       "      <td>4756</td>\n",
       "      <td>4365.855580</td>\n",
       "      <td>Wallet Balance</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>58</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Middle</td>\n",
       "      <td>878</td>\n",
       "      <td>9.482481e+06</td>\n",
       "      <td>236</td>\n",
       "      <td>1992</td>\n",
       "      <td>4161.523827</td>\n",
       "      <td>UPI</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>116</td>\n",
       "      <td>39</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Middle</td>\n",
       "      <td>599</td>\n",
       "      <td>3.962774e+06</td>\n",
       "      <td>95</td>\n",
       "      <td>2147</td>\n",
       "      <td>4093.283791</td>\n",
       "      <td>Debit Card</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>117</td>\n",
       "      <td>26</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Middle</td>\n",
       "      <td>897</td>\n",
       "      <td>8.518076e+06</td>\n",
       "      <td>199</td>\n",
       "      <td>1302</td>\n",
       "      <td>1818.347786</td>\n",
       "      <td>Debit Card</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>118</td>\n",
       "      <td>66</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>Middle</td>\n",
       "      <td>19</td>\n",
       "      <td>2.653046e+04</td>\n",
       "      <td>146</td>\n",
       "      <td>3075</td>\n",
       "      <td>2999.723174</td>\n",
       "      <td>Wallet Balance</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>119</td>\n",
       "      <td>32</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Low</td>\n",
       "      <td>46</td>\n",
       "      <td>4.889507e+04</td>\n",
       "      <td>155</td>\n",
       "      <td>4805</td>\n",
       "      <td>242.415140</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>120</td>\n",
       "      <td>23</td>\n",
       "      <td>Rural</td>\n",
       "      <td>High</td>\n",
       "      <td>451</td>\n",
       "      <td>2.645244e+06</td>\n",
       "      <td>270</td>\n",
       "      <td>1960</td>\n",
       "      <td>2264.491186</td>\n",
       "      <td>UPI</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  Age  Location Income_Level  Total_Transactions   Total_Spent  \\\n",
       "0        1   54     Urban          Low                 192  3.213386e+06   \n",
       "1        2   67  Suburban         High                 979  1.423146e+07   \n",
       "2        3   44     Urban         High                 329  2.323192e+06   \n",
       "3        4   30     Rural         High                  71  1.166308e+06   \n",
       "4        5   58     Urban       Middle                 878  9.482481e+06   \n",
       "..     ...  ...       ...          ...                 ...           ...   \n",
       "115    116   39     Urban       Middle                 599  3.962774e+06   \n",
       "116    117   26     Rural       Middle                 897  8.518076e+06   \n",
       "117    118   66  Suburban       Middle                  19  2.653046e+04   \n",
       "118    119   32     Urban          Low                  46  4.889507e+04   \n",
       "119    120   23     Rural         High                 451  2.645244e+06   \n",
       "\n",
       "     Active_Days  Loyalty_Points_Earned  Cashback_Received  \\\n",
       "0            140                   2114        2224.012140   \n",
       "1            229                   2960        4026.823518   \n",
       "2             73                   3170        1441.011395   \n",
       "3            299                   4756        4365.855580   \n",
       "4            236                   1992        4161.523827   \n",
       "..           ...                    ...                ...   \n",
       "115           95                   2147        4093.283791   \n",
       "116          199                   1302        1818.347786   \n",
       "117          146                   3075        2999.723174   \n",
       "118          155                   4805         242.415140   \n",
       "119          270                   1960        2264.491186   \n",
       "\n",
       "    Preferred_Payment_Method  Customer_Satisfaction_Score  \n",
       "0                 Debit Card                            1  \n",
       "1                        UPI                            8  \n",
       "2                 Debit Card                            4  \n",
       "3             Wallet Balance                            1  \n",
       "4                        UPI                            5  \n",
       "..                       ...                          ...  \n",
       "115               Debit Card                           10  \n",
       "116               Debit Card                            9  \n",
       "117           Wallet Balance                            8  \n",
       "118              Credit Card                            3  \n",
       "119                      UPI                            2  \n",
       "\n",
       "[120 rows x 11 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = dataframes_dict['finance_data_sentence'].dropna()\n",
    "\n",
    "# Step 2: Select the first 120 rows from the cleaned DataFrame\n",
    "df_selected = df_clean.head(120)\n",
    "df_selected.drop([\"Customer_ID\"],axis=1,inplace=True)\n",
    "df_selected['index'] = range(1, len(df_selected) + 1)\n",
    "dataframes_dict['finance_data_sentence'] = df_selected\n",
    "# Move 'index' column to the beginning\n",
    "dataframes_dict['finance_data_sentence'] = dataframes_dict['finance_data_sentence'][['index'] + [col for col in dataframes_dict['finance_data_sentence'].columns if col != 'index']]\n",
    "\n",
    "dataframes_dict['finance_data_sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_763581/1494870347.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentence'] = df.apply(lambda row: row_to_sentence(row, primary_key=\"index\"), axis=1)\n"
     ]
    }
   ],
   "source": [
    "def row_to_sentence(row,primary_key=\"\"):\n",
    "    # Start the sentence with an introductory phrase\n",
    "    sentence = \"\"\n",
    "    \n",
    "    # Dynamically iterate over all columns\n",
    "    for col in row.index:\n",
    "        if col!=primary_key:\n",
    "            value = str(row[col]).strip()  # Ensure value is a string and remove leading/trailing spaces\n",
    "            if value.lower() != \"nan\":  # Skip NaN values\n",
    "                col = col.lower()\n",
    "                value = value.strip(\"'\")  # Remove surrounding single quotes if present\n",
    "                value = value.lower()\n",
    "                sentence += f\"{col} is {value}, \"\n",
    "    \n",
    "    # Remove the trailing comma and space, then end with a period\n",
    "    sentence = sentence.rstrip(\", \") + \".\"\n",
    "    return sentence\n",
    "\n",
    "\n",
    "for key in dataframes_dict:\n",
    "    df = dataframes_dict[key]\n",
    "    df['sentence'] = df.apply(lambda row: row_to_sentence(row, primary_key=\"index\"), axis=1)\n",
    "    dataframes_dict[key] = df\n",
    "\n",
    "\n",
    "dataframes_dict['tourism_data_sentence'].to_csv(\"sources/tourism/data2_sentence.csv\",index=False)\n",
    "dataframes_dict['healthcare_data_sentence'].to_csv(\"sources/healthcare/data2_sentence.csv\",index=False)\n",
    "dataframes_dict['education_data_sentence'].to_csv(\"sources/education/data2_sentence.csv\",index=False)\n",
    "dataframes_dict['ecommerce_data_sentence'].to_csv(\"sources/ecommerce/data2_sentence.csv\",index=False)\n",
    "dataframes_dict['finance_data_sentence'].to_csv(\"sources/finance/data2_sentence.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Run from here*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataframes_dict = {}\n",
    "dataframes_dict['tourism_data_sentence'] = pd.read_csv(\"sources/tourism/data2_sentence.csv\")\n",
    "dataframes_dict['healthcare_data_sentence'] = pd.read_csv(\"sources/healthcare/data2_sentence.csv\")\n",
    "dataframes_dict['education_data_sentence'] = pd.read_csv(\"sources/education/data2_sentence.csv\")\n",
    "dataframes_dict['ecommerce_data_sentence'] = pd.read_csv(\"sources/ecommerce/data2_sentence.csv\")\n",
    "dataframes_dict['finance_data_sentence'] = pd.read_csv(\"sources/finance/data2_sentence.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 00:22:24.041069: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-07 00:22:24.063754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746577344.088988 2334953 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746577344.096459 2334953 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746577344.117000 2334953 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746577344.117022 2334953 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746577344.117024 2334953 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746577344.117026 2334953 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-07 00:22:24.123933: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, AutoConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "        self.model = LlamaForCausalLM.from_pretrained(\n",
    "            model_name,# config = config, \n",
    "            torch_dtype=torch.float16,\n",
    "            device_map='auto',\n",
    "        ).to(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer.eos_token = self.tokenizer.pad_token  # Set PAD token to EOS\n",
    "        \n",
    "    def predict(self,prompt, user_prompt=\"\"):\n",
    "        model,tokenizer = self.model, self.tokenizer\n",
    "        temp = random.random()\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "        # print(inputs)\n",
    "        generate_ids = model.generate(**inputs, max_new_tokens=4096, do_sample=True, temperature=temp,pad_token_id=tokenizer.eos_token_id) # Disable sampling for deterministic output\n",
    "        generate_ids = generate_ids[0][len(inputs[\"input_ids\"][0]):-1]\n",
    "        infer_res = tokenizer.decode(generate_ids)\n",
    "        return infer_res\n",
    "        \n",
    "    def enhance_sentence_with_llama(self,sentence):\n",
    "        model = self.model\n",
    "        # Construct the prompt\n",
    "        system_prompt = \"You are a creative AI that rephrases given sentences into engaging, conversational stories while incorporating all provided datapoints. Ensure that no information is omitted or added, and skip any datapoints labeled as 'nan'. Do not rephrase the object of a sentence. For example, if the sentence is 'start date is 9/22/2023', do not change the date to a different format. Respond only with the rephrased sentence without any additional commentary.\"\n",
    "        user_prompt = f\"\"\"\n",
    "    Rephrase the following sentence into a conversational story, ensuring all datapoints are included while skipping 'nan' values. Do not introduce any extra or false details.\n",
    "    \n",
    "    Original sentence: {sentence}\n",
    "    \n",
    "    Creative sentence:\"\"\"\n",
    "        creative_sentence = self.predict(system_prompt,user_prompt)\n",
    "        creative_sentence = creative_sentence.replace(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\", \"\")\n",
    "        \n",
    "        # Extract only the generated part\n",
    "        # creative_sentence = response.split(\"Creative sentence:\")[-1].strip()\n",
    "        return creative_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "# model.enhance_sentence_with_llama(\"Row ID 348: ROW_ID_x is '1136896.0', SUBJECT_ID is '23', HADM_ID_x is '124321.0', ICUSTAY_ID is '234044.0', STARTDATE is '2157-10-21 00:00:00', ENDDATE is '2157-10-25 00:00:00', DRUG_TYPE is 'MAIN', DRUG is 'Sodium Chloride 0.9%  Flush', DRUG_NAME_POE is 'Sodium Chloride 0.9%  Flush', DRUG_NAME_GENERIC is 'Sodium Chloride 0.9%  Flush', FORMULARY_DRUG_CD is 'NACLFLUSH', GSN is 'nan', NDC is '0.0', PROD_STRENGTH is 'Syringe', DOSE_VAL_RX is '3', DOSE_UNIT_RX is 'mL', FORM_VAL_DISP is '0.6', FORM_UNIT_DISP is 'SYR', ROUTE is 'IV', ROW_ID_y is '23.0', HADM_ID_y is '124321.0', ADMITTIME is '2157-10-18 19:34:00', DISCHTIME is '2157-10-25 14:00:00', DEATHTIME is 'nan', ADMISSION_TYPE is 'EMERGENCY', ADMISSION_LOCATION is 'TRANSFER FROM HOSP/EXTRAM', DISCHARGE_LOCATION is 'HOME HEALTH CARE', INSURANCE is 'Medicare', LANGUAGE is 'ENGL', RELIGION is 'CATHOLIC', MARITAL_STATUS is 'MARRIED', ETHNICITY is 'WHITE', EDREGTIME is 'nan', EDOUTTIME is 'nan', DIAGNOSIS is 'BRAIN MASS', HOSPITAL_EXPIRE_FLAG is '0.0', HAS_CHARTEVENTS_DATA is '1.0'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the checkpoint function\n",
    "def process_with_checkpoint(model,df, checkpoint_file, start_index=0, batch_size=10):\n",
    "    # Load existing checkpoint if it exists\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        df = pd.read_csv(checkpoint_file)\n",
    "        print(\"Loaded existing checkpoint.\")\n",
    "        if 'creative_sentence' not in df.columns:\n",
    "            df['creative_sentence'] = None\n",
    "        \n",
    "        # df = df.head(100)\n",
    "    \n",
    "    try:\n",
    "        # Process the dataframe in batches\n",
    "        for i in range(start_index, len(df), batch_size):\n",
    "            # Process a batch of rows\n",
    "            batch = df.iloc[i:i + batch_size]\n",
    "            \n",
    "            for idx, row in batch.iterrows():\n",
    "                if pd.isna(row['creative_sentence']):  # Only process rows not yet completed\n",
    "                    df.at[idx, 'creative_sentence'] = model.enhance_sentence_with_llama(row['sentence'])\n",
    "            \n",
    "            # Save progress after processing each batch\n",
    "            df.to_csv(checkpoint_file, index=False)\n",
    "            print(f\"Checkpoint saved at row {i + batch_size}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        # Save the checkpoint if an error occurs\n",
    "        df.to_csv(checkpoint_file, index=False)\n",
    "        print(\"Checkpoint saved after error.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def dataframe_to_json(shortened_df, dataset_path, primary_key):\n",
    "    json_data = []\n",
    "    shortened_df = shortened_df.drop(columns=['sentence'])\n",
    "    \n",
    "    for _, row in shortened_df.iterrows():\n",
    "        # Extract the primary key value if it's valid\n",
    "        primary_id = str(row[primary_key]).lower() if primary_key in row and pd.notna(row[primary_key]) and str(row[primary_key]).lower() != \"nan\" else None\n",
    "        \n",
    "        # Build key-value pairs, skipping 'creative_sentence' and the primary key\n",
    "        key_value = {\n",
    "            col.lower(): (primary_id, str(row[col]).lower()) \n",
    "            for col in shortened_df.columns \n",
    "            if col not in [\"creative_sentence\", primary_key] and pd.notna(row[col]) and str(row[col]).lower() != \"nan\"\n",
    "        }\n",
    "\n",
    "        entities = list(key_value.keys())\n",
    "\n",
    "        json_data.append({\n",
    "            \"text\": row[\"creative_sentence\"],\n",
    "            \"entities\": entities,\n",
    "            \"key_value\": key_value\n",
    "        })\n",
    "\n",
    "    with open(f\"{dataset_path}.json\", 'w') as json_file:\n",
    "        json.dump(json_data, json_file, indent=4)\n",
    "    \n",
    "    print(\"JSON file saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def combine_jsons(dataset_path):\n",
    "    # Load the JSON file\n",
    "    with open(f\"{dataset_path}.json\", \"r\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "    \n",
    "    # Combine every 5 entries\n",
    "    combined_data = []\n",
    "    batch_size = 5  # Number of entries to combine\n",
    "    \n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i + batch_size]  # Take a batch of 5\n",
    "    \n",
    "        # Merge text fields\n",
    "        combined_text = \"\\n\".join(str(entry[\"text\"]) for entry in batch)\n",
    "    \n",
    "        # Merge unique entities\n",
    "        combined_entities = list(set(entity for entry in batch for entity in entry[\"entities\"]))\n",
    "    \n",
    "        # Merge key-value pairs, keeping all values in a list\n",
    "        combined_key_value = {}\n",
    "    \n",
    "        for entry in batch:\n",
    "            for key, value in entry[\"key_value\"].items():\n",
    "                if key in combined_key_value:\n",
    "                    if value not in combined_key_value[key]:  # Avoid duplicate values\n",
    "                        combined_key_value[key].append(value)\n",
    "                else:\n",
    "                    combined_key_value[key] = [value]\n",
    "        difficulty = None\n",
    "        domain = None\n",
    "        if \"education\" in dataset_path:\n",
    "            difficulty = \"medium\"\n",
    "            domain = \"education\"\n",
    "        elif \"tourism\" in dataset_path:\n",
    "            difficulty = \"easy\"\n",
    "            domain = \"tourism\"\n",
    "        elif \"ecommerce\" in dataset_path:\n",
    "            difficulty = \"medium\"\n",
    "            domain = \"ecommerce\"\n",
    "        elif \"healthcare\" in dataset_path:\n",
    "            difficulty = \"hard\"\n",
    "            domain = \"healthcare\"\n",
    "        elif \"finance\" in dataset_path:\n",
    "            difficulty = \"medium\"\n",
    "            domain = \"finance\"\n",
    "        # Append combined entry\n",
    "        combined_data.append({\n",
    "            \"text\": combined_text,\n",
    "            \"entities\": combined_entities,\n",
    "            \"key_value\": combined_key_value,\n",
    "            \"difficulty\": difficulty,\n",
    "            \"domain\": domain\n",
    "        })\n",
    "    \n",
    "    # Save the new JSON file\n",
    "    with open(f\"{dataset_path}_combined.json\", \"w\") as json_file:\n",
    "        json.dump(combined_data, json_file, indent=4)\n",
    "    \n",
    "    print(\"Combined JSON file saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the combined JSON file\n",
    "def clean_combined_jsons(dataset_path):\n",
    "    with open(f\"{dataset_path}_combined.json\", \"r\") as json_file:\n",
    "        combined_data = json.load(json_file)\n",
    "    \n",
    "    # Function to clean entities and key-value pairs\n",
    "    def clean_entry(entry):\n",
    "        text = entry[\"text\"].lower()  # Convert text to lowercase for case-insensitive matching\n",
    "    \n",
    "        # Keep only entities that exist in the text\n",
    "        filtered_entities = [entity for entity in entry[\"entities\"] if entity.lower() in text]\n",
    "    \n",
    "        # Initialize filtered key-value store\n",
    "        filtered_key_value = {}\n",
    "    \n",
    "        for key, value_list in entry[\"key_value\"].items():\n",
    "            if isinstance(value_list, list):\n",
    "                valid_pairs = []\n",
    "                \n",
    "                for pair in value_list:\n",
    "                    if isinstance(pair, list) and len(pair) == 2:  # Ensure it's a (trip_id, value) structure\n",
    "                        trip_id, actual_value = pair\n",
    "                        \n",
    "                        # Keep only pairs where the value appears in the text\n",
    "                        if str(actual_value).lower() in text:\n",
    "                            valid_pairs.append([trip_id, actual_value])\n",
    "                \n",
    "                # Only add key if it has valid values\n",
    "                if valid_pairs:\n",
    "                    filtered_key_value[key] = valid_pairs\n",
    "\n",
    "        # Return cleaned entry\n",
    "        return {\n",
    "            \"text\": entry[\"text\"],\n",
    "            \"entities\": filtered_entities,\n",
    "            \"key_value\": filtered_key_value,\n",
    "            \"difficulty\": entry[\"difficulty\"],\n",
    "            \"domain\": entry[\"domain\"]\n",
    "            \n",
    "        }\n",
    "    # Apply the cleaning function to each entry\n",
    "    cleaned_data = [clean_entry(entry) for entry in combined_data]\n",
    "    \n",
    "    # Save the cleaned JSON file\n",
    "    with open(f\"{dataset_path}_combined_cleaned.json\", \"w\") as json_file:\n",
    "        json.dump(cleaned_data, json_file, indent=4)\n",
    "    \n",
    "    print(\"Cleaned JSON file saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column_count in tourism_data_sentence 120\n",
      "nan_rows in tourism_data_sentence 0\n",
      "column_count in healthcare_data_sentence 120\n",
      "nan_rows in healthcare_data_sentence 0\n",
      "column_count in education_data_sentence 120\n",
      "nan_rows in education_data_sentence 0\n",
      "column_count in ecommerce_data_sentence 120\n",
      "nan_rows in ecommerce_data_sentence 0\n",
      "column_count in finance_data_sentence 120\n",
      "nan_rows in finance_data_sentence 0\n"
     ]
    }
   ],
   "source": [
    "for name, df in dataframes_dict.items():\n",
    "    row_count = df.shape[0]\n",
    "    print(\"column_count in\",name,row_count)\n",
    "    nan_rows = df[df.isna().any(axis=1)]\n",
    "    print(\"nan_rows in\",name,len(nan_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in dataframes_dict.items():\n",
    "    df.dropna(axis=1, thresh=len(df) * 0.5, inplace=True)  # Drops columns with >50% NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column_count in tourism_data_sentence 120\n",
      "nan_rows in tourism_data_sentence 0\n",
      "column_count in healthcare_data_sentence 120\n",
      "nan_rows in healthcare_data_sentence 0\n",
      "column_count in education_data_sentence 120\n",
      "nan_rows in education_data_sentence 0\n",
      "column_count in ecommerce_data_sentence 120\n",
      "nan_rows in ecommerce_data_sentence 0\n",
      "column_count in finance_data_sentence 120\n",
      "nan_rows in finance_data_sentence 0\n"
     ]
    }
   ],
   "source": [
    "for name, df in dataframes_dict.items():\n",
    "    row_count = df.shape[0]\n",
    "    print(\"column_count in\",name,row_count)\n",
    "    nan_rows = df[df.isna().any(axis=1)]\n",
    "    print(\"nan_rows in\",name,len(nan_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: sources/tourism/data2_sentence.csv\n",
      "Loaded existing checkpoint.\n",
      "Checkpoint saved at row 20.\n",
      "Checkpoint saved at row 40.\n",
      "Checkpoint saved at row 60.\n",
      "Checkpoint saved at row 80.\n",
      "Checkpoint saved at row 100.\n",
      "Checkpoint saved at row 120.\n",
      "Saved: sources/tourism/data2_sentence.csv\n",
      "Processing: sources/healthcare/data2_sentence.csv\n",
      "Loaded existing checkpoint.\n",
      "Checkpoint saved at row 20.\n",
      "Checkpoint saved at row 40.\n",
      "Checkpoint saved at row 60.\n",
      "Checkpoint saved at row 80.\n",
      "Checkpoint saved at row 100.\n",
      "Checkpoint saved at row 120.\n",
      "Saved: sources/healthcare/data2_sentence.csv\n",
      "Processing: sources/education/data2_sentence.csv\n",
      "Loaded existing checkpoint.\n",
      "Checkpoint saved at row 20.\n",
      "Checkpoint saved at row 40.\n",
      "Checkpoint saved at row 60.\n",
      "Checkpoint saved at row 80.\n",
      "Checkpoint saved at row 100.\n",
      "Checkpoint saved at row 120.\n",
      "Saved: sources/education/data2_sentence.csv\n",
      "Processing: sources/ecommerce/data2_sentence.csv\n",
      "Loaded existing checkpoint.\n",
      "Checkpoint saved at row 20.\n",
      "Checkpoint saved at row 40.\n",
      "Checkpoint saved at row 60.\n",
      "Checkpoint saved at row 80.\n",
      "Checkpoint saved at row 100.\n",
      "Checkpoint saved at row 120.\n",
      "Saved: sources/ecommerce/data2_sentence.csv\n",
      "Processing: sources/finance/data2_sentence.csv\n",
      "Loaded existing checkpoint.\n",
      "Checkpoint saved at row 20.\n",
      "Checkpoint saved at row 40.\n",
      "Checkpoint saved at row 60.\n",
      "Checkpoint saved at row 80.\n",
      "Checkpoint saved at row 100.\n",
      "Checkpoint saved at row 120.\n",
      "Saved: sources/finance/data2_sentence.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the base directory\n",
    "base_dir = \"sources\"\n",
    "\n",
    "# Sort the dictionary by DataFrame length in ascending order\n",
    "names = ['tourism','healthcare','education','ecommerce','finance'] #'ecommerce','education'\n",
    "\n",
    "# Loop through each DataFrame in sorted order\n",
    "for name in names:\n",
    "    # Construct the checkpoint file path dynamically\n",
    "    checkpoint_path = f\"{base_dir}/{name}/data2_sentence.csv\"\n",
    "\n",
    "    print(f\"Processing: {checkpoint_path}\")\n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(checkpoint_path)\n",
    "\n",
    "    # Process the DataFrame with checkpointing\n",
    "    df = process_with_checkpoint(model, df, checkpoint_file=checkpoint_path, batch_size=20)\n",
    "\n",
    "    # Save the final result back to the same CSV file\n",
    "    df.to_csv(checkpoint_path, index=False)\n",
    "\n",
    "    print(f\"Saved: {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: sources/tourism/data2_sentence\n",
      "JSON file saved successfully!\n",
      "Combined JSON file saved successfully!\n",
      "Processing: sources/healthcare/data2_sentence\n",
      "JSON file saved successfully!\n",
      "Combined JSON file saved successfully!\n",
      "Processing: sources/education/data2_sentence\n",
      "JSON file saved successfully!\n",
      "Combined JSON file saved successfully!\n",
      "Processing: sources/ecommerce/data2_sentence\n",
      "JSON file saved successfully!\n",
      "Combined JSON file saved successfully!\n",
      "Processing: sources/finance/data2_sentence\n",
      "JSON file saved successfully!\n",
      "Combined JSON file saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "base_dir = \"sources\"\n",
    "\n",
    "names = ['tourism','healthcare','education','ecommerce','finance'] #\n",
    "\n",
    "# Loop through each DataFrame in sorted order\n",
    "for name in names:\n",
    "    # Construct the checkpoint file path dynamically\n",
    "    checkpoint_path = f\"{base_dir}/{name}/data2_sentence\"\n",
    "    primary_id = \"index\"\n",
    "\n",
    "    print(f\"Processing: {checkpoint_path}\")\n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(checkpoint_path + \".csv\")\n",
    "    \n",
    "    dataframe_to_json(df,checkpoint_path,primary_id)\n",
    "    combine_jsons(checkpoint_path)\n",
    "    # clean_combined_jsons(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 files. Cleaning and merging...\n",
      "File: tourism/data2_sentence_combined.json contains 24 valid entries.\n",
      "File: education/data2_sentence_combined.json contains 24 valid entries.\n",
      "File: healthcare/data2_sentence_combined.json contains 24 valid entries.\n",
      "File: ecommerce/data2_sentence_combined.json contains 24 valid entries.\n",
      "File: finance/data2_sentence_combined.json contains 24 valid entries.\n",
      "Successfully merged 120 entries into merged_dataset2.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "def find_json_files(root_folder, target_filename):\n",
    "    \"\"\"Recursively find all files named target_filename in root_folder.\"\"\"\n",
    "    json_files = []\n",
    "    \n",
    "    for root, _, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            if file == target_filename:\n",
    "                file_path = os.path.join(root, file)\n",
    "                json_files.append(file_path)\n",
    "\n",
    "    return json_files\n",
    "\n",
    "def clean_data(data):\n",
    "    \"\"\"Remove entries with empty 'text', 'entities', and 'key_value', and clean empty keys in 'key_value'.\"\"\"\n",
    "    cleaned = []\n",
    "    for entry in data:\n",
    "        if (\n",
    "            entry.get(\"entities\") == [] and\n",
    "            entry.get(\"key_value\") == {}\n",
    "        ):\n",
    "            continue  # Skip unwanted entry\n",
    "\n",
    "        if \"key_value\" in entry:\n",
    "            # Remove empty key_value entries\n",
    "            entry[\"key_value\"] = {k: v for k, v in entry[\"key_value\"].items() if v}\n",
    "\n",
    "        cleaned.append(entry)\n",
    "    return cleaned\n",
    "\n",
    "def merge_and_shuffle_json_files(json_files, root_folder):\n",
    "    \"\"\"Merge and shuffle all lists of dictionaries from found JSON files after cleaning.\"\"\"\n",
    "    combined_data = []\n",
    "    \n",
    "    for file in json_files:\n",
    "        relative_path = os.path.relpath(file, root_folder)\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    data = clean_data(data)\n",
    "                    print(f\"File: {relative_path} contains {len(data)} valid entries.\")\n",
    "                    combined_data.extend(data)\n",
    "                else:\n",
    "                    print(f\"Warning: {relative_path} does not contain a list.\")\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error: Could not decode {relative_path}\")\n",
    "\n",
    "    random.shuffle(combined_data)\n",
    "    return combined_data\n",
    "\n",
    "# Settings\n",
    "root_folder = \"sources\"\n",
    "target_filename = \"data2_sentence_combined.json\"\n",
    "\n",
    "# Run\n",
    "json_files = find_json_files(root_folder, target_filename)\n",
    "\n",
    "if not json_files:\n",
    "    print(\"No matching JSON files found.\")\n",
    "\n",
    "print(f\"Found {len(json_files)} files. Cleaning and merging...\")\n",
    "\n",
    "combined_data = merge_and_shuffle_json_files(json_files, root_folder)\n",
    "\n",
    "output_file = \"merged_dataset2.json\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(combined_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Successfully merged {len(combined_data)} entries into {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "\n",
    "# def remove_empty_key_value_entries(root_dir):\n",
    "#     \"\"\"Remove entries with empty 'key_value' dict from all JSON files in subdirectories.\"\"\"\n",
    "#     for root, _, files in os.walk(root_dir):\n",
    "#         for file in files:\n",
    "#             if file.endswith(\".json\"):\n",
    "#                 file_path = os.path.join(root, file)\n",
    "#                 try:\n",
    "#                     with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#                         data = json.load(f)\n",
    "\n",
    "#                     if isinstance(data, list):\n",
    "#                         cleaned_data = [\n",
    "#                             entry for entry in data\n",
    "#                             if not (isinstance(entry, dict) and entry.get(\"key_value\") == {})\n",
    "#                         ]\n",
    "\n",
    "#                         if len(cleaned_data) != len(data):\n",
    "#                             print(f\"Cleaned {len(data) - len(cleaned_data)} entries from: {file_path}\")\n",
    "\n",
    "#                         # Save back the cleaned data\n",
    "#                         with open(file_path, 'w', encoding='utf-8') as f:\n",
    "#                             json.dump(cleaned_data, f, indent=4, ensure_ascii=False)\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# # Change to your root directory\n",
    "# root_directory = \"sources\"\n",
    "# remove_empty_key_value_entries(root_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finance: 24\n",
      "tourism: 24\n",
      "healthcare: 24\n",
      "education: 24\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from itertools import cycle\n",
    "\n",
    "# Define the path to your JSON file\n",
    "file_path = '/home/mushtari/nl2db/nl2db-main/data-generation/merged_dataset2.json'\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data = [entry for entry in data if entry.get(\"domain\") != \"ecommerce\"]\n",
    "finance_entries = [entry for entry in data if entry.get(\"domain\") == \"finance\"]\n",
    "non_finance_entries = [entry for entry in data if entry.get(\"domain\") != \"finance\"]\n",
    "\n",
    "# Keep only the first 24 finance entries\n",
    "filtered_finance_entries = finance_entries[:24]\n",
    "\n",
    "# Combine the filtered finance entries with the rest\n",
    "data = non_finance_entries + filtered_finance_entries\n",
    "\n",
    "domain_buckets = defaultdict(list)\n",
    "for entry in data:\n",
    "    domain = entry.get(\"domain\")\n",
    "    domain_buckets[domain].append(entry)\n",
    "\n",
    "# Define the domains to interleave (and their order)\n",
    "target_domains = [\"finance\", \"tourism\", \"healthcare\", \"education\"]\n",
    "\n",
    "# Create a round-robin interleaving of entries\n",
    "interleaved = []\n",
    "domain_iters = {d: iter(domain_buckets[d]) for d in target_domains}\n",
    "\n",
    "# Continue pulling one entry at a time from each domain in order\n",
    "while any(domain_buckets[d] for d in target_domains):\n",
    "    for d in target_domains:\n",
    "        if domain_buckets[d]:  # Only append if there are entries left\n",
    "            interleaved.append(domain_buckets[d].pop(0))\n",
    "\n",
    "data = interleaved\n",
    "\n",
    "# Count how many entries per domain\n",
    "domain_counts = Counter(entry.get(\"domain\") for entry in data)\n",
    "\n",
    "# Print the counts\n",
    "for domain, count in domain_counts.items():\n",
    "    print(f\"{domain}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in data:\n",
    "    gtv = entry.get(\"key_value\", {})\n",
    "    for key in gtv:\n",
    "        for i, pair in enumerate(gtv[key]):\n",
    "            pair[0] = str(i + 1)  # Replace first value with \"1\", \"2\", ..., \"5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON files\n",
    "with open('/home/mushtari/nl2db/nl2db-main/data-generation/merged_dataset.json', 'r') as f1, open('/home/mushtari/nl2db/nl2db-main/data-generation/merged_dataset2.json', 'r') as f2:\n",
    "    data1 = json.load(f1)\n",
    "    data2 = json.load(f2)\n",
    "\n",
    "# Filter healthcare entries from file1\n",
    "healthcare_entries_file1 = [entry for entry in data1 if entry.get('domain') == 'healthcare']\n",
    "\n",
    "# Replace healthcare entries in file2 at their original positions\n",
    "replaced_data2 = []\n",
    "healthcare_idx = 0\n",
    "\n",
    "for entry in data2:\n",
    "    if entry.get('domain') == 'healthcare':\n",
    "        if healthcare_idx < len(healthcare_entries_file1):\n",
    "            replaced_data2.append(healthcare_entries_file1[healthcare_idx])\n",
    "            healthcare_idx += 1\n",
    "        else:\n",
    "            # Optionally skip or keep original if not enough entries in file1\n",
    "            replaced_data2.append(entry)\n",
    "    else:\n",
    "        replaced_data2.append(entry)\n",
    "\n",
    "# Save the modified data2\n",
    "with open('/home/mushtari/nl2db/nl2db-main/data-generation/merged_dataset3.json', 'w') as f:\n",
    "    json.dump(replaced_data2, f, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
