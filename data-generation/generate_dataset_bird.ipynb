{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing donor.sqlite in /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/donor\n",
      "  -> Exported sqlite_sequence to sqlite_sequence.csv\n",
      "  -> Exported essays to essays.csv\n",
      "  -> Exported projects to projects.csv\n",
      "  -> Exported donations to donations.csv\n",
      "  -> Exported resources to resources.csv\n",
      "\n",
      "Processing law_episode.sqlite in /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/law_episode\n",
      "  -> Exported Episode to Episode.csv\n",
      "  -> Exported Keyword to Keyword.csv\n",
      "  -> Exported Person to Person.csv\n",
      "  -> Exported Award to Award.csv\n",
      "  -> Exported Credit to Credit.csv\n",
      "  -> Exported Vote to Vote.csv\n",
      "\n",
      "Processing student_club.sqlite in /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/student_club\n",
      "  -> Exported event to event.csv\n",
      "  -> Exported major to major.csv\n",
      "  -> Exported zip_code to zip_code.csv\n",
      "  -> Exported attendance to attendance.csv\n",
      "  -> Exported budget to budget.csv\n",
      "  -> Exported expense to expense.csv\n",
      "  -> Exported income to income.csv\n",
      "  -> Exported member to member.csv\n",
      "\n",
      "Processing books.sqlite in /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/books\n",
      "  -> Exported address_status to address_status.csv\n",
      "  -> Exported author to author.csv\n",
      "  -> Exported book_language to book_language.csv\n",
      "  -> Exported country to country.csv\n",
      "  -> Exported address to address.csv\n",
      "  -> Exported customer to customer.csv\n",
      "  -> Exported customer_address to customer_address.csv\n",
      "  -> Exported order_status to order_status.csv\n",
      "  -> Exported publisher to publisher.csv\n",
      "  -> Exported book to book.csv\n",
      "  -> Exported book_author to book_author.csv\n",
      "  -> Exported shipping_method to shipping_method.csv\n",
      "  -> Exported cust_order to cust_order.csv\n",
      "  -> Exported sqlite_sequence to sqlite_sequence.csv\n",
      "  -> Exported order_history to order_history.csv\n",
      "  -> Exported order_line to order_line.csv\n",
      "\n",
      "Processing computer_student.sqlite in /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/computer_student\n",
      "  -> Exported course to course.csv\n",
      "  -> Exported person to person.csv\n",
      "  -> Exported advisedBy to advisedBy.csv\n",
      "  -> Exported taughtBy to taughtBy.csv\n",
      "\n",
      "Processing mental_health_survey.sqlite in /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/mental_health_survey\n",
      "  -> Exported Question to Question.csv\n",
      "  -> Exported Survey to Survey.csv\n",
      "  -> Exported Answer to Answer.csv\n",
      "\n",
      "Processing airline.sqlite in /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/airline\n",
      "  -> Exported Air Carriers to Air Carriers.csv\n",
      "  -> Exported Airports to Airports.csv\n",
      "  -> Exported Airlines to Airlines.csv\n",
      "\n",
      "Processing authors.sqlite in /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/authors\n",
      "  -> Exported Author to Author.csv\n",
      "  -> Exported Conference to Conference.csv\n",
      "  -> Exported Journal to Journal.csv\n",
      "  -> Exported Paper to Paper.csv\n",
      "  -> Exported PaperAuthor to PaperAuthor.csv\n",
      "\n",
      "Processing financial.sqlite in /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/financial\n",
      "  -> Exported account to account.csv\n",
      "  -> Exported card to card.csv\n",
      "  -> Exported client to client.csv\n",
      "  -> Exported disp to disp.csv\n",
      "  -> Exported district to district.csv\n",
      "  -> Exported loan to loan.csv\n",
      "  -> Exported order to order.csv\n",
      "  -> Exported trans to trans.csv\n",
      "\n",
      "Processing human_resources.sqlite in /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/human_resources\n",
      "  -> Exported location to location.csv\n",
      "  -> Exported position to position.csv\n",
      "  -> Exported employee to employee.csv\n",
      "\n",
      "Processing california_schools.sqlite in /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/california_schools\n",
      "  -> Exported frpm to frpm.csv\n",
      "  -> Exported satscores to satscores.csv\n",
      "  -> Exported schools to schools.csv\n",
      "\n",
      "Processing superhero.sqlite in /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/superhero\n",
      "  -> Exported alignment to alignment.csv\n",
      "  -> Exported attribute to attribute.csv\n",
      "  -> Exported colour to colour.csv\n",
      "  -> Exported gender to gender.csv\n",
      "  -> Exported publisher to publisher.csv\n",
      "  -> Exported race to race.csv\n",
      "  -> Exported superhero to superhero.csv\n",
      "  -> Exported hero_attribute to hero_attribute.csv\n",
      "  -> Exported superpower to superpower.csv\n",
      "  -> Exported hero_power to hero_power.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "processed = [\"california_schools\",\"financial\",\"superhero\"]\n",
    "def export_sqlite_tables_to_csv_in_place(root_dir):\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for file in filenames:\n",
    "            if file.endswith((\".sqlite\", \".db\")):\n",
    "                if file not in processed:\n",
    "                    db_path = os.path.join(dirpath, file)\n",
    "                    export_db_tables_to_csv(db_path)\n",
    "\n",
    "def export_db_tables_to_csv(db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = cursor.fetchall()\n",
    "\n",
    "    db_dir = os.path.dirname(db_path)\n",
    "    db_name = os.path.basename(db_path)\n",
    "    print(f\"\\nProcessing {db_name} in {db_dir}\")\n",
    "\n",
    "    for (table_name,) in tables:\n",
    "        df = pd.read_sql_query(f\"SELECT * FROM `{table_name}`;\", conn)\n",
    "        csv_filename = f\"{table_name}.csv\"\n",
    "        csv_path = os.path.join(db_dir, csv_filename)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"  -> Exported {table_name} to {csv_filename}\")\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "# Example usage\n",
    "export_sqlite_tables_to_csv_in_place(\"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def merge_two_csvs_with_different_keys(csv_file1, csv_file2, join_col1, join_col2, how='inner'):\n",
    "    try:\n",
    "        # Load both CSVs\n",
    "        df1 = pd.read_csv(csv_file1)\n",
    "        df2 = pd.read_csv(csv_file2)\n",
    "\n",
    "        # Check join columns exist\n",
    "        if join_col1 not in df1.columns:\n",
    "            print(f\"Join column '{join_col1}' not found in {csv_file1}\")\n",
    "            return\n",
    "        if join_col2 not in df2.columns:\n",
    "            print(f\"Join column '{join_col2}' not found in {csv_file2}\")\n",
    "            return\n",
    "\n",
    "        # Rename join_col2 in df2 to match join_col1 for merging\n",
    "        df2_renamed = df2.rename(columns={join_col2: join_col1})\n",
    "\n",
    "        # Merge on renamed column\n",
    "        merged_df = pd.merge(df1, df2_renamed, on=join_col1, how=how)\n",
    "\n",
    "        # Save result in same directory as first file\n",
    "        output_dir = os.path.dirname(csv_file1)\n",
    "        merged_filename = f\"merged.csv\"\n",
    "        output_path = os.path.join(output_dir, merged_filename)\n",
    "\n",
    "        merged_df.to_csv(output_path, index=False)\n",
    "        print(f\"Merged CSV saved to: {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during merge: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved to: /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/books/merged.csv\n"
     ]
    }
   ],
   "source": [
    "merge_two_csvs_with_different_keys(\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/books/book.csv\",\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/books/book_author.csv\",\n",
    "    join_col1=\"book_id\",  # from file1\n",
    "    join_col2=\"book_id\",      # from file2\n",
    "    how=\"inner\"           # or 'outer', 'left', 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved to: /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/books/merged.csv\n"
     ]
    }
   ],
   "source": [
    "merge_two_csvs_with_different_keys(\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/books/merged.csv\",\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/books/book_language.csv\",\n",
    "    join_col1=\"language_id\",  # from file1\n",
    "    join_col2=\"language_id\",      # from file2\n",
    "    how=\"inner\"           # or 'outer', 'left', 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved to: /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/computer_student/merged.csv\n"
     ]
    }
   ],
   "source": [
    "merge_two_csvs_with_different_keys(\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/computer_student/person.csv\",\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/computer_student/taughtBy.csv\",\n",
    "    join_col1=\"p_id\",  # from file1\n",
    "    join_col2=\"p_id\",      # from file2\n",
    "    how=\"inner\"           # or 'outer', 'left', 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved to: /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/computer_student/merged.csv\n"
     ]
    }
   ],
   "source": [
    "merge_two_csvs_with_different_keys(\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/computer_student/merged.csv\",\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/computer_student/course.csv\",\n",
    "    join_col1=\"course_id\",  # from file1\n",
    "    join_col2=\"course_id\",      # from file2\n",
    "    how=\"inner\"           # or 'outer', 'left', 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved to: /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/human_resources/merged.csv\n"
     ]
    }
   ],
   "source": [
    "merge_two_csvs_with_different_keys(\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/human_resources/employee.csv\",\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/human_resources/location.csv\",\n",
    "    join_col1=\"locationID\",  # from file1\n",
    "    join_col2=\"locationID\",      # from file2\n",
    "    how=\"inner\"           # or 'outer', 'left', 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved to: /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/human_resources/merged.csv\n"
     ]
    }
   ],
   "source": [
    "merge_two_csvs_with_different_keys(\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/human_resources/merged.csv\",\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/human_resources/position.csv\",\n",
    "    join_col1=\"positionID\",  # from file1\n",
    "    join_col2=\"positionID\",      # from file2\n",
    "    how=\"inner\"           # or 'outer', 'left', 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_two_csvs_with_different_keys(\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/human_resources/merged.csv\",\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/human_resources/position.csv\",\n",
    "    join_col1=\"positionID\",  # from file1\n",
    "    join_col2=\"positionID\",      # from file2\n",
    "    how=\"inner\"           # or 'outer', 'left', 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_two_csvs_with_different_keys(\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/human_resources/merged.csv\",\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/human_resources/position.csv\",\n",
    "    join_col1=\"positionID\",  # from file1\n",
    "    join_col2=\"positionID\",      # from file2\n",
    "    how=\"inner\"           # or 'outer', 'left', 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_two_csvs_with_different_keys(\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/books/merged.csv\",\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/books/author.csv\",\n",
    "    join_col1=\"author_id\",  # from file1\n",
    "    join_col2=\"author_id\",      # from file2\n",
    "    how=\"inner\"           # or 'outer', 'left', 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_two_csvs_with_different_keys(\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/authors/Author.csv\",\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/authors/Journal.csv\",\n",
    "    join_col1=\"Id\",  # from file1\n",
    "    join_col2=\"Id\",      # from file2\n",
    "    how=\"inner\"           # or 'outer', 'left', 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved to: /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/law_episode/merged.csv\n"
     ]
    }
   ],
   "source": [
    "merge_two_csvs_with_different_keys(\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/law_episode/Award.csv\",\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/law_episode/Credit.csv\",\n",
    "    join_col1=\"episode_id\",  # from file1\n",
    "    join_col2=\"episode_id\",      # from file2\n",
    "    how=\"inner\"           # or 'outer', 'left', 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved to: /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/law_episode/merged.csv\n"
     ]
    }
   ],
   "source": [
    "merge_two_csvs_with_different_keys(\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/law_episode/merged.csv\",\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/law_episode/Episode.csv\",\n",
    "    join_col1=\"episode_id\",  # from file1\n",
    "    join_col2=\"episode_id\",      # from file2\n",
    "    how=\"inner\"           # or 'outer', 'left', 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved to: /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/mental_health_survey/merged.csv\n"
     ]
    }
   ],
   "source": [
    "merge_two_csvs_with_different_keys(\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/mental_health_survey/Question.csv\",\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/mental_health_survey/Answer.csv\",\n",
    "    join_col1=\"questionid\",  # from file1\n",
    "    join_col2=\"QuestionID\",      # from file2\n",
    "    how=\"inner\"           # or 'outer', 'left', 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved to: /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/california_schools/merged.csv\n"
     ]
    }
   ],
   "source": [
    "merge_two_csvs_with_different_keys(\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/california_schools/frpm.csv\",\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/california_schools/satscores.csv\",\n",
    "    join_col1=\"CDSCode\",  # from file1\n",
    "    join_col2=\"cds\",      # from file2\n",
    "    how=\"inner\"           # or 'outer', 'left', 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved to: /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/california_schools/merged.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2273155/3370939895.py:8: DtypeWarning: Columns (23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df2 = pd.read_csv(csv_file2)\n"
     ]
    }
   ],
   "source": [
    "merge_two_csvs_with_different_keys(\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/california_schools/merged.csv\",\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/california_schools/schools.csv\",\n",
    "    join_col1=\"CDSCode\",  # from file1\n",
    "    join_col2=\"CDSCode\",      # from file2\n",
    "    how=\"inner\"           # or 'outer', 'left', 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved to: /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/financial/merged.csv\n"
     ]
    }
   ],
   "source": [
    "merge_two_csvs_with_different_keys(\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/financial/account.csv\",\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/financial/disp.csv\",\n",
    "    join_col1=\"account_id\",  # from file1\n",
    "    join_col2=\"account_id\",      # from file2\n",
    "    how=\"inner\"           # or 'outer', 'left', 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved to: /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/financial/merged.csv\n"
     ]
    }
   ],
   "source": [
    "merge_two_csvs_with_different_keys(\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/financial/client.csv\",\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/financial/merged.csv\",\n",
    "    join_col1=\"client_id\",  # from file1\n",
    "    join_col2=\"client_id\",      # from file2\n",
    "    how=\"inner\"           # or 'outer', 'left', 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved to: /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/financial/merged.csv\n"
     ]
    }
   ],
   "source": [
    "merge_two_csvs_with_different_keys(\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/financial/card.csv\",\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/financial/merged.csv\",\n",
    "    join_col1=\"disp_id\",  # from file1\n",
    "    join_col2=\"disp_id\",      # from file2\n",
    "    how=\"inner\"           # or 'outer', 'left', 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved to: /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/financial/merged.csv\n"
     ]
    }
   ],
   "source": [
    "merge_two_csvs_with_different_keys(\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/financial/merged.csv\",\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/financial/order.csv\",\n",
    "    join_col1=\"account_id\",  # from file1\n",
    "    join_col2=\"account_id\",      # from file2\n",
    "    how=\"inner\"           # or 'outer', 'left', 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2273155/3370939895.py:8: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df2 = pd.read_csv(csv_file2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved to: /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/financial/merged.csv\n"
     ]
    }
   ],
   "source": [
    "merge_two_csvs_with_different_keys(\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/financial/merged.csv\",\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/financial/trans.csv\",\n",
    "    join_col1=\"account_id\",  # from file1\n",
    "    join_col2=\"account_id\",      # from file2\n",
    "    how=\"inner\"           # or 'outer', 'left', 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved to: /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/thrombosis_prediction/merged.csv\n"
     ]
    }
   ],
   "source": [
    "merge_two_csvs_with_different_keys(\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/thrombosis_prediction/Examination.csv\",\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/thrombosis_prediction/Laboratory.csv\",\n",
    "    join_col1=\"ID\",  # from file1\n",
    "    join_col2=\"ID\",      # from file2\n",
    "    how=\"inner\"           # or 'outer', 'left', 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved to: /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/thrombosis_prediction/merged.csv\n"
     ]
    }
   ],
   "source": [
    "merge_two_csvs_with_different_keys(\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/thrombosis_prediction/merged.csv\",\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/thrombosis_prediction/Patient.csv\",\n",
    "    join_col1=\"ID\",  # from file1\n",
    "    join_col2=\"ID\",      # from file2\n",
    "    how=\"inner\"           # or 'outer', 'left', 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved to: /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/debit_card_specializing/merged.csv\n"
     ]
    }
   ],
   "source": [
    "merge_two_csvs_with_different_keys(\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/debit_card_specializing/customers.csv\",\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/debit_card_specializing/transactions_1k.csv\",\n",
    "    join_col1=\"CustomerID\",  # from file1\n",
    "    join_col2=\"CustomerID\",      # from file2\n",
    "    how=\"inner\"           # or 'outer', 'left', 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved to: /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/debit_card_specializing/merged.csv\n"
     ]
    }
   ],
   "source": [
    "merge_two_csvs_with_different_keys(\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/debit_card_specializing/merged.csv\",\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/debit_card_specializing/gasstations.csv\",\n",
    "    join_col1=\"GasStationID\",  # from file1\n",
    "    join_col2=\"GasStationID\",      # from file2\n",
    "    how=\"inner\"           # or 'outer', 'left', 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved to: /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/debit_card_specializing/merged.csv\n"
     ]
    }
   ],
   "source": [
    "merge_two_csvs_with_different_keys(\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/debit_card_specializing/merged.csv\",\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/debit_card_specializing/products.csv\",\n",
    "    join_col1=\"ProductID\",  # from file1\n",
    "    join_col2=\"ProductID\",      # from file2\n",
    "    how=\"inner\"           # or 'outer', 'left', 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved to: /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/superhero/merged.csv\n"
     ]
    }
   ],
   "source": [
    "merge_two_csvs_with_different_keys(\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/superhero/superhero.csv\",\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/superhero/alignment.csv\",\n",
    "    join_col1=\"alignment_id\",  # from file1\n",
    "    join_col2=\"id\",      # from file2\n",
    "    how=\"inner\"           # or 'outer', 'left', 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved to: /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/superhero/merged.csv\n"
     ]
    }
   ],
   "source": [
    "merge_two_csvs_with_different_keys(\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/superhero/merged.csv\",\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/superhero/race.csv\",\n",
    "    join_col1=\"race_id\",  # from file1\n",
    "    join_col2=\"id\",      # from file2\n",
    "    how=\"inner\"           # or 'outer', 'left', 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved to: /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/superhero/merged.csv\n"
     ]
    }
   ],
   "source": [
    "merge_two_csvs_with_different_keys(\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/superhero/merged.csv\",\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/superhero/publisher.csv\",\n",
    "    join_col1=\"publisher_id\",  # from file1\n",
    "    join_col2=\"id\",      # from file2\n",
    "    how=\"inner\"           # or 'outer', 'left', 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved to: /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/superhero/merged.csv\n"
     ]
    }
   ],
   "source": [
    "merge_two_csvs_with_different_keys(\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/superhero/merged.csv\",\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/superhero/gender.csv\",\n",
    "    join_col1=\"gender_id\",  # from file1\n",
    "    join_col2=\"id\",      # from file2\n",
    "    how=\"inner\"           # or 'outer', 'left', 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved to: /home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/superhero/merged.csv\n"
     ]
    }
   ],
   "source": [
    "merge_two_csvs_with_different_keys(\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/superhero/merged.csv\",\n",
    "    \"/home/mushtari/nl2db/nl2db-main/data-generation/data_minidev/MINIDEV/dev_databases/superhero/colour.csv\",\n",
    "    join_col1=\"hair_colour_id\",  # from file1\n",
    "    join_col2=\"id\",      # from file2\n",
    "    how=\"inner\"           # or 'outer', 'left', 'right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mushtari/nl2db/nl2db-main/data-generation\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: data_minidev/MINIDEV/dev_databases/law_episode/merged.csv -> DataFrame: law_episode_merged\n",
      "Loaded: data_minidev/MINIDEV/dev_databases/books/merged.csv -> DataFrame: books_merged\n",
      "Loaded: data_minidev/MINIDEV/dev_databases/computer_student/merged.csv -> DataFrame: computer_student_merged\n",
      "Loaded: data_minidev/MINIDEV/dev_databases/mental_health_survey/merged.csv -> DataFrame: mental_health_survey_merged\n",
      "Loaded: data_minidev/MINIDEV/dev_databases/airline/merged.csv -> DataFrame: airline_merged\n",
      "Loaded: data_minidev/MINIDEV/dev_databases/authors/merged.csv -> DataFrame: authors_merged\n",
      "Loaded: data_minidev/MINIDEV/dev_databases/human_resources/merged.csv -> DataFrame: human_resources_merged\n",
      "DataFrame Name: law_episode_merged\n",
      "   award_id                  organization  year award_category  \\\n",
      "0       258  International Monitor Awards  1999        Monitor   \n",
      "1       258  International Monitor Awards  1999        Monitor   \n",
      "2       258  International Monitor Awards  1999        Monitor   \n",
      "3       258  International Monitor Awards  1999        Monitor   \n",
      "4       258  International Monitor Awards  1999        Monitor   \n",
      "\n",
      "                                               award       series_x  \\\n",
      "0  Film Originated Television Series - Best Achie...  Law and Order   \n",
      "1  Film Originated Television Series - Best Achie...  Law and Order   \n",
      "2  Film Originated Television Series - Best Achie...  Law and Order   \n",
      "3  Film Originated Television Series - Best Achie...  Law and Order   \n",
      "4  Film Originated Television Series - Best Achie...  Law and Order   \n",
      "\n",
      "  episode_id person_id_x role_x  result  ...       series_y season episode  \\\n",
      "0  tt0629149   nm0937725    NaN  Winner  ...  Law and Order      9       5   \n",
      "1  tt0629149   nm0937725    NaN  Winner  ...  Law and Order      9       5   \n",
      "2  tt0629149   nm0937725    NaN  Winner  ...  Law and Order      9       5   \n",
      "3  tt0629149   nm0937725    NaN  Winner  ...  Law and Order      9       5   \n",
      "4  tt0629149   nm0937725    NaN  Winner  ...  Law and Order      9       5   \n",
      "\n",
      "   number_in_series  title                                            summary  \\\n",
      "0               186  Agony  After a woman is brutally attacked, the police...   \n",
      "1               186  Agony  After a woman is brutally attacked, the police...   \n",
      "2               186  Agony  After a woman is brutally attacked, the police...   \n",
      "3               186  Agony  After a woman is brutally attacked, the police...   \n",
      "4               186  Agony  After a woman is brutally attacked, the police...   \n",
      "\n",
      "     air_date                                      episode_image rating votes  \n",
      "0  1998-11-04  https://m.media-amazon.com/images/M/MV5BMzQ2Zm...    8.2   187  \n",
      "1  1998-11-04  https://m.media-amazon.com/images/M/MV5BMzQ2Zm...    8.2   187  \n",
      "2  1998-11-04  https://m.media-amazon.com/images/M/MV5BMzQ2Zm...    8.2   187  \n",
      "3  1998-11-04  https://m.media-amazon.com/images/M/MV5BMzQ2Zm...    8.2   187  \n",
      "4  1998-11-04  https://m.media-amazon.com/images/M/MV5BMzQ2Zm...    8.2   187  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "DataFrame Name: books_merged\n",
      "   book_id                                              title       isbn13  \\\n",
      "0        1        The World's First Love: Mary  Mother of God   8987059752   \n",
      "1        2                                     The Illuminati  20049130001   \n",
      "2        3                                 The Servant Leader  23755004321   \n",
      "3        4  What Life Was Like in the Jewel in the Crown: ...  34406054602   \n",
      "4        5  Cliffs Notes on Aristophanes' Lysistrata  The ...  49086007763   \n",
      "\n",
      "   language_id  num_pages publication_date  publisher_id  author_id  \\\n",
      "0            2        276       1996-09-01          1010       2778   \n",
      "1            1        352       2004-10-04          1967       5049   \n",
      "2            1        128       2003-03-11          1967       4902   \n",
      "3            1        168       1999-09-01          1978       8651   \n",
      "4            1         80       1983-12-29           416       8904   \n",
      "\n",
      "            author_name language_code          language_name  \n",
      "0       Fulton J. Sheen         en-US  United States English  \n",
      "1         Larry Burkett           eng                English  \n",
      "2  Kenneth H. Blanchard           eng                English  \n",
      "3       Time-Life Books           eng                English  \n",
      "4      W. John Campbell           eng                English  \n",
      "DataFrame Name: computer_student_merged\n",
      "   p_id  professor  student hasPosition        inPhase yearsInProgram  \\\n",
      "0     5          1        0     Faculty              0              0   \n",
      "1     5          1        0     Faculty              0              0   \n",
      "2     5          1        0     Faculty              0              0   \n",
      "3     9          0        1           0  Post_Generals         Year_5   \n",
      "4    18          0        1           0      Pre_Quals         Year_3   \n",
      "\n",
      "   course_id courseLevel  \n",
      "0         19   Level_500  \n",
      "1         51   Level_400  \n",
      "2         71   Level_500  \n",
      "3        124   Level_300  \n",
      "4         51   Level_400  \n",
      "DataFrame Name: mental_health_survey_merged\n",
      "        questiontext  questionid AnswerText  SurveyID  UserID\n",
      "0  What is your age?           1         37      2014       1\n",
      "1  What is your age?           1         44      2014       2\n",
      "2  What is your age?           1         32      2014       3\n",
      "3  What is your age?           1         31      2014       4\n",
      "4  What is your age?           1         31      2014       5\n",
      "DataFrame Name: airline_merged\n",
      "    FL_DATE  OP_CARRIER_AIRLINE_ID TAIL_NUM  OP_CARRIER_FL_NUM  \\\n",
      "0  2018/8/1                  19805   N956AN               1587   \n",
      "1  2018/8/1                  19805   N973AN               1588   \n",
      "2  2018/8/1                  19805    N9006               1590   \n",
      "3  2018/8/1                  19805   N870NN               1591   \n",
      "4  2018/8/1                  19805   N9023N               1593   \n",
      "\n",
      "   ORIGIN_AIRPORT_ID  ORIGIN_AIRPORT_SEQ_ID  ORIGIN_CITY_MARKET_ID ORIGIN  \\\n",
      "0              12478                1247805                  31703    JFK   \n",
      "1              14107                1410702                  30466    PHX   \n",
      "2              11042                1104205                  30647    CLE   \n",
      "3              14843                1484306                  34819    SJU   \n",
      "4              10423                1042302                  30423    AUS   \n",
      "\n",
      "   DEST_AIRPORT_ID  DEST_AIRPORT_SEQ_ID  ...  ARR_DELAY_NEW CANCELLED  \\\n",
      "0            14107              1410702  ...           44.0         0   \n",
      "1            11618              1161802  ...           53.0         0   \n",
      "2            11298              1129806  ...            0.0         0   \n",
      "3            11298              1129806  ...           43.0         0   \n",
      "4            13303              1330303  ...            0.0         0   \n",
      "\n",
      "   CANCELLATION_CODE  CRS_ELAPSED_TIME  ACTUAL_ELAPSED_TIME  CARRIER_DELAY  \\\n",
      "0                NaN               342                377.0            9.0   \n",
      "1                NaN               285                309.0            0.0   \n",
      "2                NaN               176                177.0            NaN   \n",
      "3                NaN               304                303.0           43.0   \n",
      "4                NaN               173                175.0            NaN   \n",
      "\n",
      "   WEATHER_DELAY  NAS_DELAY  SECURITY_DELAY  LATE_AIRCRAFT_DELAY  \n",
      "0            0.0       35.0             0.0                  0.0  \n",
      "1            0.0       53.0             0.0                  0.0  \n",
      "2            NaN        NaN             NaN                  NaN  \n",
      "3            0.0        0.0             0.0                  0.0  \n",
      "4            NaN        NaN             NaN                  NaN  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "DataFrame Name: authors_merged\n",
      "   Id               Name                                Affiliation ShortName  \\\n",
      "0   9      Ernest Jordan                                        NaN    COMSUR   \n",
      "1  14          K. MORIBE                                        NaN    EXPERT   \n",
      "2  15      D. Jakominich                                        NaN  INTERNET   \n",
      "3  25  William H. Nailon                                        NaN      PAMI   \n",
      "4  37   P. B. Littlewood  Cavendish Laboratory|Cambridge University       IFE   \n",
      "\n",
      "                                            FullName  \\\n",
      "0          IEEE Communications Surveys and Tutorials   \n",
      "1             IEEE Expert / IEEE Intelligent Systems   \n",
      "2                            IEEE Internet Computing   \n",
      "3  IEEE Transactions on Pattern Analysis and Mach...   \n",
      "4             Informatik - Forschung Und Entwicklung   \n",
      "\n",
      "                                            HomePage  \n",
      "0  http://www.comsoc.org/livepubs/surveys/index.html  \n",
      "1  http://ieeexplore.ieee.org/servlet/opac?punumb...  \n",
      "2                  http://www.computer.org/internet/  \n",
      "3                         http://computer.org/tpami/  \n",
      "4        http://www.springerlink.com/content/100528/  \n",
      "DataFrame Name: human_resources_merged\n",
      "           ssn lastname firstname hiredate        salary gender performance  \\\n",
      "0  000-01-0000  Milgrom  Patricia  10/1/04  US$57,500.00      F     Average   \n",
      "1  000-02-2222    Adams     Sandy  1/15/01  US$19,500.00      F     Average   \n",
      "2  109-87-6543     Wood     Emily  3/12/97  US$69,000.00      F     Average   \n",
      "3  109-87-6544   Foster    Harold  8/14/05  US$55,000.00      M        Good   \n",
      "4  111-12-1111  Johnson     James   5/3/96  US$47,500.00      M        Good   \n",
      "\n",
      "   positionID  locationID   locationcity               address state  zipcode  \\\n",
      "0           2           2         Boston        3 Commons Blvd    MA     2190   \n",
      "1           3           1        Atlanta      450 Peachtree Rd    GA    30316   \n",
      "2           2           5  New York City  1650 Washington Blvd    NY    15648   \n",
      "3           1           3        Chicago      500 Loop Highway    IL    60620   \n",
      "4           1           3        Chicago      500 Loop Highway    IL    60620   \n",
      "\n",
      "     officephone           positiontitle educationrequired     minsalary  \\\n",
      "0  (617)123-4444                 Manager     4 year degree  US$50,000.00   \n",
      "1  (404)333-5555                 Trainee     2 year degree  US$18,000.00   \n",
      "2  (518)256-3100                 Manager     4 year degree  US$50,000.00   \n",
      "3  (312)444-6666  Account Representative     4 year degree  US$25,000.00   \n",
      "4  (312)444-6666  Account Representative     4 year degree  US$25,000.00   \n",
      "\n",
      "       maxsalary  \n",
      "0  US$150,000.00  \n",
      "1   US$25,000.00  \n",
      "2  US$150,000.00  \n",
      "3   US$75,000.00  \n",
      "4   US$75,000.00  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_csv_files(folder_path, skip_folders=[]):\n",
    "    \"\"\"\n",
    "    Recursively load all CSV files from the specified folder and its subfolders into Pandas DataFrames,\n",
    "    while skipping a specified folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing CSV files.\n",
    "        skip_folder (str, optional): Name of the folder to skip (relative to folder_path).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are CSV file paths (relative to folder_path)\n",
    "              and values are the corresponding Pandas DataFrames.\n",
    "    \"\"\"\n",
    "    dataframes = {}\n",
    "\n",
    "    # Walk through all directories and files\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        # Skip the specified folder\n",
    "        for skip_folder in skip_folders:\n",
    "            if skip_folder and skip_folder in dirs:\n",
    "                dirs.remove(skip_folder)  # This prevents os.walk() from descending into the folder\n",
    "\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(\"merged.csv\"):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "\n",
    "                # Create a unique name for the DataFrame (relative path without extension)\n",
    "                rel_path = os.path.relpath(file_path, folder_path)\n",
    "                dataframe_name = os.path.splitext(rel_path)[0].replace(os.sep, \"_\")\n",
    "\n",
    "                try:\n",
    "                    # Read the CSV file into a Pandas DataFrame\n",
    "                    df = pd.read_csv(file_path)\n",
    "\n",
    "                    # Store DataFrame in the dictionary\n",
    "                    dataframes[dataframe_name] = df\n",
    "\n",
    "                    print(f\"Loaded: {file_path} -> DataFrame: {dataframe_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not load: {file_path} -> DataFrame: {dataframe_name}. Error: {e}\")\n",
    "\n",
    "    return dataframes\n",
    "\n",
    "# Specify the folder containing your CSV files\n",
    "folder_path = \"data_minidev/MINIDEV/dev_databases\"\n",
    "processed = [\"california_schools\",\"financial\",\"superhero\"]\n",
    "# Load all CSV files into DataFrames\n",
    "dataframes_dict = load_csv_files(folder_path,processed)\n",
    "\n",
    "# Example: Access a specific DataFrame\n",
    "for name, df in dataframes_dict.items():\n",
    "    print(f\"DataFrame Name: {name}\")\n",
    "    print(df.head(5))  # Display the first few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Name: law_episode_merged\n",
      "Columns:\n",
      "  - award_id\n",
      "  - organization\n",
      "  - year\n",
      "  - award_category\n",
      "  - award\n",
      "  - series_x\n",
      "  - episode_id\n",
      "  - person_id_x\n",
      "  - role_x\n",
      "  - result\n",
      "  - person_id_y\n",
      "  - category\n",
      "  - role_y\n",
      "  - credited\n",
      "  - series_y\n",
      "  - season\n",
      "  - episode\n",
      "  - number_in_series\n",
      "  - title\n",
      "  - summary\n",
      "  - air_date\n",
      "  - episode_image\n",
      "  - rating\n",
      "  - votes\n",
      "\n",
      "DataFrame Name: books_merged\n",
      "Columns:\n",
      "  - book_id\n",
      "  - title\n",
      "  - isbn13\n",
      "  - language_id\n",
      "  - num_pages\n",
      "  - publication_date\n",
      "  - publisher_id\n",
      "  - author_id\n",
      "  - author_name\n",
      "  - language_code\n",
      "  - language_name\n",
      "\n",
      "DataFrame Name: computer_student_merged\n",
      "Columns:\n",
      "  - p_id\n",
      "  - professor\n",
      "  - student\n",
      "  - hasPosition\n",
      "  - inPhase\n",
      "  - yearsInProgram\n",
      "  - course_id\n",
      "  - courseLevel\n",
      "\n",
      "DataFrame Name: mental_health_survey_merged\n",
      "Columns:\n",
      "  - questiontext\n",
      "  - questionid\n",
      "  - AnswerText\n",
      "  - SurveyID\n",
      "  - UserID\n",
      "\n",
      "DataFrame Name: airline_merged\n",
      "Columns:\n",
      "  - FL_DATE\n",
      "  - OP_CARRIER_AIRLINE_ID\n",
      "  - TAIL_NUM\n",
      "  - OP_CARRIER_FL_NUM\n",
      "  - ORIGIN_AIRPORT_ID\n",
      "  - ORIGIN_AIRPORT_SEQ_ID\n",
      "  - ORIGIN_CITY_MARKET_ID\n",
      "  - ORIGIN\n",
      "  - DEST_AIRPORT_ID\n",
      "  - DEST_AIRPORT_SEQ_ID\n",
      "  - DEST_CITY_MARKET_ID\n",
      "  - DEST\n",
      "  - CRS_DEP_TIME\n",
      "  - DEP_TIME\n",
      "  - DEP_DELAY\n",
      "  - DEP_DELAY_NEW\n",
      "  - ARR_TIME\n",
      "  - ARR_DELAY\n",
      "  - ARR_DELAY_NEW\n",
      "  - CANCELLED\n",
      "  - CANCELLATION_CODE\n",
      "  - CRS_ELAPSED_TIME\n",
      "  - ACTUAL_ELAPSED_TIME\n",
      "  - CARRIER_DELAY\n",
      "  - WEATHER_DELAY\n",
      "  - NAS_DELAY\n",
      "  - SECURITY_DELAY\n",
      "  - LATE_AIRCRAFT_DELAY\n",
      "\n",
      "DataFrame Name: authors_merged\n",
      "Columns:\n",
      "  - Id\n",
      "  - Name\n",
      "  - Affiliation\n",
      "  - ShortName\n",
      "  - FullName\n",
      "  - HomePage\n",
      "\n",
      "DataFrame Name: human_resources_merged\n",
      "Columns:\n",
      "  - ssn\n",
      "  - lastname\n",
      "  - firstname\n",
      "  - hiredate\n",
      "  - salary\n",
      "  - gender\n",
      "  - performance\n",
      "  - positionID\n",
      "  - locationID\n",
      "  - locationcity\n",
      "  - address\n",
      "  - state\n",
      "  - zipcode\n",
      "  - officephone\n",
      "  - positiontitle\n",
      "  - educationrequired\n",
      "  - minsalary\n",
      "  - maxsalary\n"
     ]
    }
   ],
   "source": [
    "for name, df in dataframes_dict.items():\n",
    "    print(f\"\\nDataFrame Name: {name}\")\n",
    "    print(\"Columns:\")\n",
    "    for column in df.columns:\n",
    "        print(f\"  - {column}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - law_episode_merged\n",
      "\n",
      " - books_merged\n",
      "\n",
      " - computer_student_merged\n",
      "\n",
      " - mental_health_survey_merged\n",
      "\n",
      " - airline_merged\n",
      "\n",
      " - authors_merged\n",
      "\n",
      " - human_resources_merged\n"
     ]
    }
   ],
   "source": [
    "for name, df in dataframes_dict.items():\n",
    "    print(f\"\\n - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in dataframes_dict.items():\n",
    "    # Drop columns containing 'id', 'ID', or '_y' in their names\n",
    "    cols_to_drop = [col for col in df.columns if 'id' in col.lower() or '_y' in col]\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Rename columns by removing '_x'\n",
    "    df.columns = [col.replace('_x', '') for col in df.columns]\n",
    "\n",
    "    # Update the dictionary\n",
    "    dataframes_dict[key] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_dict['debit_card_specializing_merged'].drop([\"CustomerID\",\"TransactionID\",\"CardID\",\"GasStationID\",\"ProductID\",\"ChainID\",\"Segment_y\"],axis=1,inplace=True)\n",
    "dataframes_dict['financial_merged'].drop([\"card_id\",\"disp_id\",\"client_id\",\"district_id_x\",\"account_id\",\"district_id_y\",\"type_y\",\"order_id\",\"trans_id\",\"date_y\",\"type_x\",\"type_y\",\"amount_y\",\"k_symbol_y\"],axis=1,inplace=True)\n",
    "del dataframes_dict['thrombosis_prediction_merged']\n",
    "dataframes_dict['california_schools_merged'].drop([\"NSLP Provision Status\", \"Charter School (Y/N)\", \"Charter School Number\", \"Charter Funding Type\", \"IRC\", \"Low Grade\", \"High Grade\", \"Enrollment (K-12)\", \"Free Meal Count (K-12)\", \"Percent (%) Eligible Free (K-12)\", \"FRPM Count (K-12)\", \"Percent (%) Eligible FRPM (K-12)\", \"Enrollment (Ages 5-17)\", \"Free Meal Count (Ages 5-17)\", \"Percent (%) Eligible Free (Ages 5-17)\", \"FRPM Count (Ages 5-17)\", \"Percent (%) Eligible FRPM (Ages 5-17)\", \"2013-14 CALPADS Fall 1 Certification Status\", \"rtype\", \"sname\", \"dname\", \"cname\", \"enroll12\", \"NumTstTakr\", \"AvgScrRead\", \"AvgScrMath\", \"AvgScrWrite\", \"NumGE1500\", \"NCESDist\", \"NCESSchool\", \"StatusType\",\"Charter\", \"CharterNum\", \"FundingType\", \"DOC\", \"DOCType\", \"SOC\", \"SOCType\", \"EdOpsCode\", \"EdOpsName\", \"EILCode\", \"EILName\", \"GSoffered\", \"GSserved\", \"Virtual\", \"Magnet\", \"Latitude\", \"Longitude\", \"AdmFName1\", \"AdmLName1\", \"AdmEmail1\", \"AdmFName2\", \"AdmLName2\", \"AdmEmail2\", \"AdmFName3\", \"AdmLName3\", \"AdmEmail3\", \"LastUpdate\"],axis=1,inplace=True)\n",
    "dataframes_dict['superhero_merged'] = dataframes_dict['superhero_merged'].loc[:, ~dataframes_dict['superhero_merged'].columns.str.contains('id', case=False)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Segment_x</th>\n",
       "      <th>Currency</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Price</th>\n",
       "      <th>Country</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LAM</td>\n",
       "      <td>EUR</td>\n",
       "      <td>2012-08-24</td>\n",
       "      <td>09:26:00</td>\n",
       "      <td>29</td>\n",
       "      <td>28.17</td>\n",
       "      <td>SVK</td>\n",
       "      <td>Nat.Super</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LAM</td>\n",
       "      <td>EUR</td>\n",
       "      <td>2012-08-24</td>\n",
       "      <td>11:44:00</td>\n",
       "      <td>18</td>\n",
       "      <td>16.43</td>\n",
       "      <td>SVK</td>\n",
       "      <td>Diesel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LAM</td>\n",
       "      <td>EUR</td>\n",
       "      <td>2012-08-24</td>\n",
       "      <td>12:18:00</td>\n",
       "      <td>18</td>\n",
       "      <td>16.63</td>\n",
       "      <td>SVK</td>\n",
       "      <td>Diesel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LAM</td>\n",
       "      <td>EUR</td>\n",
       "      <td>2012-08-24</td>\n",
       "      <td>12:25:00</td>\n",
       "      <td>23</td>\n",
       "      <td>20.38</td>\n",
       "      <td>SVK</td>\n",
       "      <td>Diesel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LAM</td>\n",
       "      <td>EUR</td>\n",
       "      <td>2012-08-24</td>\n",
       "      <td>11:40:00</td>\n",
       "      <td>62</td>\n",
       "      <td>59.76</td>\n",
       "      <td>SVK</td>\n",
       "      <td>Nat.Super</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Segment_x Currency        Date      Time  Amount  Price Country Description\n",
       "0       LAM      EUR  2012-08-24  09:26:00      29  28.17     SVK   Nat.Super\n",
       "1       LAM      EUR  2012-08-24  11:44:00      18  16.43     SVK      Diesel\n",
       "2       LAM      EUR  2012-08-24  12:18:00      18  16.63     SVK      Diesel\n",
       "3       LAM      EUR  2012-08-24  12:25:00      23  20.38     SVK      Diesel\n",
       "4       LAM      EUR  2012-08-24  11:40:00      62  59.76     SVK   Nat.Super"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dataframes_dict['debit_card_specializing_merged']))\n",
    "dataframes_dict['debit_card_specializing_merged'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataframes_dict['debit_card_specializing_merged']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352697\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issued</th>\n",
       "      <th>gender</th>\n",
       "      <th>birth_date</th>\n",
       "      <th>frequency</th>\n",
       "      <th>date_x</th>\n",
       "      <th>bank_to</th>\n",
       "      <th>account_to</th>\n",
       "      <th>amount_x</th>\n",
       "      <th>k_symbol_x</th>\n",
       "      <th>type</th>\n",
       "      <th>operation</th>\n",
       "      <th>balance</th>\n",
       "      <th>bank</th>\n",
       "      <th>account</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1998-10-16</td>\n",
       "      <td>M</td>\n",
       "      <td>1935-10-16</td>\n",
       "      <td>POPLATEK MESICNE</td>\n",
       "      <td>1996-11-24</td>\n",
       "      <td>IJ</td>\n",
       "      <td>34452903</td>\n",
       "      <td>4880.0</td>\n",
       "      <td>SIPO</td>\n",
       "      <td>PRIJEM</td>\n",
       "      <td>VKLAD</td>\n",
       "      <td>900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1998-10-16</td>\n",
       "      <td>M</td>\n",
       "      <td>1935-10-16</td>\n",
       "      <td>POPLATEK MESICNE</td>\n",
       "      <td>1996-11-24</td>\n",
       "      <td>IJ</td>\n",
       "      <td>34452903</td>\n",
       "      <td>4880.0</td>\n",
       "      <td>SIPO</td>\n",
       "      <td>PRIJEM</td>\n",
       "      <td>VKLAD</td>\n",
       "      <td>34875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1998-10-16</td>\n",
       "      <td>M</td>\n",
       "      <td>1935-10-16</td>\n",
       "      <td>POPLATEK MESICNE</td>\n",
       "      <td>1996-11-24</td>\n",
       "      <td>IJ</td>\n",
       "      <td>34452903</td>\n",
       "      <td>4880.0</td>\n",
       "      <td>SIPO</td>\n",
       "      <td>PRIJEM</td>\n",
       "      <td>VKLAD</td>\n",
       "      <td>50955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1998-10-16</td>\n",
       "      <td>M</td>\n",
       "      <td>1935-10-16</td>\n",
       "      <td>POPLATEK MESICNE</td>\n",
       "      <td>1996-11-24</td>\n",
       "      <td>IJ</td>\n",
       "      <td>34452903</td>\n",
       "      <td>4880.0</td>\n",
       "      <td>SIPO</td>\n",
       "      <td>PRIJEM</td>\n",
       "      <td>VKLAD</td>\n",
       "      <td>61436</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1998-10-16</td>\n",
       "      <td>M</td>\n",
       "      <td>1935-10-16</td>\n",
       "      <td>POPLATEK MESICNE</td>\n",
       "      <td>1996-11-24</td>\n",
       "      <td>IJ</td>\n",
       "      <td>34452903</td>\n",
       "      <td>4880.0</td>\n",
       "      <td>SIPO</td>\n",
       "      <td>PRIJEM</td>\n",
       "      <td>VKLAD</td>\n",
       "      <td>69755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       issued gender  birth_date         frequency      date_x bank_to  \\\n",
       "0  1998-10-16      M  1935-10-16  POPLATEK MESICNE  1996-11-24      IJ   \n",
       "1  1998-10-16      M  1935-10-16  POPLATEK MESICNE  1996-11-24      IJ   \n",
       "2  1998-10-16      M  1935-10-16  POPLATEK MESICNE  1996-11-24      IJ   \n",
       "3  1998-10-16      M  1935-10-16  POPLATEK MESICNE  1996-11-24      IJ   \n",
       "4  1998-10-16      M  1935-10-16  POPLATEK MESICNE  1996-11-24      IJ   \n",
       "\n",
       "   account_to  amount_x k_symbol_x    type operation  balance bank  account  \n",
       "0    34452903    4880.0       SIPO  PRIJEM     VKLAD      900  NaN      NaN  \n",
       "1    34452903    4880.0       SIPO  PRIJEM     VKLAD    34875  NaN      NaN  \n",
       "2    34452903    4880.0       SIPO  PRIJEM     VKLAD    50955  NaN      NaN  \n",
       "3    34452903    4880.0       SIPO  PRIJEM     VKLAD    61436  NaN      NaN  \n",
       "4    34452903    4880.0       SIPO  PRIJEM     VKLAD    69755  NaN      NaN  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dataframes_dict['financial_merged']))\n",
    "dataframes_dict['financial_merged'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_dict['financial_merged'].rename(columns={\"date_x\": \"date\", \"amount_x\": \"amount\",\"k_symbol_x\" : \"symbol\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1782\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CDSCode</th>\n",
       "      <th>Academic Year</th>\n",
       "      <th>County Code</th>\n",
       "      <th>District Code</th>\n",
       "      <th>School Code</th>\n",
       "      <th>County Name</th>\n",
       "      <th>District Name</th>\n",
       "      <th>School Name</th>\n",
       "      <th>District Type</th>\n",
       "      <th>School Type</th>\n",
       "      <th>...</th>\n",
       "      <th>MailStreet</th>\n",
       "      <th>MailStrAbr</th>\n",
       "      <th>MailCity</th>\n",
       "      <th>MailZip</th>\n",
       "      <th>MailState</th>\n",
       "      <th>Phone</th>\n",
       "      <th>Ext</th>\n",
       "      <th>Website</th>\n",
       "      <th>OpenDate</th>\n",
       "      <th>ClosedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1100170109835</td>\n",
       "      <td>2014-2015</td>\n",
       "      <td>1</td>\n",
       "      <td>10017</td>\n",
       "      <td>109835</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>Alameda County Office of Education</td>\n",
       "      <td>FAME Public Charter</td>\n",
       "      <td>County Office of Education (COE)</td>\n",
       "      <td>K-12 Schools (Public)</td>\n",
       "      <td>...</td>\n",
       "      <td>39899 Balentine Drive, Suite 335</td>\n",
       "      <td>39899 Balentine Dr., Ste. 335</td>\n",
       "      <td>Newark</td>\n",
       "      <td>94560-5359</td>\n",
       "      <td>CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2005-08-29</td>\n",
       "      <td>2015-07-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1100170112607</td>\n",
       "      <td>2014-2015</td>\n",
       "      <td>1</td>\n",
       "      <td>10017</td>\n",
       "      <td>112607</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>Alameda County Office of Education</td>\n",
       "      <td>Envision Academy for Arts &amp; Technology</td>\n",
       "      <td>County Office of Education (COE)</td>\n",
       "      <td>High Schools (Public)</td>\n",
       "      <td>...</td>\n",
       "      <td>1515 Webster Street</td>\n",
       "      <td>1515 Webster St.</td>\n",
       "      <td>Oakland</td>\n",
       "      <td>94612</td>\n",
       "      <td>CA</td>\n",
       "      <td>(510) 596-8901</td>\n",
       "      <td>NaN</td>\n",
       "      <td>www.envisionacademy.org/</td>\n",
       "      <td>2006-08-28</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1100170118489</td>\n",
       "      <td>2014-2015</td>\n",
       "      <td>1</td>\n",
       "      <td>10017</td>\n",
       "      <td>118489</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>Alameda County Office of Education</td>\n",
       "      <td>Aspire California College Preparatory Academy</td>\n",
       "      <td>County Office of Education (COE)</td>\n",
       "      <td>High Schools (Public)</td>\n",
       "      <td>...</td>\n",
       "      <td>1001 22nd Avenue, Suite 100</td>\n",
       "      <td>1001 22nd Ave., Ste. 100</td>\n",
       "      <td>Oakland</td>\n",
       "      <td>94606</td>\n",
       "      <td>CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>www.aspirepublicschools.org</td>\n",
       "      <td>2008-08-21</td>\n",
       "      <td>2015-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1611190106401</td>\n",
       "      <td>2014-2015</td>\n",
       "      <td>1</td>\n",
       "      <td>61119</td>\n",
       "      <td>106401</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>Alameda Unified</td>\n",
       "      <td>Alameda Science and Technology Institute</td>\n",
       "      <td>Unified School District</td>\n",
       "      <td>Alternative Schools of Choice</td>\n",
       "      <td>...</td>\n",
       "      <td>555 Atlantic Avenue</td>\n",
       "      <td>555 Atlantic Ave.</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>94501-2109</td>\n",
       "      <td>CA</td>\n",
       "      <td>(510) 748-4021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004-08-19</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1611190119222</td>\n",
       "      <td>2014-2015</td>\n",
       "      <td>1</td>\n",
       "      <td>61119</td>\n",
       "      <td>119222</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>Alameda Unified</td>\n",
       "      <td>Nea Community Learning Center</td>\n",
       "      <td>Unified School District</td>\n",
       "      <td>K-12 Schools (Public)</td>\n",
       "      <td>...</td>\n",
       "      <td>1900 Third Street</td>\n",
       "      <td>1900 Third St.</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>94501-1851</td>\n",
       "      <td>CA</td>\n",
       "      <td>(510) 748-4008</td>\n",
       "      <td>130.0</td>\n",
       "      <td>http://neaclc.org</td>\n",
       "      <td>2009-08-31</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CDSCode Academic Year  County Code  District Code  School Code  \\\n",
       "0  1100170109835     2014-2015            1          10017       109835   \n",
       "1  1100170112607     2014-2015            1          10017       112607   \n",
       "2  1100170118489     2014-2015            1          10017       118489   \n",
       "3  1611190106401     2014-2015            1          61119       106401   \n",
       "4  1611190119222     2014-2015            1          61119       119222   \n",
       "\n",
       "  County Name                       District Name  \\\n",
       "0     Alameda  Alameda County Office of Education   \n",
       "1     Alameda  Alameda County Office of Education   \n",
       "2     Alameda  Alameda County Office of Education   \n",
       "3     Alameda                     Alameda Unified   \n",
       "4     Alameda                     Alameda Unified   \n",
       "\n",
       "                                     School Name  \\\n",
       "0                            FAME Public Charter   \n",
       "1         Envision Academy for Arts & Technology   \n",
       "2  Aspire California College Preparatory Academy   \n",
       "3       Alameda Science and Technology Institute   \n",
       "4                  Nea Community Learning Center   \n",
       "\n",
       "                      District Type                    School Type  ...  \\\n",
       "0  County Office of Education (COE)          K-12 Schools (Public)  ...   \n",
       "1  County Office of Education (COE)          High Schools (Public)  ...   \n",
       "2  County Office of Education (COE)          High Schools (Public)  ...   \n",
       "3           Unified School District  Alternative Schools of Choice  ...   \n",
       "4           Unified School District          K-12 Schools (Public)  ...   \n",
       "\n",
       "                         MailStreet                     MailStrAbr MailCity  \\\n",
       "0  39899 Balentine Drive, Suite 335  39899 Balentine Dr., Ste. 335   Newark   \n",
       "1               1515 Webster Street               1515 Webster St.  Oakland   \n",
       "2       1001 22nd Avenue, Suite 100       1001 22nd Ave., Ste. 100  Oakland   \n",
       "3               555 Atlantic Avenue              555 Atlantic Ave.  Alameda   \n",
       "4                 1900 Third Street                 1900 Third St.  Alameda   \n",
       "\n",
       "      MailZip MailState           Phone    Ext                      Website  \\\n",
       "0  94560-5359        CA             NaN    NaN                          NaN   \n",
       "1       94612        CA  (510) 596-8901    NaN     www.envisionacademy.org/   \n",
       "2       94606        CA             NaN    NaN  www.aspirepublicschools.org   \n",
       "3  94501-2109        CA  (510) 748-4021    NaN                          NaN   \n",
       "4  94501-1851        CA  (510) 748-4008  130.0            http://neaclc.org   \n",
       "\n",
       "     OpenDate  ClosedDate  \n",
       "0  2005-08-29  2015-07-31  \n",
       "1  2006-08-28         NaN  \n",
       "2  2008-08-21  2015-06-30  \n",
       "3  2004-08-19         NaN  \n",
       "4  2009-08-31         NaN  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dataframes_dict['california_schools_merged']))\n",
    "dataframes_dict['california_schools_merged'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "737\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>superhero_name</th>\n",
       "      <th>full_name</th>\n",
       "      <th>height_cm</th>\n",
       "      <th>weight_kg</th>\n",
       "      <th>alignment</th>\n",
       "      <th>race</th>\n",
       "      <th>publisher_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>colour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3-D Man</td>\n",
       "      <td>Charles Chandler</td>\n",
       "      <td>188.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Good</td>\n",
       "      <td>-</td>\n",
       "      <td>Marvel Comics</td>\n",
       "      <td>Male</td>\n",
       "      <td>Grey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A-Bomb</td>\n",
       "      <td>Richard Milhouse Jones</td>\n",
       "      <td>203.0</td>\n",
       "      <td>441.0</td>\n",
       "      <td>Good</td>\n",
       "      <td>Human</td>\n",
       "      <td>Marvel Comics</td>\n",
       "      <td>Male</td>\n",
       "      <td>No Colour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abe Sapien</td>\n",
       "      <td>Abraham Sapien</td>\n",
       "      <td>191.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>Good</td>\n",
       "      <td>Icthyo Sapien</td>\n",
       "      <td>Dark Horse Comics</td>\n",
       "      <td>Male</td>\n",
       "      <td>No Colour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abin Sur</td>\n",
       "      <td>-</td>\n",
       "      <td>185.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Good</td>\n",
       "      <td>Ungaran</td>\n",
       "      <td>DC Comics</td>\n",
       "      <td>Male</td>\n",
       "      <td>No Colour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abomination</td>\n",
       "      <td>Emil Blonsky</td>\n",
       "      <td>203.0</td>\n",
       "      <td>441.0</td>\n",
       "      <td>Bad</td>\n",
       "      <td>Human / Radiation</td>\n",
       "      <td>Marvel Comics</td>\n",
       "      <td>Male</td>\n",
       "      <td>No Colour</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  superhero_name               full_name  height_cm  weight_kg alignment  \\\n",
       "0        3-D Man        Charles Chandler      188.0       90.0      Good   \n",
       "1         A-Bomb  Richard Milhouse Jones      203.0      441.0      Good   \n",
       "2     Abe Sapien          Abraham Sapien      191.0       65.0      Good   \n",
       "3       Abin Sur                       -      185.0       90.0      Good   \n",
       "4    Abomination            Emil Blonsky      203.0      441.0       Bad   \n",
       "\n",
       "                race     publisher_name gender     colour  \n",
       "0                  -      Marvel Comics   Male       Grey  \n",
       "1              Human      Marvel Comics   Male  No Colour  \n",
       "2      Icthyo Sapien  Dark Horse Comics   Male  No Colour  \n",
       "3            Ungaran          DC Comics   Male  No Colour  \n",
       "4  Human / Radiation      Marvel Comics   Male  No Colour  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dataframes_dict['superhero_merged']))\n",
    "dataframes_dict['superhero_merged'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2273155/4255919130.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['index'] = range(1, len(df_selected) + 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>issued</th>\n",
       "      <th>gender</th>\n",
       "      <th>birth_date</th>\n",
       "      <th>frequency</th>\n",
       "      <th>date</th>\n",
       "      <th>bank_to</th>\n",
       "      <th>account_to</th>\n",
       "      <th>amount</th>\n",
       "      <th>symbol</th>\n",
       "      <th>type</th>\n",
       "      <th>operation</th>\n",
       "      <th>balance</th>\n",
       "      <th>bank</th>\n",
       "      <th>account</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>1998-10-16</td>\n",
       "      <td>M</td>\n",
       "      <td>1935-10-16</td>\n",
       "      <td>POPLATEK MESICNE</td>\n",
       "      <td>1996-11-24</td>\n",
       "      <td>IJ</td>\n",
       "      <td>34452903</td>\n",
       "      <td>4880.0</td>\n",
       "      <td>SIPO</td>\n",
       "      <td>VYDAJ</td>\n",
       "      <td>PREVOD NA UCET</td>\n",
       "      <td>46708</td>\n",
       "      <td>IJ</td>\n",
       "      <td>34452903.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>1998-10-16</td>\n",
       "      <td>M</td>\n",
       "      <td>1935-10-16</td>\n",
       "      <td>POPLATEK MESICNE</td>\n",
       "      <td>1996-11-24</td>\n",
       "      <td>IJ</td>\n",
       "      <td>34452903</td>\n",
       "      <td>4880.0</td>\n",
       "      <td>SIPO</td>\n",
       "      <td>VYDAJ</td>\n",
       "      <td>PREVOD NA UCET</td>\n",
       "      <td>42813</td>\n",
       "      <td>IJ</td>\n",
       "      <td>34452903.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>1998-10-16</td>\n",
       "      <td>M</td>\n",
       "      <td>1935-10-16</td>\n",
       "      <td>POPLATEK MESICNE</td>\n",
       "      <td>1996-11-24</td>\n",
       "      <td>IJ</td>\n",
       "      <td>34452903</td>\n",
       "      <td>4880.0</td>\n",
       "      <td>SIPO</td>\n",
       "      <td>VYDAJ</td>\n",
       "      <td>PREVOD NA UCET</td>\n",
       "      <td>47859</td>\n",
       "      <td>IJ</td>\n",
       "      <td>34452903.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>1998-10-16</td>\n",
       "      <td>M</td>\n",
       "      <td>1935-10-16</td>\n",
       "      <td>POPLATEK MESICNE</td>\n",
       "      <td>1996-11-24</td>\n",
       "      <td>IJ</td>\n",
       "      <td>34452903</td>\n",
       "      <td>4880.0</td>\n",
       "      <td>SIPO</td>\n",
       "      <td>VYDAJ</td>\n",
       "      <td>PREVOD NA UCET</td>\n",
       "      <td>43505</td>\n",
       "      <td>IJ</td>\n",
       "      <td>34452903.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5</td>\n",
       "      <td>1998-10-16</td>\n",
       "      <td>M</td>\n",
       "      <td>1935-10-16</td>\n",
       "      <td>POPLATEK MESICNE</td>\n",
       "      <td>1996-11-24</td>\n",
       "      <td>IJ</td>\n",
       "      <td>34452903</td>\n",
       "      <td>4880.0</td>\n",
       "      <td>SIPO</td>\n",
       "      <td>VYDAJ</td>\n",
       "      <td>PREVOD NA UCET</td>\n",
       "      <td>45298</td>\n",
       "      <td>IJ</td>\n",
       "      <td>34452903.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>116</td>\n",
       "      <td>1998-11-26</td>\n",
       "      <td>M</td>\n",
       "      <td>1935-08-17</td>\n",
       "      <td>POPLATEK TYDNE</td>\n",
       "      <td>1993-09-17</td>\n",
       "      <td>WX</td>\n",
       "      <td>21845197</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>LEASING</td>\n",
       "      <td>PRIJEM</td>\n",
       "      <td>PREVOD Z UCTU</td>\n",
       "      <td>67777</td>\n",
       "      <td>EF</td>\n",
       "      <td>2569228.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>117</td>\n",
       "      <td>1998-11-26</td>\n",
       "      <td>M</td>\n",
       "      <td>1935-08-17</td>\n",
       "      <td>POPLATEK TYDNE</td>\n",
       "      <td>1993-09-17</td>\n",
       "      <td>WX</td>\n",
       "      <td>21845197</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>LEASING</td>\n",
       "      <td>PRIJEM</td>\n",
       "      <td>PREVOD Z UCTU</td>\n",
       "      <td>93261</td>\n",
       "      <td>EF</td>\n",
       "      <td>2569228.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>118</td>\n",
       "      <td>1998-11-26</td>\n",
       "      <td>M</td>\n",
       "      <td>1935-08-17</td>\n",
       "      <td>POPLATEK TYDNE</td>\n",
       "      <td>1993-09-17</td>\n",
       "      <td>WX</td>\n",
       "      <td>21845197</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>LEASING</td>\n",
       "      <td>PRIJEM</td>\n",
       "      <td>PREVOD Z UCTU</td>\n",
       "      <td>72036</td>\n",
       "      <td>EF</td>\n",
       "      <td>2569228.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>119</td>\n",
       "      <td>1998-11-26</td>\n",
       "      <td>M</td>\n",
       "      <td>1935-08-17</td>\n",
       "      <td>POPLATEK TYDNE</td>\n",
       "      <td>1993-09-17</td>\n",
       "      <td>WX</td>\n",
       "      <td>21845197</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>LEASING</td>\n",
       "      <td>PRIJEM</td>\n",
       "      <td>PREVOD Z UCTU</td>\n",
       "      <td>72624</td>\n",
       "      <td>EF</td>\n",
       "      <td>2569228.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>120</td>\n",
       "      <td>1998-11-26</td>\n",
       "      <td>M</td>\n",
       "      <td>1935-08-17</td>\n",
       "      <td>POPLATEK TYDNE</td>\n",
       "      <td>1993-09-17</td>\n",
       "      <td>WX</td>\n",
       "      <td>21845197</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>LEASING</td>\n",
       "      <td>PRIJEM</td>\n",
       "      <td>PREVOD Z UCTU</td>\n",
       "      <td>83942</td>\n",
       "      <td>EF</td>\n",
       "      <td>2569228.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index      issued gender  birth_date         frequency        date  \\\n",
       "26       1  1998-10-16      M  1935-10-16  POPLATEK MESICNE  1996-11-24   \n",
       "27       2  1998-10-16      M  1935-10-16  POPLATEK MESICNE  1996-11-24   \n",
       "28       3  1998-10-16      M  1935-10-16  POPLATEK MESICNE  1996-11-24   \n",
       "29       4  1998-10-16      M  1935-10-16  POPLATEK MESICNE  1996-11-24   \n",
       "30       5  1998-10-16      M  1935-10-16  POPLATEK MESICNE  1996-11-24   \n",
       "..     ...         ...    ...         ...               ...         ...   \n",
       "615    116  1998-11-26      M  1935-08-17    POPLATEK TYDNE  1993-09-17   \n",
       "616    117  1998-11-26      M  1935-08-17    POPLATEK TYDNE  1993-09-17   \n",
       "617    118  1998-11-26      M  1935-08-17    POPLATEK TYDNE  1993-09-17   \n",
       "618    119  1998-11-26      M  1935-08-17    POPLATEK TYDNE  1993-09-17   \n",
       "619    120  1998-11-26      M  1935-08-17    POPLATEK TYDNE  1993-09-17   \n",
       "\n",
       "    bank_to  account_to  amount   symbol    type       operation  balance  \\\n",
       "26       IJ    34452903  4880.0     SIPO   VYDAJ  PREVOD NA UCET    46708   \n",
       "27       IJ    34452903  4880.0     SIPO   VYDAJ  PREVOD NA UCET    42813   \n",
       "28       IJ    34452903  4880.0     SIPO   VYDAJ  PREVOD NA UCET    47859   \n",
       "29       IJ    34452903  4880.0     SIPO   VYDAJ  PREVOD NA UCET    43505   \n",
       "30       IJ    34452903  4880.0     SIPO   VYDAJ  PREVOD NA UCET    45298   \n",
       "..      ...         ...     ...      ...     ...             ...      ...   \n",
       "615      WX    21845197  1479.0  LEASING  PRIJEM   PREVOD Z UCTU    67777   \n",
       "616      WX    21845197  1479.0  LEASING  PRIJEM   PREVOD Z UCTU    93261   \n",
       "617      WX    21845197  1479.0  LEASING  PRIJEM   PREVOD Z UCTU    72036   \n",
       "618      WX    21845197  1479.0  LEASING  PRIJEM   PREVOD Z UCTU    72624   \n",
       "619      WX    21845197  1479.0  LEASING  PRIJEM   PREVOD Z UCTU    83942   \n",
       "\n",
       "    bank     account  \n",
       "26    IJ  34452903.0  \n",
       "27    IJ  34452903.0  \n",
       "28    IJ  34452903.0  \n",
       "29    IJ  34452903.0  \n",
       "30    IJ  34452903.0  \n",
       "..   ...         ...  \n",
       "615   EF   2569228.0  \n",
       "616   EF   2569228.0  \n",
       "617   EF   2569228.0  \n",
       "618   EF   2569228.0  \n",
       "619   EF   2569228.0  \n",
       "\n",
       "[120 rows x 15 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = dataframes_dict['financial_merged'].dropna()\n",
    "\n",
    "# Step 2: Select the first 120 rows from the cleaned DataFrame\n",
    "df_selected = df_clean.head(120)\n",
    "df_selected['index'] = range(1, len(df_selected) + 1)\n",
    "dataframes_dict['financial_merged'] = df_selected\n",
    "# Move 'index' column to the beginning\n",
    "dataframes_dict['financial_merged'] = dataframes_dict['financial_merged'][['index'] + [col for col in dataframes_dict['financial_merged'].columns if col != 'index']]\n",
    "\n",
    "dataframes_dict['financial_merged']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2273155/1399687790.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['index'] = range(1, len(df_selected) + 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>CDSCode</th>\n",
       "      <th>Academic Year</th>\n",
       "      <th>County Code</th>\n",
       "      <th>District Code</th>\n",
       "      <th>School Code</th>\n",
       "      <th>County Name</th>\n",
       "      <th>District Name</th>\n",
       "      <th>School Name</th>\n",
       "      <th>District Type</th>\n",
       "      <th>...</th>\n",
       "      <th>Zip</th>\n",
       "      <th>State</th>\n",
       "      <th>MailStreet</th>\n",
       "      <th>MailStrAbr</th>\n",
       "      <th>MailCity</th>\n",
       "      <th>MailZip</th>\n",
       "      <th>MailState</th>\n",
       "      <th>Phone</th>\n",
       "      <th>Website</th>\n",
       "      <th>OpenDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1100170109835</td>\n",
       "      <td>2014-2015</td>\n",
       "      <td>1</td>\n",
       "      <td>10017</td>\n",
       "      <td>109835</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>Alameda County Office of Education</td>\n",
       "      <td>FAME Public Charter</td>\n",
       "      <td>County Office of Education (COE)</td>\n",
       "      <td>...</td>\n",
       "      <td>94560-5359</td>\n",
       "      <td>CA</td>\n",
       "      <td>39899 Balentine Drive, Suite 335</td>\n",
       "      <td>39899 Balentine Dr., Ste. 335</td>\n",
       "      <td>Newark</td>\n",
       "      <td>94560-5359</td>\n",
       "      <td>CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2005-08-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1100170112607</td>\n",
       "      <td>2014-2015</td>\n",
       "      <td>1</td>\n",
       "      <td>10017</td>\n",
       "      <td>112607</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>Alameda County Office of Education</td>\n",
       "      <td>Envision Academy for Arts &amp; Technology</td>\n",
       "      <td>County Office of Education (COE)</td>\n",
       "      <td>...</td>\n",
       "      <td>94612-3355</td>\n",
       "      <td>CA</td>\n",
       "      <td>1515 Webster Street</td>\n",
       "      <td>1515 Webster St.</td>\n",
       "      <td>Oakland</td>\n",
       "      <td>94612</td>\n",
       "      <td>CA</td>\n",
       "      <td>(510) 596-8901</td>\n",
       "      <td>www.envisionacademy.org/</td>\n",
       "      <td>2006-08-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1100170118489</td>\n",
       "      <td>2014-2015</td>\n",
       "      <td>1</td>\n",
       "      <td>10017</td>\n",
       "      <td>118489</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>Alameda County Office of Education</td>\n",
       "      <td>Aspire California College Preparatory Academy</td>\n",
       "      <td>County Office of Education (COE)</td>\n",
       "      <td>...</td>\n",
       "      <td>94703-1414</td>\n",
       "      <td>CA</td>\n",
       "      <td>1001 22nd Avenue, Suite 100</td>\n",
       "      <td>1001 22nd Ave., Ste. 100</td>\n",
       "      <td>Oakland</td>\n",
       "      <td>94606</td>\n",
       "      <td>CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>www.aspirepublicschools.org</td>\n",
       "      <td>2008-08-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1611190106401</td>\n",
       "      <td>2014-2015</td>\n",
       "      <td>1</td>\n",
       "      <td>61119</td>\n",
       "      <td>106401</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>Alameda Unified</td>\n",
       "      <td>Alameda Science and Technology Institute</td>\n",
       "      <td>Unified School District</td>\n",
       "      <td>...</td>\n",
       "      <td>94501-2109</td>\n",
       "      <td>CA</td>\n",
       "      <td>555 Atlantic Avenue</td>\n",
       "      <td>555 Atlantic Ave.</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>94501-2109</td>\n",
       "      <td>CA</td>\n",
       "      <td>(510) 748-4021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004-08-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1611190119222</td>\n",
       "      <td>2014-2015</td>\n",
       "      <td>1</td>\n",
       "      <td>61119</td>\n",
       "      <td>119222</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>Alameda Unified</td>\n",
       "      <td>Nea Community Learning Center</td>\n",
       "      <td>Unified School District</td>\n",
       "      <td>...</td>\n",
       "      <td>94501-1851</td>\n",
       "      <td>CA</td>\n",
       "      <td>1900 Third Street</td>\n",
       "      <td>1900 Third St.</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>94501-1851</td>\n",
       "      <td>CA</td>\n",
       "      <td>(510) 748-4008</td>\n",
       "      <td>http://neaclc.org</td>\n",
       "      <td>2009-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>116</td>\n",
       "      <td>7616480730465</td>\n",
       "      <td>2014-2015</td>\n",
       "      <td>7</td>\n",
       "      <td>61648</td>\n",
       "      <td>730465</td>\n",
       "      <td>Contra Costa</td>\n",
       "      <td>Antioch Unified</td>\n",
       "      <td>Deer Valley High</td>\n",
       "      <td>Unified School District</td>\n",
       "      <td>...</td>\n",
       "      <td>94531-8486</td>\n",
       "      <td>CA</td>\n",
       "      <td>4700 Lone Tree Way</td>\n",
       "      <td>4700 Lone Tree Way</td>\n",
       "      <td>Antioch</td>\n",
       "      <td>94531-8486</td>\n",
       "      <td>CA</td>\n",
       "      <td>(925) 776-5555</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1996-08-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>117</td>\n",
       "      <td>7616480730861</td>\n",
       "      <td>2014-2015</td>\n",
       "      <td>7</td>\n",
       "      <td>61648</td>\n",
       "      <td>730861</td>\n",
       "      <td>Contra Costa</td>\n",
       "      <td>Antioch Unified</td>\n",
       "      <td>Antioch High</td>\n",
       "      <td>Unified School District</td>\n",
       "      <td>...</td>\n",
       "      <td>94509-1576</td>\n",
       "      <td>CA</td>\n",
       "      <td>700 West 18th Street</td>\n",
       "      <td>700 West 18th St.</td>\n",
       "      <td>Antioch</td>\n",
       "      <td>94509-1576</td>\n",
       "      <td>CA</td>\n",
       "      <td>(925) 779-7550</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1980-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>118</td>\n",
       "      <td>7616630130930</td>\n",
       "      <td>2014-2015</td>\n",
       "      <td>7</td>\n",
       "      <td>61663</td>\n",
       "      <td>130930</td>\n",
       "      <td>Contra Costa</td>\n",
       "      <td>Byron Union Elementary</td>\n",
       "      <td>Vista Oaks Charter</td>\n",
       "      <td>Elementary School District</td>\n",
       "      <td>...</td>\n",
       "      <td>94514-2515</td>\n",
       "      <td>CA</td>\n",
       "      <td>315 South Lower Sacramento Rd Suite A</td>\n",
       "      <td>315 South Lower Sacramento Rd Ste. A</td>\n",
       "      <td>Lodi</td>\n",
       "      <td>95242</td>\n",
       "      <td>CA</td>\n",
       "      <td>(209) 365-4060</td>\n",
       "      <td>www.vistaoaks.net</td>\n",
       "      <td>2014-08-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>119</td>\n",
       "      <td>7616970737023</td>\n",
       "      <td>2014-2015</td>\n",
       "      <td>7</td>\n",
       "      <td>61697</td>\n",
       "      <td>737023</td>\n",
       "      <td>Contra Costa</td>\n",
       "      <td>John Swett Unified</td>\n",
       "      <td>John Swett High</td>\n",
       "      <td>Unified School District</td>\n",
       "      <td>...</td>\n",
       "      <td>94525-1426</td>\n",
       "      <td>CA</td>\n",
       "      <td>1098 Pomona Street</td>\n",
       "      <td>1098 Pomona St.</td>\n",
       "      <td>Crockett</td>\n",
       "      <td>94525-1426</td>\n",
       "      <td>CA</td>\n",
       "      <td>(510) 787-1088</td>\n",
       "      <td>www.jsusd.org</td>\n",
       "      <td>1980-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>120</td>\n",
       "      <td>7617210107797</td>\n",
       "      <td>2014-2015</td>\n",
       "      <td>7</td>\n",
       "      <td>61721</td>\n",
       "      <td>107797</td>\n",
       "      <td>Contra Costa</td>\n",
       "      <td>Liberty Union High</td>\n",
       "      <td>Heritage High</td>\n",
       "      <td>High School District</td>\n",
       "      <td>...</td>\n",
       "      <td>94513-4604</td>\n",
       "      <td>CA</td>\n",
       "      <td>101 American Avenue</td>\n",
       "      <td>101 American Ave.</td>\n",
       "      <td>Brentwood</td>\n",
       "      <td>94513-4604</td>\n",
       "      <td>CA</td>\n",
       "      <td>(925) 634-0037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2005-08-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index        CDSCode Academic Year  County Code  District Code  \\\n",
       "0        1  1100170109835     2014-2015            1          10017   \n",
       "1        2  1100170112607     2014-2015            1          10017   \n",
       "2        3  1100170118489     2014-2015            1          10017   \n",
       "3        4  1611190106401     2014-2015            1          61119   \n",
       "4        5  1611190119222     2014-2015            1          61119   \n",
       "..     ...            ...           ...          ...            ...   \n",
       "115    116  7616480730465     2014-2015            7          61648   \n",
       "116    117  7616480730861     2014-2015            7          61648   \n",
       "117    118  7616630130930     2014-2015            7          61663   \n",
       "118    119  7616970737023     2014-2015            7          61697   \n",
       "119    120  7617210107797     2014-2015            7          61721   \n",
       "\n",
       "     School Code   County Name                       District Name  \\\n",
       "0         109835       Alameda  Alameda County Office of Education   \n",
       "1         112607       Alameda  Alameda County Office of Education   \n",
       "2         118489       Alameda  Alameda County Office of Education   \n",
       "3         106401       Alameda                     Alameda Unified   \n",
       "4         119222       Alameda                     Alameda Unified   \n",
       "..           ...           ...                                 ...   \n",
       "115       730465  Contra Costa                     Antioch Unified   \n",
       "116       730861  Contra Costa                     Antioch Unified   \n",
       "117       130930  Contra Costa              Byron Union Elementary   \n",
       "118       737023  Contra Costa                  John Swett Unified   \n",
       "119       107797  Contra Costa                  Liberty Union High   \n",
       "\n",
       "                                       School Name  \\\n",
       "0                              FAME Public Charter   \n",
       "1           Envision Academy for Arts & Technology   \n",
       "2    Aspire California College Preparatory Academy   \n",
       "3         Alameda Science and Technology Institute   \n",
       "4                    Nea Community Learning Center   \n",
       "..                                             ...   \n",
       "115                               Deer Valley High   \n",
       "116                                   Antioch High   \n",
       "117                             Vista Oaks Charter   \n",
       "118                                John Swett High   \n",
       "119                                  Heritage High   \n",
       "\n",
       "                        District Type  ...         Zip State  \\\n",
       "0    County Office of Education (COE)  ...  94560-5359    CA   \n",
       "1    County Office of Education (COE)  ...  94612-3355    CA   \n",
       "2    County Office of Education (COE)  ...  94703-1414    CA   \n",
       "3             Unified School District  ...  94501-2109    CA   \n",
       "4             Unified School District  ...  94501-1851    CA   \n",
       "..                                ...  ...         ...   ...   \n",
       "115           Unified School District  ...  94531-8486    CA   \n",
       "116           Unified School District  ...  94509-1576    CA   \n",
       "117        Elementary School District  ...  94514-2515    CA   \n",
       "118           Unified School District  ...  94525-1426    CA   \n",
       "119              High School District  ...  94513-4604    CA   \n",
       "\n",
       "                                MailStreet  \\\n",
       "0         39899 Balentine Drive, Suite 335   \n",
       "1                      1515 Webster Street   \n",
       "2              1001 22nd Avenue, Suite 100   \n",
       "3                      555 Atlantic Avenue   \n",
       "4                        1900 Third Street   \n",
       "..                                     ...   \n",
       "115                     4700 Lone Tree Way   \n",
       "116                   700 West 18th Street   \n",
       "117  315 South Lower Sacramento Rd Suite A   \n",
       "118                     1098 Pomona Street   \n",
       "119                    101 American Avenue   \n",
       "\n",
       "                               MailStrAbr   MailCity     MailZip MailState  \\\n",
       "0           39899 Balentine Dr., Ste. 335     Newark  94560-5359        CA   \n",
       "1                        1515 Webster St.    Oakland       94612        CA   \n",
       "2                1001 22nd Ave., Ste. 100    Oakland       94606        CA   \n",
       "3                       555 Atlantic Ave.    Alameda  94501-2109        CA   \n",
       "4                          1900 Third St.    Alameda  94501-1851        CA   \n",
       "..                                    ...        ...         ...       ...   \n",
       "115                    4700 Lone Tree Way    Antioch  94531-8486        CA   \n",
       "116                     700 West 18th St.    Antioch  94509-1576        CA   \n",
       "117  315 South Lower Sacramento Rd Ste. A       Lodi       95242        CA   \n",
       "118                       1098 Pomona St.   Crockett  94525-1426        CA   \n",
       "119                     101 American Ave.  Brentwood  94513-4604        CA   \n",
       "\n",
       "              Phone                      Website    OpenDate  \n",
       "0               NaN                          NaN  2005-08-29  \n",
       "1    (510) 596-8901     www.envisionacademy.org/  2006-08-28  \n",
       "2               NaN  www.aspirepublicschools.org  2008-08-21  \n",
       "3    (510) 748-4021                          NaN  2004-08-19  \n",
       "4    (510) 748-4008            http://neaclc.org  2009-08-31  \n",
       "..              ...                          ...         ...  \n",
       "115  (925) 776-5555                          NaN  1996-08-06  \n",
       "116  (925) 779-7550                          NaN  1980-07-01  \n",
       "117  (209) 365-4060            www.vistaoaks.net  2014-08-11  \n",
       "118  (510) 787-1088                www.jsusd.org  1980-07-01  \n",
       "119  (925) 634-0037                          NaN  2005-08-01  \n",
       "\n",
       "[120 rows x 28 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataframes_dict['california_schools_merged']\n",
    "\n",
    "# Step 1: Drop columns with more than 80% null values\n",
    "threshold = 0.8  # 80%\n",
    "df_clean = df.loc[:, df.isnull().mean() < threshold]\n",
    "\n",
    "# Step 2: Drop any remaining rows with all values null (optional)\n",
    "df_clean = df_clean.dropna(how='all')\n",
    "\n",
    "# Step 3: Select the first 120 rows\n",
    "df_selected = df_clean.head(120)\n",
    "\n",
    "# Step 4: Add an index column starting from 1\n",
    "df_selected['index'] = range(1, len(df_selected) + 1)\n",
    "\n",
    "# Step 5: Move 'index' column to the beginning\n",
    "cols = ['index'] + [col for col in df_selected.columns if col != 'index']\n",
    "df_selected[cols]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in dataframes_dict.items():\n",
    "\n",
    "    # Step 3: Drop NaN rows\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Step 4: Select first 120 rows\n",
    "    df = df.head(120)\n",
    "\n",
    "    # Step 5: Add index column starting from 1\n",
    "    df['index'] = range(1, len(df) + 1)\n",
    "\n",
    "    # Step 6: Move 'index' column to the beginning\n",
    "    cols = ['index'] + [col for col in df.columns if col != 'index']\n",
    "    df = df[cols]\n",
    "\n",
    "    # Update in dictionary\n",
    "    dataframes_dict[key] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2273155/1379163965.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframes_dict['california_schools_merged'].drop([\"Website\"],axis=1,inplace=True)\n"
     ]
    }
   ],
   "source": [
    "dataframes_dict['california_schools_merged'] = df_selected[cols]\n",
    "dataframes_dict['california_schools_merged'].drop([\"Website\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Head of 'law_episode_merged' ===\n",
      "     index           organization  year  award_category  \\\n",
      "634      1  Primetime Emmy Awards  1999  Primetime Emmy   \n",
      "635      2  Primetime Emmy Awards  1999  Primetime Emmy   \n",
      "636      3  Primetime Emmy Awards  1999  Primetime Emmy   \n",
      "637      4  Primetime Emmy Awards  1999  Primetime Emmy   \n",
      "638      5  Primetime Emmy Awards  1999  Primetime Emmy   \n",
      "\n",
      "                                           award         series  \\\n",
      "634  Outstanding Guest Actress in a Drama Series  Law and Order   \n",
      "635  Outstanding Guest Actress in a Drama Series  Law and Order   \n",
      "636  Outstanding Guest Actress in a Drama Series  Law and Order   \n",
      "637  Outstanding Guest Actress in a Drama Series  Law and Order   \n",
      "638  Outstanding Guest Actress in a Drama Series  Law and Order   \n",
      "\n",
      "               role   result         category  credited  season  episode  \\\n",
      "634  Katrina Ludlow  Nominee  Additional Crew      True       9       20   \n",
      "635  Katrina Ludlow  Nominee  Additional Crew      True       9       20   \n",
      "636  Katrina Ludlow  Nominee  Additional Crew      True       9       20   \n",
      "637  Katrina Ludlow  Nominee  Additional Crew      True       9       20   \n",
      "638  Katrina Ludlow  Nominee  Additional Crew      True       9       20   \n",
      "\n",
      "     number_in_series   title  \\\n",
      "634               201  Empire   \n",
      "635               201  Empire   \n",
      "636               201  Empire   \n",
      "637               201  Empire   \n",
      "638               201  Empire   \n",
      "\n",
      "                                               summary    air_date  \\\n",
      "634  A businessman planning to build a new football...  1999-05-05   \n",
      "635  A businessman planning to build a new football...  1999-05-05   \n",
      "636  A businessman planning to build a new football...  1999-05-05   \n",
      "637  A businessman planning to build a new football...  1999-05-05   \n",
      "638  A businessman planning to build a new football...  1999-05-05   \n",
      "\n",
      "                                         episode_image  rating  votes  \n",
      "634  https://m.media-amazon.com/images/M/MV5BNjI0Yj...     7.6    218  \n",
      "635  https://m.media-amazon.com/images/M/MV5BNjI0Yj...     7.6    218  \n",
      "636  https://m.media-amazon.com/images/M/MV5BNjI0Yj...     7.6    218  \n",
      "637  https://m.media-amazon.com/images/M/MV5BNjI0Yj...     7.6    218  \n",
      "638  https://m.media-amazon.com/images/M/MV5BNjI0Yj...     7.6    218  \n",
      "\n",
      "=== Head of 'books_merged' ===\n",
      "   index                                              title       isbn13  \\\n",
      "0      1        The World's First Love: Mary  Mother of God   8987059752   \n",
      "1      2                                     The Illuminati  20049130001   \n",
      "2      3                                 The Servant Leader  23755004321   \n",
      "3      4  What Life Was Like in the Jewel in the Crown: ...  34406054602   \n",
      "4      5  Cliffs Notes on Aristophanes' Lysistrata  The ...  49086007763   \n",
      "\n",
      "   num_pages publication_date           author_name language_code  \\\n",
      "0        276       1996-09-01       Fulton J. Sheen         en-US   \n",
      "1        352       2004-10-04         Larry Burkett           eng   \n",
      "2        128       2003-03-11  Kenneth H. Blanchard           eng   \n",
      "3        168       1999-09-01       Time-Life Books           eng   \n",
      "4         80       1983-12-29      W. John Campbell           eng   \n",
      "\n",
      "           language_name  \n",
      "0  United States English  \n",
      "1                English  \n",
      "2                English  \n",
      "3                English  \n",
      "4                English  \n",
      "\n",
      "=== Head of 'computer_student_merged' ===\n",
      "   index  professor  student hasPosition        inPhase yearsInProgram  \\\n",
      "0      1          1        0     Faculty              0              0   \n",
      "1      2          1        0     Faculty              0              0   \n",
      "2      3          1        0     Faculty              0              0   \n",
      "3      4          0        1           0  Post_Generals         Year_5   \n",
      "4      5          0        1           0      Pre_Quals         Year_3   \n",
      "\n",
      "  courseLevel  \n",
      "0   Level_500  \n",
      "1   Level_400  \n",
      "2   Level_500  \n",
      "3   Level_300  \n",
      "4   Level_400  \n",
      "\n",
      "=== Head of 'mental_health_survey_merged' ===\n",
      "   index       questiontext AnswerText\n",
      "0      1  What is your age?         37\n",
      "1      2  What is your age?         44\n",
      "2      3  What is your age?         32\n",
      "3      4  What is your age?         31\n",
      "4      5  What is your age?         31\n",
      "\n",
      "=== Head of 'airline_merged' ===\n",
      "Empty DataFrame\n",
      "Columns: [index, FL_DATE, TAIL_NUM, OP_CARRIER_FL_NUM, ORIGIN, DEST, CRS_DEP_TIME, DEP_TIME, DEP_DELAY, DEP_DELAY_NEW, ARR_TIME, ARR_DELAY, ARR_DELAY_NEW, CANCELLED, CANCELLATION_CODE, CRS_ELAPSED_TIME, ACTUAL_ELAPSED_TIME, CARRIER_DELAY, WEATHER_DELAY, NAS_DELAY, SECURITY_DELAY, LATE_AIRCRAFT_DELAY]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 22 columns]\n",
      "\n",
      "=== Head of 'authors_merged' ===\n",
      "   index              Name                                        Affiliation  \\\n",
      "4      1  P. B. Littlewood          Cavendish Laboratory|Cambridge University   \n",
      "5      2        A. Kuroiwa  Department of Molecular Biology|School of Scie...   \n",
      "6      3      Jose Pereira       HASLab / INESC TEC and Universidade do Minho   \n",
      "7      4       Buzz Aldrin  Purdue University, West Lafayette, Indiana 479...   \n",
      "9      5      E.c.c. Tsang  This project,is supported,by a Hong,Kong,Polyt...   \n",
      "\n",
      "  ShortName                                      FullName  \\\n",
      "4       IFE        Informatik - Forschung Und Entwicklung   \n",
      "5      IMCS    Information Management & Computer Security   \n",
      "6   INFORMS                  Informs Journal on Computing   \n",
      "7      IJIS  International Journal of Intelligent Systems   \n",
      "9      TOIT       ACM Transactions on Internet Technology   \n",
      "\n",
      "                                            HomePage  \n",
      "4        http://www.springerlink.com/content/100528/  \n",
      "5             http://www.emeraldinsight.com/imcs.htm  \n",
      "6                       http://joc.pubs.informs.org/  \n",
      "7  http://www.interscience.wiley.com/jpages/0884-...  \n",
      "9                           http://www.acm.org/toit/  \n",
      "\n",
      "=== Head of 'human_resources_merged' ===\n",
      "   index          ssn lastname firstname hiredate        salary gender  \\\n",
      "0      1  000-01-0000  Milgrom  Patricia  10/1/04  US$57,500.00      F   \n",
      "1      2  000-02-2222    Adams     Sandy  1/15/01  US$19,500.00      F   \n",
      "2      3  109-87-6543     Wood     Emily  3/12/97  US$69,000.00      F   \n",
      "3      4  109-87-6544   Foster    Harold  8/14/05  US$55,000.00      M   \n",
      "4      5  111-12-1111  Johnson     James   5/3/96  US$47,500.00      M   \n",
      "\n",
      "  performance   locationcity               address state  zipcode  \\\n",
      "0     Average         Boston        3 Commons Blvd    MA     2190   \n",
      "1     Average        Atlanta      450 Peachtree Rd    GA    30316   \n",
      "2     Average  New York City  1650 Washington Blvd    NY    15648   \n",
      "3        Good        Chicago      500 Loop Highway    IL    60620   \n",
      "4        Good        Chicago      500 Loop Highway    IL    60620   \n",
      "\n",
      "     officephone           positiontitle educationrequired     minsalary  \\\n",
      "0  (617)123-4444                 Manager     4 year degree  US$50,000.00   \n",
      "1  (404)333-5555                 Trainee     2 year degree  US$18,000.00   \n",
      "2  (518)256-3100                 Manager     4 year degree  US$50,000.00   \n",
      "3  (312)444-6666  Account Representative     4 year degree  US$25,000.00   \n",
      "4  (312)444-6666  Account Representative     4 year degree  US$25,000.00   \n",
      "\n",
      "       maxsalary  \n",
      "0  US$150,000.00  \n",
      "1   US$25,000.00  \n",
      "2  US$150,000.00  \n",
      "3   US$75,000.00  \n",
      "4   US$75,000.00  \n"
     ]
    }
   ],
   "source": [
    "for key, df in dataframes_dict.items():\n",
    "    print(f\"\\n=== Head of '{key}' ===\")\n",
    "    print(df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_to_sentence(row,primary_key=\"\"):\n",
    "    # Start the sentence with an introductory phrase\n",
    "    sentence = \"\"\n",
    "    \n",
    "    # Dynamically iterate over all columns\n",
    "    for col in row.index:\n",
    "        if col!=primary_key:\n",
    "            value = str(row[col]).strip()  # Ensure value is a string and remove leading/trailing spaces\n",
    "            if value.lower() != \"nan\":  # Skip NaN values\n",
    "                col = col.lower()\n",
    "                value = value.strip(\"'\")  # Remove surrounding single quotes if present\n",
    "                value = value.lower()\n",
    "                sentence += f\"{col} is {value}, \"\n",
    "    \n",
    "    # Remove the trailing comma and space, then end with a period\n",
    "    sentence = sentence.rstrip(\", \") + \".\"\n",
    "    return sentence\n",
    "\n",
    "\n",
    "for key in dataframes_dict:\n",
    "    df = dataframes_dict[key]\n",
    "    df['sentence'] = df.apply(lambda row: row_to_sentence(row, primary_key=\"index\"), axis=1)\n",
    "    dataframes_dict[key] = df\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# Define output paths\n",
    "paths = {\n",
    "    'law_episode_merged': \"sources/BIRD/law_episode/data_sentence.csv\",\n",
    "    'books_merged':\"sources/BIRD/books/data_sentence.csv\",\n",
    "    'computer_student_merged':\"sources/BIRD/computer_student/data_sentence.csv\",\n",
    "    'mental_health_survey_merged':\"sources/BIRD/mental_health_survey/data_sentence.csv\",\n",
    "    'airline_merged':\"sources/BIRD/airline/data_sentence.csv\",\n",
    "    'authors_merged':\"sources/BIRD/authors/data_sentence.csv\",\n",
    "    'human_resources_merged': \"sources/BIRD/human_resources/data_sentence.csv\"\n",
    "}\n",
    "\n",
    "# Create directories and save CSVs\n",
    "for key, path in paths.items():\n",
    "    dir_path = os.path.dirname(path)\n",
    "    os.makedirs(dir_path, exist_ok=True)  # Ensure directory exists\n",
    "    dataframes_dict[key].to_csv(path, index=False)  # Save CSV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Run from here*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataframes_dict = {}\n",
    "paths = {\n",
    "    'california_schools': \"sources/BIRD/california_schools/data_sentence.csv\",\n",
    "    'financial': \"sources/BIRD/financial/data_sentence.csv\",\n",
    "    'superhero': \"sources/BIRD/superhero/data_sentence.csv\",\n",
    "    'law_episode_merged': \"sources/BIRD/law_episode/data_sentence.csv\",\n",
    "    'books_merged':\"sources/BIRD/books/data_sentence.csv\",\n",
    "    'computer_student_merged':\"sources/BIRD/computer_student/data_sentence.csv\",\n",
    "    'mental_health_survey_merged':\"sources/BIRD/mental_health_survey/data_sentence.csv\",\n",
    "    'airline_merged':\"sources/BIRD/airline/data_sentence.csv\",\n",
    "    'authors_merged':\"sources/BIRD/authors/data_sentence.csv\",\n",
    "    'human_resources_merged': \"sources/BIRD/human_resources/data_sentence.csv\"\n",
    "}\n",
    "\n",
    "# Create directories and save CSVs\n",
    "for key, path in paths.items():\n",
    "    dataframes_dict[key] = pd.read_csv(path)  # Save CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 00:32:14.780074: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-07 00:32:14.794796: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746577934.810686 2336158 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746577934.815634 2336158 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746577934.828795 2336158 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746577934.828811 2336158 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746577934.828812 2336158 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746577934.828814 2336158 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-07 00:32:14.833372: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, AutoConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "        self.model = LlamaForCausalLM.from_pretrained(\n",
    "            model_name,# config = config, \n",
    "            torch_dtype=torch.float16,\n",
    "            device_map='auto',\n",
    "        ).to(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer.eos_token = self.tokenizer.pad_token  # Set PAD token to EOS\n",
    "        \n",
    "    def predict(self,prompt, user_prompt=\"\"):\n",
    "        model,tokenizer = self.model, self.tokenizer\n",
    "        temp = random.random()\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "        # print(inputs)\n",
    "        generate_ids = model.generate(**inputs, max_new_tokens=4096, do_sample=True, temperature=temp,pad_token_id=tokenizer.eos_token_id) # Disable sampling for deterministic output\n",
    "        generate_ids = generate_ids[0][len(inputs[\"input_ids\"][0]):-1]\n",
    "        infer_res = tokenizer.decode(generate_ids)\n",
    "        return infer_res\n",
    "        \n",
    "    def enhance_sentence_with_llama(self,sentence):\n",
    "        model = self.model\n",
    "        # Construct the prompt\n",
    "        system_prompt = \"You are a creative AI that rephrases given sentences into engaging, conversational stories of a person while incorporating all provided datapoints. Ensure that no information is omitted or added, and skip any datapoints labeled as 'nan'. Do not rephrase the object of a sentence. For example, if the sentence is 'start date is 9/22/2023', do not change the date to a different format. Respond only with the rephrased sentence without any additional commentary.\"\n",
    "        user_prompt = f\"\"\"\n",
    "    Rephrase the following sentence into a conversational story, ensuring all datapoints are included while skipping 'nan' values. Do not introduce any extra or false details.\n",
    "    \n",
    "    Original sentence: {sentence}\n",
    "    \n",
    "    Creative sentence:\"\"\"\n",
    "        creative_sentence = self.predict(system_prompt,user_prompt)\n",
    "        creative_sentence = creative_sentence.replace(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\", \"\")\n",
    "        \n",
    "        # Extract only the generated part\n",
    "        # creative_sentence = response.split(\"Creative sentence:\")[-1].strip()\n",
    "        return creative_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:06<00:00,  1.54s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"As I scrolled through the hospital records, I came across the case of patient 23, who was admitted on October 18th, 2157, at 7:34 PM, after being transferred from another hospital. The emergency admission was a result of a brain mass diagnosis, and the patient was discharged on October 25th, 2157, at 2 PM, back to their home healthcare. The patient's insurance covered their stay, which was paid for by Medicare, and their marital status was listed as married, with a self-identified ethnicity of white and a Catholic religion.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "model.enhance_sentence_with_llama(\"Row ID 348: ROW_ID_x is '1136896.0', SUBJECT_ID is '23', HADM_ID_x is '124321.0', ICUSTAY_ID is '234044.0', STARTDATE is '2157-10-21 00:00:00', ENDDATE is '2157-10-25 00:00:00', DRUG_TYPE is 'MAIN', DRUG is 'Sodium Chloride 0.9%  Flush', DRUG_NAME_POE is 'Sodium Chloride 0.9%  Flush', DRUG_NAME_GENERIC is 'Sodium Chloride 0.9%  Flush', FORMULARY_DRUG_CD is 'NACLFLUSH', GSN is 'nan', NDC is '0.0', PROD_STRENGTH is 'Syringe', DOSE_VAL_RX is '3', DOSE_UNIT_RX is 'mL', FORM_VAL_DISP is '0.6', FORM_UNIT_DISP is 'SYR', ROUTE is 'IV', ROW_ID_y is '23.0', HADM_ID_y is '124321.0', ADMITTIME is '2157-10-18 19:34:00', DISCHTIME is '2157-10-25 14:00:00', DEATHTIME is 'nan', ADMISSION_TYPE is 'EMERGENCY', ADMISSION_LOCATION is 'TRANSFER FROM HOSP/EXTRAM', DISCHARGE_LOCATION is 'HOME HEALTH CARE', INSURANCE is 'Medicare', LANGUAGE is 'ENGL', RELIGION is 'CATHOLIC', MARITAL_STATUS is 'MARRIED', ETHNICITY is 'WHITE', EDREGTIME is 'nan', EDOUTTIME is 'nan', DIAGNOSIS is 'BRAIN MASS', HOSPITAL_EXPIRE_FLAG is '0.0', HAS_CHARTEVENTS_DATA is '1.0'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the checkpoint function\n",
    "def process_with_checkpoint(model,df, checkpoint_file, start_index=0, batch_size=10):\n",
    "    # Load existing checkpoint if it exists\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        df = pd.read_csv(checkpoint_file)\n",
    "        print(\"Loaded existing checkpoint.\")\n",
    "        if 'creative_sentence' not in df.columns:\n",
    "            df['creative_sentence'] = None\n",
    "    try:\n",
    "        df = df.head(120)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        # Process the dataframe in batches\n",
    "        for i in range(start_index, len(df), batch_size):\n",
    "            # Process a batch of rows\n",
    "            batch = df.iloc[i:i + batch_size]\n",
    "            \n",
    "            for idx, row in batch.iterrows():\n",
    "                if pd.isna(row['creative_sentence']):  # Only process rows not yet completed\n",
    "                    df.at[idx, 'creative_sentence'] = model.enhance_sentence_with_llama(row['sentence'])\n",
    "            \n",
    "            # Save progress after processing each batch\n",
    "            df.to_csv(checkpoint_file, index=False)\n",
    "            print(f\"Checkpoint saved at row {i + batch_size}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        # Save the checkpoint if an error occurs\n",
    "        df.to_csv(checkpoint_file, index=False)\n",
    "        print(\"Checkpoint saved after error.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def dataframe_to_json(shortened_df, dataset_path, primary_key):\n",
    "    json_data = []\n",
    "    shortened_df = shortened_df.drop(columns=['sentence'])\n",
    "    \n",
    "    for _, row in shortened_df.iterrows():\n",
    "        # Extract the primary key value if it's valid\n",
    "        primary_id = str(row[primary_key]).lower() if primary_key in row and pd.notna(row[primary_key]) and str(row[primary_key]).lower() != \"nan\" else None\n",
    "        \n",
    "        # Build key-value pairs, skipping 'creative_sentence' and the primary key\n",
    "        key_value = {\n",
    "            col.lower(): (primary_id, str(row[col]).lower()) \n",
    "            for col in shortened_df.columns \n",
    "            if col not in [\"creative_sentence\", primary_key] and pd.notna(row[col]) and str(row[col]).lower() != \"nan\"\n",
    "        }\n",
    "\n",
    "        entities = list(key_value.keys())\n",
    "\n",
    "        json_data.append({\n",
    "            \"text\": row[\"creative_sentence\"],\n",
    "            \"ground_truth_entities\": entities,\n",
    "            \"ground_truth_key_value\": key_value\n",
    "        })\n",
    "\n",
    "    with open(f\"{dataset_path}.json\", 'w') as json_file:\n",
    "        json.dump(json_data, json_file, indent=4)\n",
    "    \n",
    "    print(\"JSON file saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def combine_jsons(dataset_path):\n",
    "    # Load the JSON file\n",
    "    with open(f\"{dataset_path}.json\", \"r\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "    \n",
    "    # Combine every 5 entries\n",
    "    combined_data = []\n",
    "    batch_size = 5  # Number of entries to combine\n",
    "    \n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i + batch_size]  # Take a batch of 5\n",
    "    \n",
    "        # Merge text fields\n",
    "        combined_text = \"\\n\".join(str(entry[\"text\"]) for entry in batch)\n",
    "    \n",
    "        # Merge unique entities\n",
    "        combined_entities = list(set(entity for entry in batch for entity in entry[\"ground_truth_entities\"]))\n",
    "    \n",
    "        # Merge key-value pairs, keeping all values in a list\n",
    "        combined_key_value = {}\n",
    "    \n",
    "        for entry in batch:\n",
    "            for key, value in entry[\"ground_truth_key_value\"].items():\n",
    "                if key in combined_key_value:\n",
    "                    if value not in combined_key_value[key]:  # Avoid duplicate values\n",
    "                        combined_key_value[key].append(value)\n",
    "                else:\n",
    "                    combined_key_value[key] = [value]\n",
    "\n",
    "        difficulty = None\n",
    "        domain = None\n",
    "        if \"california_schools\" in dataset_path:\n",
    "            domain = \"california_schools\"\n",
    "        elif \"financial\" in dataset_path:\n",
    "            domain = \"financial\"\n",
    "        elif \"superhero\" in dataset_path:\n",
    "            domain = \"superhero\"\n",
    "        elif \"law_episode\" in dataset_path:\n",
    "            domain = \"law_episode\"\n",
    "        elif \"books\" in dataset_path:\n",
    "            domain = \"books\"\n",
    "        elif \"computer_student\" in dataset_path:\n",
    "            domain = \"computer_student\"\n",
    "        elif \"mental_health_survey\" in dataset_path:\n",
    "            domain = \"mental_health_survey\"\n",
    "        elif \"airline\" in dataset_path:\n",
    "            domain = \"airline\"\n",
    "        elif \"authors\" in dataset_path:\n",
    "            domain = \"authors\"\n",
    "        elif \"human_resources\" in dataset_path:\n",
    "            domain = \"human_resources\"\n",
    "        else:\n",
    "            domain = \"unknown\"\n",
    "        \n",
    "        \n",
    "        # Append combined entry\n",
    "        combined_data.append({\n",
    "            \"text\": combined_text,\n",
    "            \"ground_truth_entities\": combined_entities,\n",
    "            \"ground_truth_key_value\": combined_key_value,\n",
    "            \"difficulty\": difficulty,\n",
    "            \"domain\": domain\n",
    "        })\n",
    "    \n",
    "    # Save the new JSON file\n",
    "    with open(f\"{dataset_path}_combined.json\", \"w\") as json_file:\n",
    "        json.dump(combined_data, json_file, indent=4)\n",
    "    \n",
    "    print(\"Combined JSON file saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the combined JSON file\n",
    "def clean_combined_jsons(dataset_path):\n",
    "    with open(f\"{dataset_path}_combined.json\", \"r\") as json_file:\n",
    "        combined_data = json.load(json_file)\n",
    "    \n",
    "    # Function to clean entities and key-value pairs\n",
    "    def clean_entry(entry):\n",
    "        text = entry[\"text\"].lower()  # Convert text to lowercase for case-insensitive matching\n",
    "    \n",
    "        # Keep only entities that exist in the text\n",
    "        filtered_entities = [entity for entity in entry[\"ground_truth_entities\"] if entity.lower() in text]\n",
    "    \n",
    "        # Initialize filtered key-value store\n",
    "        filtered_key_value = {}\n",
    "    \n",
    "        for key, value_list in entry[\"ground_truth_key_value\"].items():\n",
    "            if isinstance(value_list, list):\n",
    "                valid_pairs = []\n",
    "                \n",
    "                for pair in value_list:\n",
    "                    if isinstance(pair, list) and len(pair) == 2:  # Ensure it's a (trip_id, value) structure\n",
    "                        trip_id, actual_value = pair\n",
    "                        \n",
    "                        # Keep only pairs where the value appears in the text\n",
    "                        if str(actual_value).lower() in text:\n",
    "                            valid_pairs.append([trip_id, actual_value])\n",
    "                \n",
    "                # Only add key if it has valid values\n",
    "                if valid_pairs:\n",
    "                    filtered_key_value[key] = valid_pairs\n",
    "\n",
    "        # Return cleaned entry\n",
    "        return {\n",
    "            \"text\": entry[\"text\"],\n",
    "            \"ground_truth_entities\": filtered_entities,\n",
    "            \"ground_truth_key_value\": filtered_key_value,\n",
    "            \"difficulty\": entry[\"difficulty\"],\n",
    "            \"domain\": entry[\"domain\"]\n",
    "            \n",
    "        }\n",
    "    # Apply the cleaning function to each entry\n",
    "    cleaned_data = [clean_entry(entry) for entry in combined_data]\n",
    "    \n",
    "    # Save the cleaned JSON file\n",
    "    with open(f\"{dataset_path}_combined_cleaned.json\", \"w\") as json_file:\n",
    "        json.dump(cleaned_data, json_file, indent=4)\n",
    "    \n",
    "    print(\"Cleaned JSON file saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column_count in california_schools 120\n",
      "nan_rows in california_schools 5\n",
      "column_count in financial 120\n",
      "nan_rows in financial 0\n",
      "column_count in superhero 120\n",
      "nan_rows in superhero 21\n",
      "column_count in law_episode_merged 120\n",
      "nan_rows in law_episode_merged 0\n",
      "column_count in books_merged 120\n",
      "nan_rows in books_merged 0\n",
      "column_count in computer_student_merged 120\n",
      "nan_rows in computer_student_merged 0\n",
      "column_count in mental_health_survey_merged 120\n",
      "nan_rows in mental_health_survey_merged 0\n",
      "column_count in airline_merged 0\n",
      "nan_rows in airline_merged 0\n",
      "column_count in authors_merged 120\n",
      "nan_rows in authors_merged 0\n",
      "column_count in human_resources_merged 22\n",
      "nan_rows in human_resources_merged 0\n"
     ]
    }
   ],
   "source": [
    "for name, df in dataframes_dict.items():\n",
    "    row_count = df.shape[0]\n",
    "    print(\"column_count in\",name,row_count)\n",
    "    nan_rows = df[df.isna().any(axis=1)]\n",
    "    print(\"nan_rows in\",name,len(nan_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in dataframes_dict.items():\n",
    "    df_clean = df.dropna()\n",
    "    dataframes_dict[name] = df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_count in california_schools 115\n",
      "nan_rows in california_schools 0\n",
      "row_count in financial 120\n",
      "nan_rows in financial 0\n",
      "row_count in superhero 99\n",
      "nan_rows in superhero 0\n",
      "row_count in law_episode_merged 120\n",
      "nan_rows in law_episode_merged 0\n",
      "row_count in books_merged 120\n",
      "nan_rows in books_merged 0\n",
      "row_count in computer_student_merged 120\n",
      "nan_rows in computer_student_merged 0\n",
      "row_count in mental_health_survey_merged 120\n",
      "nan_rows in mental_health_survey_merged 0\n",
      "row_count in airline_merged 0\n",
      "nan_rows in airline_merged 0\n",
      "row_count in authors_merged 120\n",
      "nan_rows in authors_merged 0\n",
      "row_count in human_resources_merged 22\n",
      "nan_rows in human_resources_merged 0\n"
     ]
    }
   ],
   "source": [
    "for name, df in dataframes_dict.items():\n",
    "    row_count = df.shape[0]\n",
    "    print(\"row_count in\",name,row_count)\n",
    "    nan_rows = df[df.isna().any(axis=1)]\n",
    "    print(\"nan_rows in\",name,len(nan_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataframes_dict['airline_merged']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: sources/BIRD//california_schools/data_sentence.csv\n",
      "Loaded existing checkpoint.\n",
      "Checkpoint saved at row 20.\n",
      "Checkpoint saved at row 40.\n",
      "Checkpoint saved at row 60.\n",
      "Checkpoint saved at row 80.\n",
      "Checkpoint saved at row 100.\n",
      "Checkpoint saved at row 120.\n",
      "Saved: sources/BIRD//california_schools/data_sentence.csv\n",
      "Processing: sources/BIRD//financial/data_sentence.csv\n",
      "Loaded existing checkpoint.\n",
      "Checkpoint saved at row 20.\n",
      "Checkpoint saved at row 40.\n",
      "Checkpoint saved at row 60.\n",
      "Checkpoint saved at row 80.\n",
      "Checkpoint saved at row 100.\n",
      "Checkpoint saved at row 120.\n",
      "Saved: sources/BIRD//financial/data_sentence.csv\n",
      "Processing: sources/BIRD//superhero/data_sentence.csv\n",
      "Loaded existing checkpoint.\n",
      "Checkpoint saved at row 20.\n",
      "Checkpoint saved at row 40.\n",
      "Checkpoint saved at row 60.\n",
      "Checkpoint saved at row 80.\n",
      "Checkpoint saved at row 100.\n",
      "Checkpoint saved at row 120.\n",
      "Saved: sources/BIRD//superhero/data_sentence.csv\n",
      "Processing: sources/BIRD//law_episode/data_sentence.csv\n",
      "Loaded existing checkpoint.\n",
      "Checkpoint saved at row 20.\n",
      "Checkpoint saved at row 40.\n",
      "Checkpoint saved at row 60.\n",
      "Checkpoint saved at row 80.\n",
      "Checkpoint saved at row 100.\n",
      "Checkpoint saved at row 120.\n",
      "Saved: sources/BIRD//law_episode/data_sentence.csv\n",
      "Processing: sources/BIRD//books/data_sentence.csv\n",
      "Loaded existing checkpoint.\n",
      "Checkpoint saved at row 20.\n",
      "Checkpoint saved at row 40.\n",
      "Checkpoint saved at row 60.\n",
      "Checkpoint saved at row 80.\n",
      "Checkpoint saved at row 100.\n",
      "Checkpoint saved at row 120.\n",
      "Saved: sources/BIRD//books/data_sentence.csv\n",
      "Processing: sources/BIRD//computer_student/data_sentence.csv\n",
      "Loaded existing checkpoint.\n",
      "Checkpoint saved at row 20.\n",
      "Checkpoint saved at row 40.\n",
      "Checkpoint saved at row 60.\n",
      "Checkpoint saved at row 80.\n",
      "Checkpoint saved at row 100.\n",
      "Checkpoint saved at row 120.\n",
      "Saved: sources/BIRD//computer_student/data_sentence.csv\n",
      "Processing: sources/BIRD//mental_health_survey/data_sentence.csv\n",
      "Loaded existing checkpoint.\n",
      "Checkpoint saved at row 20.\n",
      "Checkpoint saved at row 40.\n",
      "Checkpoint saved at row 60.\n",
      "Checkpoint saved at row 80.\n",
      "Checkpoint saved at row 100.\n",
      "Checkpoint saved at row 120.\n",
      "Saved: sources/BIRD//mental_health_survey/data_sentence.csv\n",
      "Processing: sources/BIRD//authors/data_sentence.csv\n",
      "Loaded existing checkpoint.\n",
      "Checkpoint saved at row 20.\n",
      "Checkpoint saved at row 40.\n",
      "Checkpoint saved at row 60.\n",
      "Checkpoint saved at row 80.\n",
      "Checkpoint saved at row 100.\n",
      "Checkpoint saved at row 120.\n",
      "Saved: sources/BIRD//authors/data_sentence.csv\n",
      "Processing: sources/BIRD//human_resources/data_sentence.csv\n",
      "Loaded existing checkpoint.\n",
      "Checkpoint saved at row 20.\n",
      "Checkpoint saved at row 40.\n",
      "Saved: sources/BIRD//human_resources/data_sentence.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the base directory\n",
    "base_dir = \"sources/BIRD/\"\n",
    "\n",
    "# Sort the dictionary by DataFrame length in ascending order\n",
    "names = ['california_schools','financial','superhero','law_episode','books','computer_student','mental_health_survey','authors','human_resources']\n",
    "\n",
    "# Loop through each DataFrame in sorted order\n",
    "for name in names:\n",
    "    # Construct the checkpoint file path dynamically\n",
    "    checkpoint_path = f\"{base_dir}/{name}/data_sentence.csv\"\n",
    "\n",
    "    print(f\"Processing: {checkpoint_path}\")\n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(checkpoint_path)\n",
    "    try:\n",
    "        df = df.head(120)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Process the DataFrame with checkpointing\n",
    "    df = process_with_checkpoint(model, df, checkpoint_file=checkpoint_path, batch_size=20)\n",
    "\n",
    "    # Save the final result back to the same CSV file\n",
    "    df.to_csv(checkpoint_path, index=False)\n",
    "\n",
    "    print(f\"Saved: {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: sources/BIRD//california_schools/data_sentence\n",
      "JSON file saved successfully!\n",
      "Combined JSON file saved successfully!\n",
      "Processing: sources/BIRD//financial/data_sentence\n",
      "JSON file saved successfully!\n",
      "Combined JSON file saved successfully!\n",
      "Processing: sources/BIRD//superhero/data_sentence\n",
      "JSON file saved successfully!\n",
      "Combined JSON file saved successfully!\n",
      "Processing: sources/BIRD//law_episode/data_sentence\n",
      "JSON file saved successfully!\n",
      "Combined JSON file saved successfully!\n",
      "Processing: sources/BIRD//books/data_sentence\n",
      "JSON file saved successfully!\n",
      "Combined JSON file saved successfully!\n",
      "Processing: sources/BIRD//computer_student/data_sentence\n",
      "JSON file saved successfully!\n",
      "Combined JSON file saved successfully!\n",
      "Processing: sources/BIRD//mental_health_survey/data_sentence\n",
      "JSON file saved successfully!\n",
      "Combined JSON file saved successfully!\n",
      "Processing: sources/BIRD//authors/data_sentence\n",
      "JSON file saved successfully!\n",
      "Combined JSON file saved successfully!\n",
      "Processing: sources/BIRD//human_resources/data_sentence\n",
      "JSON file saved successfully!\n",
      "Combined JSON file saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "base_dir = \"sources/BIRD/\"\n",
    "\n",
    "names = ['california_schools','financial','superhero','law_episode','books','computer_student','mental_health_survey','authors','human_resources']\n",
    "\n",
    "# Loop through each DataFrame in sorted order\n",
    "for name in names:\n",
    "    # Construct the checkpoint file path dynamically\n",
    "    checkpoint_path = f\"{base_dir}/{name}/data_sentence\"\n",
    "    primary_id = \"index\"\n",
    "\n",
    "    print(f\"Processing: {checkpoint_path}\")\n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(checkpoint_path + \".csv\")\n",
    "    \n",
    "    dataframe_to_json(df,checkpoint_path,primary_id)\n",
    "    combine_jsons(checkpoint_path)\n",
    "    # clean_combined_jsons(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 files. Cleaning and merging...\n",
      "File: law_episode/data_sentence_combined.json contains 24 valid entries.\n",
      "File: books/data_sentence_combined.json contains 24 valid entries.\n",
      "File: computer_student/data_sentence_combined.json contains 24 valid entries.\n",
      "File: mental_health_survey/data_sentence_combined.json contains 24 valid entries.\n",
      "File: authors/data_sentence_combined.json contains 24 valid entries.\n",
      "File: financial/data_sentence_combined.json contains 24 valid entries.\n",
      "File: human_resources/data_sentence_combined.json contains 5 valid entries.\n",
      "File: california_schools/data_sentence_combined.json contains 24 valid entries.\n",
      "File: superhero/data_sentence_combined.json contains 24 valid entries.\n",
      "Successfully merged 197 entries into bird_dataset.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "def find_json_files(root_folder, target_filename):\n",
    "    \"\"\"Recursively find all files named target_filename in root_folder.\"\"\"\n",
    "    json_files = []\n",
    "    \n",
    "    for root, _, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            if file == target_filename:\n",
    "                file_path = os.path.join(root, file)\n",
    "                json_files.append(file_path)\n",
    "\n",
    "    return json_files\n",
    "\n",
    "def clean_data(data):\n",
    "    \"\"\"Remove entries with empty 'text', 'entities', and 'key_value', and clean empty keys in 'key_value'.\"\"\"\n",
    "    cleaned = []\n",
    "    for entry in data:\n",
    "        if (\n",
    "            entry.get(\"entities\") == [] and\n",
    "            entry.get(\"key_value\") == {}\n",
    "        ):\n",
    "            continue  # Skip unwanted entry\n",
    "\n",
    "        if \"key_value\" in entry:\n",
    "            # Remove empty key_value entries\n",
    "            entry[\"key_value\"] = {k: v for k, v in entry[\"key_value\"].items() if v}\n",
    "\n",
    "        cleaned.append(entry)\n",
    "    return cleaned\n",
    "\n",
    "def merge_and_shuffle_json_files(json_files, root_folder):\n",
    "    \"\"\"Merge and shuffle all lists of dictionaries from found JSON files after cleaning.\"\"\"\n",
    "    combined_data = []\n",
    "    \n",
    "    for file in json_files:\n",
    "        relative_path = os.path.relpath(file, root_folder)\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    data = clean_data(data)\n",
    "                    print(f\"File: {relative_path} contains {len(data)} valid entries.\")\n",
    "                    combined_data.extend(data)\n",
    "                else:\n",
    "                    print(f\"Warning: {relative_path} does not contain a list.\")\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error: Could not decode {relative_path}\")\n",
    "\n",
    "    random.shuffle(combined_data)\n",
    "    return combined_data\n",
    "\n",
    "# Settings\n",
    "root_folder = \"sources/BIRD\"\n",
    "target_filename = \"data_sentence_combined.json\"\n",
    "\n",
    "# Run\n",
    "json_files = find_json_files(root_folder, target_filename)\n",
    "\n",
    "if not json_files:\n",
    "    print(\"No matching JSON files found.\")\n",
    "\n",
    "print(f\"Found {len(json_files)} files. Cleaning and merging...\")\n",
    "\n",
    "combined_data = merge_and_shuffle_json_files(json_files, root_folder)\n",
    "\n",
    "output_file = \"bird_dataset.json\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(combined_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Successfully merged {len(combined_data)} entries into {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def remove_empty_key_value_entries(root_dir):\n",
    "    \"\"\"Remove entries with empty 'key_value' dict from all JSON files in subdirectories.\"\"\"\n",
    "    for root, _, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        data = json.load(f)\n",
    "\n",
    "                    if isinstance(data, list):\n",
    "                        cleaned_data = [\n",
    "                            entry for entry in data\n",
    "                            if not (isinstance(entry, dict) and entry.get(\"key_value\") == {})\n",
    "                        ]\n",
    "\n",
    "                        if len(cleaned_data) != len(data):\n",
    "                            print(f\"Cleaned {len(data) - len(cleaned_data)} entries from: {file_path}\")\n",
    "\n",
    "                        # Save back the cleaned data\n",
    "                        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                            json.dump(cleaned_data, f, indent=4, ensure_ascii=False)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Change to your root directory\n",
    "root_directory = \"sources/BIRD\"\n",
    "remove_empty_key_value_entries(root_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed domain 'human_resources' with only 5 entries.\n",
      "california_schools: 24\n",
      "financial: 24\n",
      "superhero: 24\n",
      "law_episode: 24\n",
      "books: 24\n",
      "computer_student: 24\n",
      "mental_health_survey: 24\n",
      "authors: 24\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from itertools import cycle\n",
    "\n",
    "# Define the path to your JSON file\n",
    "file_path = '/home/mushtari/nl2db/nl2db-main/data-generation/bird_dataset.json'\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Group entries by domain\n",
    "domain_entries = defaultdict(list)\n",
    "for entry in data:\n",
    "    domain = entry.get(\"domain\")\n",
    "    domain_entries[domain].append(entry)\n",
    "\n",
    "# Filter domains: keep only those with at least 24 entries, and take the first 24\n",
    "filtered_data = []\n",
    "for domain, entries in domain_entries.items():\n",
    "    if len(entries) < 24:\n",
    "        print(f\"Removed domain '{domain}' with only {len(entries)} entries.\")\n",
    "        continue\n",
    "    filtered_data.extend(entries[:24])\n",
    "\n",
    "data = filtered_data\n",
    "\n",
    "\n",
    "domain_buckets = defaultdict(list)\n",
    "for entry in data:\n",
    "    domain = entry.get(\"domain\")\n",
    "    domain_buckets[domain].append(entry)\n",
    "\n",
    "# Define the domains to interleave (and their order)\n",
    "target_domains = ['california_schools','financial','superhero','law_episode','books','computer_student','mental_health_survey','authors','human_resources']\n",
    "\n",
    "# Create a round-robin interleaving of entries\n",
    "interleaved = []\n",
    "domain_iters = {d: iter(domain_buckets[d]) for d in target_domains}\n",
    "\n",
    "# Continue pulling one entry at a time from each domain in order\n",
    "while any(domain_buckets[d] for d in target_domains):\n",
    "    for d in target_domains:\n",
    "        if domain_buckets[d]:  # Only append if there are entries left\n",
    "            interleaved.append(domain_buckets[d].pop(0))\n",
    "\n",
    "data = interleaved\n",
    "\n",
    "# Count how many entries per domain\n",
    "domain_counts = Counter(entry.get(\"domain\") for entry in data)\n",
    "\n",
    "# Print the counts\n",
    "for domain, count in domain_counts.items():\n",
    "    print(f\"{domain}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in data:\n",
    "    gtv = entry.get(\"ground_truth_key_value\", {})\n",
    "    for key in gtv:\n",
    "        for i, pair in enumerate(gtv[key]):\n",
    "            pair[0] = str(i + 1)  # Replace first value with \"1\", \"2\", ..., \"5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON files\n",
    "with open('/home/mushtari/nl2db/nl2db-main/data-generation/merged_dataset.json', 'r') as f1, open('/home/mushtari/nl2db/nl2db-main/data-generation/merged_dataset2.json', 'r') as f2:\n",
    "    data1 = json.load(f1)\n",
    "    data2 = json.load(f2)\n",
    "\n",
    "# Filter healthcare entries from file1\n",
    "healthcare_entries_file1 = [entry for entry in data1 if entry.get('domain') == 'healthcare']\n",
    "\n",
    "# Replace healthcare entries in file2 at their original positions\n",
    "replaced_data2 = []\n",
    "healthcare_idx = 0\n",
    "\n",
    "for entry in data2:\n",
    "    if entry.get('domain') == 'healthcare':\n",
    "        if healthcare_idx < len(healthcare_entries_file1):\n",
    "            replaced_data2.append(healthcare_entries_file1[healthcare_idx])\n",
    "            healthcare_idx += 1\n",
    "        else:\n",
    "            # Optionally skip or keep original if not enough entries in file1\n",
    "            replaced_data2.append(entry)\n",
    "    else:\n",
    "        replaced_data2.append(entry)\n",
    "\n",
    "# Save the modified data2\n",
    "with open('/home/mushtari/nl2db/nl2db-main/data-generation/merged_dataset3.json', 'w') as f:\n",
    "    json.dump(replaced_data2, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
