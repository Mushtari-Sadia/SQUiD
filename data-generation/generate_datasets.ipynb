{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: sources/tourism/data.csv -> DataFrame: tourism_data\n",
      "Loaded: sources/education/data.csv -> DataFrame: education_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_730223/4010045254.py:35: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: sources/healthcare/data.csv -> DataFrame: healthcare_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_730223/4010045254.py:35: DtypeWarning: Columns (1,2,3,7,8,9,11,12,13,14,17,18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: sources/ecommerce/data.csv -> DataFrame: ecommerce_data\n",
      "Loaded: sources/finance/data.csv -> DataFrame: finance_data\n",
      "DataFrame Name: tourism_data\n",
      "   Trip ID       Destination Start date   End date  Duration (days)  \\\n",
      "0        1        London, UK   5/1/2023   5/8/2023              7.0   \n",
      "1        2  Phuket, Thailand  6/15/2023  6/20/2023              5.0   \n",
      "2        3   Bali, Indonesia   7/1/2023   7/8/2023              7.0   \n",
      "3        4     New York, USA  8/15/2023  8/29/2023             14.0   \n",
      "4        5      Tokyo, Japan  9/10/2023  9/17/2023              7.0   \n",
      "\n",
      "   Traveler name  Traveler age Traveler gender Traveler nationality  \\\n",
      "0     John Smith          35.0            Male             American   \n",
      "1       Jane Doe          28.0          Female             Canadian   \n",
      "2      David Lee          45.0            Male               Korean   \n",
      "3  Sarah Johnson          29.0          Female              British   \n",
      "4     Kim Nguyen          26.0          Female           Vietnamese   \n",
      "\n",
      "  Accommodation type Accommodation cost Transportation type  \\\n",
      "0              Hotel               1200              Flight   \n",
      "1             Resort                800              Flight   \n",
      "2              Villa               1000              Flight   \n",
      "3              Hotel               2000              Flight   \n",
      "4             Airbnb                700               Train   \n",
      "\n",
      "  Transportation cost  \n",
      "0                 600  \n",
      "1                 500  \n",
      "2                 700  \n",
      "3                1000  \n",
      "4                 200  \n",
      "DataFrame Name: education_data\n",
      "   age         workclass  fnlwgt  education  education-num  \\\n",
      "0   39         State-gov   77516  Bachelors             13   \n",
      "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
      "2   38           Private  215646    HS-grad              9   \n",
      "3   53           Private  234721       11th              7   \n",
      "4   28           Private  338409  Bachelors             13   \n",
      "\n",
      "       marital-status         occupation   relationship   race     sex  \\\n",
      "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
      "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
      "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
      "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
      "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
      "\n",
      "   capital-gain  capital-loss  hours-per-week native-country salary  \n",
      "0          2174             0              40  United-States  <=50K  \n",
      "1             0             0              13  United-States  <=50K  \n",
      "2             0             0              40  United-States  <=50K  \n",
      "3             0             0              40  United-States  <=50K  \n",
      "4             0             0              40           Cuba  <=50K  \n",
      "DataFrame Name: healthcare_data\n",
      "   Unnamed: 0           start_date             end_date drug_type  \\\n",
      "0           0  2138-07-18 00:00:00  2138-07-20 00:00:00      MAIN   \n",
      "1           1  2138-07-18 00:00:00  2138-07-20 00:00:00      BASE   \n",
      "2           2  2138-07-18 00:00:00  2138-07-21 00:00:00      MAIN   \n",
      "3           3  2138-07-18 00:00:00  2138-07-21 00:00:00      BASE   \n",
      "4           4                  NaN                  NaN       NaN   \n",
      "\n",
      "                  drug_name drug_name_poe drug_name_generic formulary_code  \\\n",
      "0         NEO*IV*Gentamicin           NaN               NaN        GENT10I   \n",
      "1  Syringe (Neonatal) *D5W*           NaN               NaN      NEOSYRD5W   \n",
      "2         Ampicillin Sodium           NaN               NaN        AMP500I   \n",
      "3           Send 500mg Vial           NaN               NaN          AMPVL   \n",
      "4                       NaN           NaN               NaN            NaN   \n",
      "\n",
      "      gsn           ndc  ... insurance language       religion marital_status  \\\n",
      "0  009298  6.332302e+10  ...   Private      NaN  NOT SPECIFIED            NaN   \n",
      "1     NaN  0.000000e+00  ...   Private      NaN  NOT SPECIFIED            NaN   \n",
      "2  008937  6.332304e+10  ...   Private      NaN  NOT SPECIFIED            NaN   \n",
      "3     NaN  0.000000e+00  ...   Private      NaN  NOT SPECIFIED            NaN   \n",
      "4     NaN           NaN  ...   Private      NaN       BUDDHIST            NaN   \n",
      "\n",
      "  ethnicity ed_registration_time  ed_exit_time  diagnosis expired_in_hospital  \\\n",
      "0     ASIAN                  NaN           NaN    NEWBORN                 0.0   \n",
      "1     ASIAN                  NaN           NaN    NEWBORN                 0.0   \n",
      "2     ASIAN                  NaN           NaN    NEWBORN                 0.0   \n",
      "3     ASIAN                  NaN           NaN    NEWBORN                 0.0   \n",
      "4     ASIAN                  NaN           NaN    NEWBORN                 0.0   \n",
      "\n",
      "  has_chartevents_data  \n",
      "0                  1.0  \n",
      "1                  1.0  \n",
      "2                  1.0  \n",
      "3                  1.0  \n",
      "4                  1.0  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "DataFrame Name: ecommerce_data\n",
      "    item_id          status created_at  \\\n",
      "0  211131.0        complete   7/1/2016   \n",
      "1  211133.0        canceled   7/1/2016   \n",
      "2  211134.0        canceled   7/1/2016   \n",
      "3  211135.0        complete   7/1/2016   \n",
      "4  211136.0  order_refunded   7/1/2016   \n",
      "\n",
      "                                                 sku   price  qty_ordered  \\\n",
      "0                                  kreations_YI 06-L  1950.0          1.0   \n",
      "1  kcc_Buy 2 Frey Air Freshener & Get 1 Kasual Bo...   240.0          1.0   \n",
      "2                                 Ego_UP0017-999-MR0  2450.0          1.0   \n",
      "3                                     kcc_krone deal   360.0          1.0   \n",
      "4                                        BK7010400AG   555.0          2.0   \n",
      "\n",
      "   grand_total increment_id    category_name_1 sales_commission_code  ...  \\\n",
      "0       1950.0    100147443    Women's Fashion                    \\N  ...   \n",
      "1        240.0    100147444  Beauty & Grooming                    \\N  ...   \n",
      "2       2450.0    100147445    Women's Fashion                    \\N  ...   \n",
      "3         60.0    100147446  Beauty & Grooming           R-FSD-52352  ...   \n",
      "4       1110.0    100147447            Soghaat                    \\N  ...   \n",
      "\n",
      "   Month Customer Since     M-Y    FY Customer ID  Unnamed: 21  Unnamed: 22  \\\n",
      "0    7.0         2016-7  7-2016  FY17         1.0          NaN          NaN   \n",
      "1    7.0         2016-7  7-2016  FY17         2.0          NaN          NaN   \n",
      "2    7.0         2016-7  7-2016  FY17         3.0          NaN          NaN   \n",
      "3    7.0         2016-7  7-2016  FY17         4.0          NaN          NaN   \n",
      "4    7.0         2016-7  7-2016  FY17         5.0          NaN          NaN   \n",
      "\n",
      "  Unnamed: 23 Unnamed: 24 Unnamed: 25  \n",
      "0         NaN         NaN         NaN  \n",
      "1         NaN         NaN         NaN  \n",
      "2         NaN         NaN         NaN  \n",
      "3         NaN         NaN         NaN  \n",
      "4         NaN         NaN         NaN  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "DataFrame Name: finance_data\n",
      "  Customer_ID  Age  Location Income_Level  Total_Transactions  \\\n",
      "0   cust_0000   54     Urban          Low                 192   \n",
      "1   cust_0001   67  Suburban         High                 979   \n",
      "2   cust_0002   44     Urban         High                 329   \n",
      "3   cust_0003   30     Rural         High                  71   \n",
      "4   cust_0004   58     Urban       Middle                 878   \n",
      "\n",
      "   Avg_Transaction_Value  Max_Transaction_Value  Min_Transaction_Value  \\\n",
      "0           16736.384023           60216.834510            6525.814861   \n",
      "1           14536.734683           48350.100272            2186.742245   \n",
      "2            7061.372800           32521.157187            2743.406808   \n",
      "3           16426.876453           17827.896720            4360.784994   \n",
      "4           10800.092660           17497.634534            4532.872520   \n",
      "\n",
      "    Total_Spent  Active_Days  Last_Transaction_Days_Ago  \\\n",
      "0  3.213386e+06          140                        209   \n",
      "1  1.423146e+07          229                        240   \n",
      "2  2.323192e+06           73                         21   \n",
      "3  1.166308e+06          299                        285   \n",
      "4  9.482481e+06          236                        329   \n",
      "\n",
      "   Loyalty_Points_Earned  Referral_Count  Cashback_Received  \\\n",
      "0                   2114              25        2224.012140   \n",
      "1                   2960              20        4026.823518   \n",
      "2                   3170               0        1441.011395   \n",
      "3                   4756              35        4365.855580   \n",
      "4                   1992              18        4161.523827   \n",
      "\n",
      "  App_Usage_Frequency Preferred_Payment_Method  Support_Tickets_Raised  \\\n",
      "0             Monthly               Debit Card                       3   \n",
      "1             Monthly                      UPI                      17   \n",
      "2             Monthly               Debit Card                      11   \n",
      "3              Weekly           Wallet Balance                       6   \n",
      "4               Daily                      UPI                      18   \n",
      "\n",
      "   Issue_Resolution_Time  Customer_Satisfaction_Score           LTV  \n",
      "0              61.568590                            1  3.279546e+05  \n",
      "1              60.392889                            8  1.437053e+06  \n",
      "2              45.305579                            4  2.419387e+05  \n",
      "3              22.030191                            1  1.284599e+05  \n",
      "4              20.634723                            5  9.569514e+05  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_csv_files(folder_path, skip_folder=None):\n",
    "    \"\"\"\n",
    "    Recursively load all CSV files from the specified folder and its subfolders into Pandas DataFrames,\n",
    "    while skipping a specified folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing CSV files.\n",
    "        skip_folder (str, optional): Name of the folder to skip (relative to folder_path).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are CSV file paths (relative to folder_path)\n",
    "              and values are the corresponding Pandas DataFrames.\n",
    "    \"\"\"\n",
    "    dataframes = {}\n",
    "\n",
    "    # Walk through all directories and files\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        # Skip the specified folder\n",
    "        if skip_folder and skip_folder in dirs:\n",
    "            dirs.remove(skip_folder)  # This prevents os.walk() from descending into the folder\n",
    "\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(\"data.csv\"):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "\n",
    "                # Create a unique name for the DataFrame (relative path without extension)\n",
    "                rel_path = os.path.relpath(file_path, folder_path)\n",
    "                dataframe_name = os.path.splitext(rel_path)[0].replace(os.sep, \"_\")\n",
    "\n",
    "                try:\n",
    "                    # Read the CSV file into a Pandas DataFrame\n",
    "                    df = pd.read_csv(file_path)\n",
    "\n",
    "                    # Store DataFrame in the dictionary\n",
    "                    dataframes[dataframe_name] = df\n",
    "\n",
    "                    print(f\"Loaded: {file_path} -> DataFrame: {dataframe_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not load: {file_path} -> DataFrame: {dataframe_name}. Error: {e}\")\n",
    "\n",
    "    return dataframes\n",
    "\n",
    "# Specify the folder containing your CSV files\n",
    "folder_path = \"sources\"\n",
    "\n",
    "# Load all CSV files into DataFrames\n",
    "dataframes_dict = load_csv_files(folder_path)\n",
    "\n",
    "# Example: Access a specific DataFrame\n",
    "for name, df in dataframes_dict.items():\n",
    "    print(f\"DataFrame Name: {name}\")\n",
    "    print(df.head(5))  # Display the first few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Name: tourism_data\n",
      "Columns:\n",
      "  - Trip ID\n",
      "  - Destination\n",
      "  - Start date\n",
      "  - End date\n",
      "  - Duration (days)\n",
      "  - Traveler name\n",
      "  - Traveler age\n",
      "  - Traveler gender\n",
      "  - Traveler nationality\n",
      "  - Accommodation type\n",
      "  - Accommodation cost\n",
      "  - Transportation type\n",
      "  - Transportation cost\n",
      "\n",
      "DataFrame Name: education_data\n",
      "Columns:\n",
      "  - age\n",
      "  - workclass\n",
      "  - fnlwgt\n",
      "  - education\n",
      "  - education-num\n",
      "  - marital-status\n",
      "  - occupation\n",
      "  - relationship\n",
      "  - race\n",
      "  - sex\n",
      "  - capital-gain\n",
      "  - capital-loss\n",
      "  - hours-per-week\n",
      "  - native-country\n",
      "  - salary\n",
      "\n",
      "DataFrame Name: healthcare_data\n",
      "Columns:\n",
      "  - Unnamed: 0\n",
      "  - start_date\n",
      "  - end_date\n",
      "  - drug_type\n",
      "  - drug_name\n",
      "  - drug_name_poe\n",
      "  - drug_name_generic\n",
      "  - formulary_code\n",
      "  - gsn\n",
      "  - ndc\n",
      "  - product_strength\n",
      "  - dose_value\n",
      "  - dose_unit\n",
      "  - form_value_dispensed\n",
      "  - form_unit_dispensed\n",
      "  - route\n",
      "  - row_id_2\n",
      "  - hospital_admission_id_2\n",
      "  - admit_time\n",
      "  - discharge_time\n",
      "  - admission_type\n",
      "  - admission_location\n",
      "  - discharge_location\n",
      "  - insurance\n",
      "  - language\n",
      "  - religion\n",
      "  - marital_status\n",
      "  - ethnicity\n",
      "  - ed_registration_time\n",
      "  - ed_exit_time\n",
      "  - diagnosis\n",
      "  - expired_in_hospital\n",
      "  - has_chartevents_data\n",
      "\n",
      "DataFrame Name: ecommerce_data\n",
      "Columns:\n",
      "  - item_id\n",
      "  - status\n",
      "  - created_at\n",
      "  - sku\n",
      "  - price\n",
      "  - qty_ordered\n",
      "  - grand_total\n",
      "  - increment_id\n",
      "  - category_name_1\n",
      "  - sales_commission_code\n",
      "  - discount_amount\n",
      "  - payment_method\n",
      "  - Working Date\n",
      "  - BI Status\n",
      "  -  MV \n",
      "  - Year\n",
      "  - Month\n",
      "  - Customer Since\n",
      "  - M-Y\n",
      "  - FY\n",
      "  - Customer ID\n",
      "  - Unnamed: 21\n",
      "  - Unnamed: 22\n",
      "  - Unnamed: 23\n",
      "  - Unnamed: 24\n",
      "  - Unnamed: 25\n",
      "\n",
      "DataFrame Name: finance_data\n",
      "Columns:\n",
      "  - Customer_ID\n",
      "  - Age\n",
      "  - Location\n",
      "  - Income_Level\n",
      "  - Total_Transactions\n",
      "  - Avg_Transaction_Value\n",
      "  - Max_Transaction_Value\n",
      "  - Min_Transaction_Value\n",
      "  - Total_Spent\n",
      "  - Active_Days\n",
      "  - Last_Transaction_Days_Ago\n",
      "  - Loyalty_Points_Earned\n",
      "  - Referral_Count\n",
      "  - Cashback_Received\n",
      "  - App_Usage_Frequency\n",
      "  - Preferred_Payment_Method\n",
      "  - Support_Tickets_Raised\n",
      "  - Issue_Resolution_Time\n",
      "  - Customer_Satisfaction_Score\n",
      "  - LTV\n"
     ]
    }
   ],
   "source": [
    "for name, df in dataframes_dict.items():\n",
    "    print(f\"\\nDataFrame Name: {name}\")\n",
    "    print(\"Columns:\")\n",
    "    for column in df.columns:\n",
    "        print(f\"  - {column}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - tourism_data\n",
      "\n",
      " - education_data\n",
      "\n",
      " - healthcare_data\n",
      "\n",
      " - ecommerce_data\n",
      "\n",
      " - finance_data\n"
     ]
    }
   ],
   "source": [
    "for name, df in dataframes_dict.items():\n",
    "    print(f\"\\n - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trip ID</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Start date</th>\n",
       "      <th>End date</th>\n",
       "      <th>Duration (days)</th>\n",
       "      <th>Traveler name</th>\n",
       "      <th>Traveler age</th>\n",
       "      <th>Traveler gender</th>\n",
       "      <th>Traveler nationality</th>\n",
       "      <th>Accommodation type</th>\n",
       "      <th>Accommodation cost</th>\n",
       "      <th>Transportation type</th>\n",
       "      <th>Transportation cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>London, UK</td>\n",
       "      <td>5/1/2023</td>\n",
       "      <td>5/8/2023</td>\n",
       "      <td>7.0</td>\n",
       "      <td>John Smith</td>\n",
       "      <td>35.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>American</td>\n",
       "      <td>Hotel</td>\n",
       "      <td>1200</td>\n",
       "      <td>Flight</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Phuket, Thailand</td>\n",
       "      <td>6/15/2023</td>\n",
       "      <td>6/20/2023</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Jane Doe</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Canadian</td>\n",
       "      <td>Resort</td>\n",
       "      <td>800</td>\n",
       "      <td>Flight</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Bali, Indonesia</td>\n",
       "      <td>7/1/2023</td>\n",
       "      <td>7/8/2023</td>\n",
       "      <td>7.0</td>\n",
       "      <td>David Lee</td>\n",
       "      <td>45.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Korean</td>\n",
       "      <td>Villa</td>\n",
       "      <td>1000</td>\n",
       "      <td>Flight</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>New York, USA</td>\n",
       "      <td>8/15/2023</td>\n",
       "      <td>8/29/2023</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Sarah Johnson</td>\n",
       "      <td>29.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>British</td>\n",
       "      <td>Hotel</td>\n",
       "      <td>2000</td>\n",
       "      <td>Flight</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Tokyo, Japan</td>\n",
       "      <td>9/10/2023</td>\n",
       "      <td>9/17/2023</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Kim Nguyen</td>\n",
       "      <td>26.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>Airbnb</td>\n",
       "      <td>700</td>\n",
       "      <td>Train</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Trip ID       Destination Start date   End date  Duration (days)  \\\n",
       "0        1        London, UK   5/1/2023   5/8/2023              7.0   \n",
       "1        2  Phuket, Thailand  6/15/2023  6/20/2023              5.0   \n",
       "2        3   Bali, Indonesia   7/1/2023   7/8/2023              7.0   \n",
       "3        4     New York, USA  8/15/2023  8/29/2023             14.0   \n",
       "4        5      Tokyo, Japan  9/10/2023  9/17/2023              7.0   \n",
       "\n",
       "   Traveler name  Traveler age Traveler gender Traveler nationality  \\\n",
       "0     John Smith          35.0            Male             American   \n",
       "1       Jane Doe          28.0          Female             Canadian   \n",
       "2      David Lee          45.0            Male               Korean   \n",
       "3  Sarah Johnson          29.0          Female              British   \n",
       "4     Kim Nguyen          26.0          Female           Vietnamese   \n",
       "\n",
       "  Accommodation type Accommodation cost Transportation type  \\\n",
       "0              Hotel               1200              Flight   \n",
       "1             Resort                800              Flight   \n",
       "2              Villa               1000              Flight   \n",
       "3              Hotel               2000              Flight   \n",
       "4             Airbnb                700               Train   \n",
       "\n",
       "  Transportation cost  \n",
       "0                 600  \n",
       "1                 500  \n",
       "2                 700  \n",
       "3                1000  \n",
       "4                 200  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tourism_data = dataframes_dict['tourism_data']\n",
    "tourism_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column_count 139\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trip ID</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Start date</th>\n",
       "      <th>End date</th>\n",
       "      <th>Duration (days)</th>\n",
       "      <th>Traveler name</th>\n",
       "      <th>Traveler age</th>\n",
       "      <th>Traveler gender</th>\n",
       "      <th>Traveler nationality</th>\n",
       "      <th>Accommodation type</th>\n",
       "      <th>Accommodation cost</th>\n",
       "      <th>Transportation type</th>\n",
       "      <th>Transportation cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>83</td>\n",
       "      <td>Rome, Italy</td>\n",
       "      <td>4/15/2025</td>\n",
       "      <td>4/22/2025</td>\n",
       "      <td>7.0</td>\n",
       "      <td>James Kim</td>\n",
       "      <td>41.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>American</td>\n",
       "      <td>Hotel</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Trip ID  Destination Start date   End date  Duration (days)  \\\n",
       "71        72          NaN        NaN        NaN              NaN   \n",
       "82        83  Rome, Italy  4/15/2025  4/22/2025              7.0   \n",
       "127      128          NaN        NaN        NaN              NaN   \n",
       "\n",
       "    Traveler name  Traveler age Traveler gender Traveler nationality  \\\n",
       "71            NaN           NaN             NaN                  NaN   \n",
       "82      James Kim          41.0            Male             American   \n",
       "127           NaN           NaN             NaN                  NaN   \n",
       "\n",
       "    Accommodation type Accommodation cost Transportation type  \\\n",
       "71                 NaN                NaN                 NaN   \n",
       "82               Hotel                100                 NaN   \n",
       "127                NaN                NaN                 NaN   \n",
       "\n",
       "    Transportation cost  \n",
       "71                  NaN  \n",
       "82                  NaN  \n",
       "127                 NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_count = tourism_data.shape[0]\n",
    "print(\"column_count\",row_count)\n",
    "nan_rows = tourism_data[tourism_data.isna().any(axis=1)]\n",
    "nan_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourism_data.dropna(inplace=True)\n",
    "education_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education-num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "       marital-status         occupation   relationship   race     sex  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country salary  \n",
       "0          2174             0              40  United-States  <=50K  \n",
       "1             0             0              13  United-States  <=50K  \n",
       "2             0             0              40  United-States  <=50K  \n",
       "3             0             0              40  United-States  <=50K  \n",
       "4             0             0              40           Cuba  <=50K  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "education_data = dataframes_dict['education_data']\n",
    "education_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_data.drop([\"fnlwgt\",\"education-num\",\"workclass\",\"relationship\",\"sex\",\"capital-gain\",\"capital-loss\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column_count 32561\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>race</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [age, education, marital-status, occupation, race, hours-per-week, native-country, salary]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_count = education_data.shape[0]\n",
    "print(\"column_count\",row_count)\n",
    "nan_rows = education_data[education_data.isna().any(axis=1)]\n",
    "nan_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>race</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>White</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>White</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>White</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>11th</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Black</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Black</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>27</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>White</td>\n",
       "      <td>38</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>40</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>White</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>58</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>White</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>22</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>White</td>\n",
       "      <td>20</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>52</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>White</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age   education      marital-status         occupation   race  \\\n",
       "0       39   Bachelors       Never-married       Adm-clerical  White   \n",
       "1       50   Bachelors  Married-civ-spouse    Exec-managerial  White   \n",
       "2       38     HS-grad            Divorced  Handlers-cleaners  White   \n",
       "3       53        11th  Married-civ-spouse  Handlers-cleaners  Black   \n",
       "4       28   Bachelors  Married-civ-spouse     Prof-specialty  Black   \n",
       "...    ...         ...                 ...                ...    ...   \n",
       "32556   27  Assoc-acdm  Married-civ-spouse       Tech-support  White   \n",
       "32557   40     HS-grad  Married-civ-spouse  Machine-op-inspct  White   \n",
       "32558   58     HS-grad             Widowed       Adm-clerical  White   \n",
       "32559   22     HS-grad       Never-married       Adm-clerical  White   \n",
       "32560   52     HS-grad  Married-civ-spouse    Exec-managerial  White   \n",
       "\n",
       "       hours-per-week native-country salary  \n",
       "0                  40  United-States  <=50K  \n",
       "1                  13  United-States  <=50K  \n",
       "2                  40  United-States  <=50K  \n",
       "3                  40  United-States  <=50K  \n",
       "4                  40           Cuba  <=50K  \n",
       "...               ...            ...    ...  \n",
       "32556              38  United-States  <=50K  \n",
       "32557              40  United-States   >50K  \n",
       "32558              40  United-States  <=50K  \n",
       "32559              20  United-States  <=50K  \n",
       "32560              40  United-States   >50K  \n",
       "\n",
       "[32561 rows x 8 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "education_data.dropna(inplace=True)\n",
    "education_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>race</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>White</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>White</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>White</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>11th</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Black</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Black</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>32557</td>\n",
       "      <td>27</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>White</td>\n",
       "      <td>38</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>32558</td>\n",
       "      <td>40</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>White</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>32559</td>\n",
       "      <td>58</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>White</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>32560</td>\n",
       "      <td>22</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>White</td>\n",
       "      <td>20</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>32561</td>\n",
       "      <td>52</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>White</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index  age   education      marital-status         occupation   race  \\\n",
       "0          1   39   Bachelors       Never-married       Adm-clerical  White   \n",
       "1          2   50   Bachelors  Married-civ-spouse    Exec-managerial  White   \n",
       "2          3   38     HS-grad            Divorced  Handlers-cleaners  White   \n",
       "3          4   53        11th  Married-civ-spouse  Handlers-cleaners  Black   \n",
       "4          5   28   Bachelors  Married-civ-spouse     Prof-specialty  Black   \n",
       "...      ...  ...         ...                 ...                ...    ...   \n",
       "32556  32557   27  Assoc-acdm  Married-civ-spouse       Tech-support  White   \n",
       "32557  32558   40     HS-grad  Married-civ-spouse  Machine-op-inspct  White   \n",
       "32558  32559   58     HS-grad             Widowed       Adm-clerical  White   \n",
       "32559  32560   22     HS-grad       Never-married       Adm-clerical  White   \n",
       "32560  32561   52     HS-grad  Married-civ-spouse    Exec-managerial  White   \n",
       "\n",
       "       hours-per-week native-country salary  \n",
       "0                  40  United-States  <=50K  \n",
       "1                  13  United-States  <=50K  \n",
       "2                  40  United-States  <=50K  \n",
       "3                  40  United-States  <=50K  \n",
       "4                  40           Cuba  <=50K  \n",
       "...               ...            ...    ...  \n",
       "32556              38  United-States  <=50K  \n",
       "32557              40  United-States   >50K  \n",
       "32558              40  United-States  <=50K  \n",
       "32559              20  United-States  <=50K  \n",
       "32560              40  United-States   >50K  \n",
       "\n",
       "[32561 rows x 9 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "education_data.insert(0, 'index', range(1, len(education_data) + 1))\n",
    "education_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_to_sentence(row,primary_key=\"\"):\n",
    "    # Start the sentence with an introductory phrase\n",
    "    sentence = \"\"\n",
    "    \n",
    "    # Dynamically iterate over all columns\n",
    "    for col in row.index:\n",
    "        if col!=primary_key:\n",
    "            value = str(row[col]).strip()  # Ensure value is a string and remove leading/trailing spaces\n",
    "            if value.lower() != \"nan\":  # Skip NaN values\n",
    "                col = col.lower()\n",
    "                value = value.strip(\"'\")  # Remove surrounding single quotes if present\n",
    "                value = value.lower()\n",
    "                sentence += f\"{col} is {value}, \"\n",
    "    \n",
    "    # Remove the trailing comma and space, then end with a period\n",
    "    sentence = sentence.rstrip(\", \") + \".\"\n",
    "    return sentence\n",
    "\n",
    "# Apply the function to each row and create a new column for sentences\n",
    "tourism_data['sentence'] = tourism_data.apply(lambda row: row_to_sentence(row, primary_key=\"Trip ID\"), axis=1)\n",
    "\n",
    "# # Display the resulting DataFrame with the sentences\n",
    "print(tourism_data[['sentence']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourism_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourism_data.to_csv(\"sources/tourism/data_sentence.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Run from here*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, AutoConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "        self.model = LlamaForCausalLM.from_pretrained(\n",
    "            model_name,# config = config, \n",
    "            torch_dtype=torch.float16,\n",
    "            device_map='auto',\n",
    "        ).to(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer.eos_token = self.tokenizer.pad_token  # Set PAD token to EOS\n",
    "        \n",
    "    def predict(self,prompt, user_prompt=\"\"):\n",
    "        model,tokenizer = self.model, self.tokenizer\n",
    "        temp = random.random()\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "        # print(inputs)\n",
    "        generate_ids = model.generate(**inputs, max_new_tokens=4096, do_sample=True, temperature=temp,pad_token_id=tokenizer.eos_token_id) # Disable sampling for deterministic output\n",
    "        generate_ids = generate_ids[0][len(inputs[\"input_ids\"][0]):-1]\n",
    "        infer_res = tokenizer.decode(generate_ids)\n",
    "        return infer_res\n",
    "        \n",
    "    def enhance_sentence_with_llama(self,sentence):\n",
    "        model = self.model\n",
    "        # Construct the prompt\n",
    "        system_prompt = \"You are a creative AI that rephrases given sentences into engaging, conversational stories while incorporating all provided datapoints. Ensure that no information is omitted or added, and skip any datapoints labeled as 'nan'. Do not rephrase the object of a sentence. For example, if the sentence is 'start date is 9/22/2023', do not change the date to a different format. Respond only with the rephrased sentence without any additional commentary.\"\n",
    "        user_prompt = f\"\"\"\n",
    "    Rephrase the following sentence into a conversational story, ensuring all datapoints are included while skipping 'nan' values. Do not introduce any extra or false details.\n",
    "    \n",
    "    Original sentence: {sentence}\n",
    "    \n",
    "    Creative sentence:\"\"\"\n",
    "        creative_sentence = self.predict(system_prompt,user_prompt)\n",
    "        creative_sentence = creative_sentence.replace(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\", \"\")\n",
    "        \n",
    "        # Extract only the generated part\n",
    "        # creative_sentence = response.split(\"Creative sentence:\")[-1].strip()\n",
    "        return creative_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.01s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It was October 21st, 2157, and in room 348, a patient with the subject ID '23' lay in bed, connected to an IV drip of Sodium Chloride 0.9% Flush, a critical medication that is also known as Sodium Chloride 0.9% Flush. This medication, classified as a main drug, came in a syringe form with a strength of Syringe. The doctor prescribed a dose of 3 milliliters to be administered via the IV route. The medication's formula value was 0.6 Syringe, but its formula unit was unspecified. The patient had been admitted to the hospital as an emergency on October 18th, 2157, at 7:34 PM and was discharged on October 25th, 2157, at 2:00 PM. The patient's discharge location was their home for home health care.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "model.enhance_sentence_with_llama(\"Row ID 348: ROW_ID_x is '1136896.0', SUBJECT_ID is '23', HADM_ID_x is '124321.0', ICUSTAY_ID is '234044.0', STARTDATE is '2157-10-21 00:00:00', ENDDATE is '2157-10-25 00:00:00', DRUG_TYPE is 'MAIN', DRUG is 'Sodium Chloride 0.9%  Flush', DRUG_NAME_POE is 'Sodium Chloride 0.9%  Flush', DRUG_NAME_GENERIC is 'Sodium Chloride 0.9%  Flush', FORMULARY_DRUG_CD is 'NACLFLUSH', GSN is 'nan', NDC is '0.0', PROD_STRENGTH is 'Syringe', DOSE_VAL_RX is '3', DOSE_UNIT_RX is 'mL', FORM_VAL_DISP is '0.6', FORM_UNIT_DISP is 'SYR', ROUTE is 'IV', ROW_ID_y is '23.0', HADM_ID_y is '124321.0', ADMITTIME is '2157-10-18 19:34:00', DISCHTIME is '2157-10-25 14:00:00', DEATHTIME is 'nan', ADMISSION_TYPE is 'EMERGENCY', ADMISSION_LOCATION is 'TRANSFER FROM HOSP/EXTRAM', DISCHARGE_LOCATION is 'HOME HEALTH CARE', INSURANCE is 'Medicare', LANGUAGE is 'ENGL', RELIGION is 'CATHOLIC', MARITAL_STATUS is 'MARRIED', ETHNICITY is 'WHITE', EDREGTIME is 'nan', EDOUTTIME is 'nan', DIAGNOSIS is 'BRAIN MASS', HOSPITAL_EXPIRE_FLAG is '0.0', HAS_CHARTEVENTS_DATA is '1.0'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the checkpoint function\n",
    "def process_with_checkpoint(model,df, checkpoint_file, start_index=0, batch_size=10):\n",
    "    # Load existing checkpoint if it exists\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        df = pd.read_csv(checkpoint_file)\n",
    "        print(\"Loaded existing checkpoint.\")\n",
    "        if 'creative_sentence' not in df.columns:\n",
    "            df['creative_sentence'] = None\n",
    "        \n",
    "        # df = df.head(100)\n",
    "    \n",
    "    try:\n",
    "        # Process the dataframe in batches\n",
    "        for i in range(start_index, len(df), batch_size):\n",
    "            # Process a batch of rows\n",
    "            batch = df.iloc[i:i + batch_size]\n",
    "            \n",
    "            for idx, row in batch.iterrows():\n",
    "                if pd.isna(row['creative_sentence']):  # Only process rows not yet completed\n",
    "                    df.at[idx, 'creative_sentence'] = model.enhance_sentence_with_llama(row['sentence'])\n",
    "            \n",
    "            # Save progress after processing each batch\n",
    "            df.to_csv(checkpoint_file, index=False)\n",
    "            print(f\"Checkpoint saved at row {i + batch_size}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        # Save the checkpoint if an error occurs\n",
    "        df.to_csv(checkpoint_file, index=False)\n",
    "        print(\"Checkpoint saved after error.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file back into a DataFrame\n",
    "checkpoint_path = \"sources/tourism/data_sentence.csv\"\n",
    "tourism_df = pd.read_csv(checkpoint_path)    \n",
    "# Process the dataframe with checkpointing\n",
    "tourism_df = process_with_checkpoint(model,tourism_df, checkpoint_file=checkpoint_path, batch_size=20)\n",
    "# Save the final result\n",
    "tourism_df.to_csv(checkpoint_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourism_df = pd.read_csv(checkpoint_path)\n",
    "none_count = tourism_df['creative_sentence'].isnull().sum()\n",
    "total = len(tourism_df)\n",
    "creative_sentence_count = total - none_count\n",
    "print(none_count)\n",
    "print(creative_sentence_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourism_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortened_df = tourism_df#.head(100)\n",
    "shortened_df['creative_sentence'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust display settings for full sentence visibility\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Display the full sentences in the 'creative_sentence' column\n",
    "shortened_df['creative_sentence'].head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def dataframe_to_json(shortened_df, dataset_path, primary_key):\n",
    "    json_data = []\n",
    "    shortened_df = shortened_df.drop(columns=['sentence'])\n",
    "    \n",
    "    for _, row in shortened_df.iterrows():\n",
    "        # Extract the primary key value if it's valid\n",
    "        primary_id = str(row[primary_key]).lower() if primary_key in row and pd.notna(row[primary_key]) and str(row[primary_key]).lower() != \"nan\" else None\n",
    "        \n",
    "        # Build key-value pairs, skipping 'creative_sentence' and the primary key\n",
    "        key_value = {\n",
    "            col.lower(): (primary_id, str(row[col]).lower()) \n",
    "            for col in shortened_df.columns \n",
    "            if col not in [\"creative_sentence\", primary_key] and pd.notna(row[col]) and str(row[col]).lower() != \"nan\"\n",
    "        }\n",
    "\n",
    "        entities = list(key_value.keys())\n",
    "\n",
    "        json_data.append({\n",
    "            \"text\": row[\"creative_sentence\"],\n",
    "            \"entities\": entities,\n",
    "            \"key_value\": key_value\n",
    "        })\n",
    "\n",
    "    with open(f\"{dataset_path}.json\", 'w') as json_file:\n",
    "        json.dump(json_data, json_file, indent=4)\n",
    "    \n",
    "    print(\"JSON file saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def combine_jsons(dataset_path):\n",
    "    # Load the JSON file\n",
    "    with open(f\"{dataset_path}.json\", \"r\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "    \n",
    "    # Combine every 5 entries\n",
    "    combined_data = []\n",
    "    batch_size = 5  # Number of entries to combine\n",
    "    \n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i + batch_size]  # Take a batch of 5\n",
    "    \n",
    "        # Merge text fields\n",
    "        combined_text = \"\\n\".join(str(entry[\"text\"]) for entry in batch)\n",
    "    \n",
    "        # Merge unique entities\n",
    "        combined_entities = list(set(entity for entry in batch for entity in entry[\"entities\"]))\n",
    "    \n",
    "        # Merge key-value pairs, keeping all values in a list\n",
    "        combined_key_value = {}\n",
    "    \n",
    "        for entry in batch:\n",
    "            for key, value in entry[\"key_value\"].items():\n",
    "                if key in combined_key_value:\n",
    "                    if value not in combined_key_value[key]:  # Avoid duplicate values\n",
    "                        combined_key_value[key].append(value)\n",
    "                else:\n",
    "                    combined_key_value[key] = [value]\n",
    "        difficulty = None\n",
    "        domain = None\n",
    "        if \"education\" in dataset_path:\n",
    "            difficulty = \"medium\"\n",
    "            domain = \"education\"\n",
    "        elif \"tourism\" in dataset_path:\n",
    "            difficulty = \"easy\"\n",
    "            domain = \"tourism\"\n",
    "        elif \"ecommerce\" in dataset_path:\n",
    "            difficulty = \"medium\"\n",
    "            domain = \"ecommerce\"\n",
    "        elif \"healthcare\" in dataset_path:\n",
    "            difficulty = \"hard\"\n",
    "            domain = \"healthcare\"\n",
    "        elif \"finance\" in dataset_path:\n",
    "            difficulty = \"medium\"\n",
    "            domain = \"finance\"\n",
    "        # Append combined entry\n",
    "        combined_data.append({\n",
    "            \"text\": combined_text,\n",
    "            \"entities\": combined_entities,\n",
    "            \"key_value\": combined_key_value,\n",
    "            \"difficulty\": difficulty,\n",
    "            \"domain\": domain\n",
    "        })\n",
    "    \n",
    "    # Save the new JSON file\n",
    "    with open(f\"{dataset_path}_combined.json\", \"w\") as json_file:\n",
    "        json.dump(combined_data, json_file, indent=4)\n",
    "    \n",
    "    print(\"Combined JSON file saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the combined JSON file\n",
    "def clean_combined_jsons(dataset_path):\n",
    "    with open(f\"{dataset_path}_combined.json\", \"r\") as json_file:\n",
    "        combined_data = json.load(json_file)\n",
    "    \n",
    "    # Function to clean entities and key-value pairs\n",
    "    def clean_entry(entry):\n",
    "        text = entry[\"text\"].lower()  # Convert text to lowercase for case-insensitive matching\n",
    "    \n",
    "        # Keep only entities that exist in the text\n",
    "        filtered_entities = [entity for entity in entry[\"entities\"] if entity.lower() in text]\n",
    "    \n",
    "        # Initialize filtered key-value store\n",
    "        filtered_key_value = {}\n",
    "    \n",
    "        for key, value_list in entry[\"key_value\"].items():\n",
    "            if isinstance(value_list, list):\n",
    "                valid_pairs = []\n",
    "                \n",
    "                for pair in value_list:\n",
    "                    if isinstance(pair, list) and len(pair) == 2:  # Ensure it's a (trip_id, value) structure\n",
    "                        trip_id, actual_value = pair\n",
    "                        \n",
    "                        # Keep only pairs where the value appears in the text\n",
    "                        if str(actual_value).lower() in text:\n",
    "                            valid_pairs.append([trip_id, actual_value])\n",
    "                \n",
    "                # Only add key if it has valid values\n",
    "                if valid_pairs:\n",
    "                    filtered_key_value[key] = valid_pairs\n",
    "\n",
    "        # Return cleaned entry\n",
    "        return {\n",
    "            \"text\": entry[\"text\"],\n",
    "            \"entities\": filtered_entities,\n",
    "            \"key_value\": filtered_key_value,\n",
    "            \"difficulty\": entry[\"difficulty\"],\n",
    "            \"domain\": entry[\"domain\"]\n",
    "            \n",
    "        }\n",
    "    # Apply the cleaning function to each entry\n",
    "    cleaned_data = [clean_entry(entry) for entry in combined_data]\n",
    "    \n",
    "    # Save the cleaned JSON file\n",
    "    with open(f\"{dataset_path}_combined_cleaned.json\", \"w\") as json_file:\n",
    "        json.dump(cleaned_data, json_file, indent=4)\n",
    "    \n",
    "    print(\"Cleaned JSON file saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del dataframes_dict['tourism_data_sentence']\n",
    "    del dataframes_dict['tourism_data']\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column_count in tourism_data 139\n",
      "nan_rows in tourism_data 3\n",
      "column_count in education_data 32561\n",
      "nan_rows in education_data 0\n",
      "column_count in healthcare_data 1249121\n",
      "nan_rows in healthcare_data 1134378\n",
      "column_count in ecommerce_data 447326\n",
      "nan_rows in ecommerce_data 0\n",
      "column_count in finance_data 7000\n",
      "nan_rows in finance_data 0\n"
     ]
    }
   ],
   "source": [
    "for name, df in dataframes_dict.items():\n",
    "    row_count = df.shape[0]\n",
    "    print(\"column_count in\",name,row_count)\n",
    "    nan_rows = df[df.isna().any(axis=1)]\n",
    "    print(\"nan_rows in\",name,len(nan_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in dataframes_dict.items():\n",
    "    df.dropna(axis=1, thresh=len(df) * 0.5, inplace=True)  # Drops columns with >50% NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column_count in tourism_data 139\n",
      "nan_rows in tourism_data 3\n",
      "column_count in education_data 32561\n",
      "nan_rows in education_data 0\n",
      "column_count in healthcare_data 1249121\n",
      "nan_rows in healthcare_data 532380\n",
      "column_count in ecommerce_data 447326\n",
      "nan_rows in ecommerce_data 0\n",
      "column_count in finance_data 7000\n",
      "nan_rows in finance_data 0\n"
     ]
    }
   ],
   "source": [
    "for name, df in dataframes_dict.items():\n",
    "    row_count = df.shape[0]\n",
    "    print(\"column_count in\",name,row_count)\n",
    "    nan_rows = df[df.isna().any(axis=1)]\n",
    "    print(\"nan_rows in\",name,len(nan_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: sources/education/data_sentence.csv\n"
     ]
    }
   ],
   "source": [
    "education_data['sentence'] = education_data.apply(lambda row: row_to_sentence(row, primary_key=\"index\"), axis=1)\n",
    "file_path = \"sources/education/data_sentence.csv\"\n",
    "# Save DataFrame to CSV\n",
    "education_data.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Saved: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in dataframes_dict.items():\n",
    "    df['sentence'] = df.apply(row_to_sentence, axis=1)\n",
    "    dataframes_dict[name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column_count in tourism_data 139\n",
      "nan_rows in tourism_data 3\n",
      "column_count in education_data 32561\n",
      "nan_rows in education_data 0\n",
      "column_count in healthcare_data 1249121\n",
      "nan_rows in healthcare_data 1134378\n",
      "column_count in ecommerce_data 1048575\n",
      "nan_rows in ecommerce_data 1048575\n",
      "column_count in finance_data 7000\n",
      "nan_rows in finance_data 0\n"
     ]
    }
   ],
   "source": [
    "for name, df in dataframes_dict.items():\n",
    "    row_count = df.shape[0]\n",
    "    print(\"column_count in\",name,row_count)\n",
    "    nan_rows = df[df.isna().any(axis=1)]\n",
    "    print(\"nan_rows in\",name,len(nan_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directory (modify as needed)\n",
    "base_dir = \"sources\"\n",
    "\n",
    "# Loop through dictionary and save each DataFrame as a CSV file\n",
    "for name, df in dataframes_dict.items():\n",
    "    # Construct the folder path dynamically based on the dataset name\n",
    "    file_path = f\"{base_dir}/{name.split('_')[0]}/data_sentence.csv\"\n",
    "\n",
    "    # Save DataFrame to CSV\n",
    "    df.to_csv(file_path, index=False)\n",
    "    \n",
    "    print(f\"Saved: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directory\n",
    "base_dir = \"sources\"\n",
    "\n",
    "# Sort the dictionary by DataFrame length in ascending order\n",
    "names = ['education'] #'ecommerce','education'\n",
    "\n",
    "# Loop through each DataFrame in sorted order\n",
    "for name in names:\n",
    "    # Construct the checkpoint file path dynamically\n",
    "    checkpoint_path = f\"{base_dir}/{name}/data_sentence.csv\"\n",
    "\n",
    "    print(f\"Processing: {checkpoint_path}\")\n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(checkpoint_path)\n",
    "\n",
    "    # Process the DataFrame with checkpointing\n",
    "    df = process_with_checkpoint(model, df, checkpoint_file=checkpoint_path, batch_size=20)\n",
    "\n",
    "    # Save the final result back to the same CSV file\n",
    "    df.to_csv(checkpoint_path, index=False)\n",
    "\n",
    "    print(f\"Saved: {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: sources/education/data_sentence\n",
      "Combined JSON file saved successfully!\n",
      "Cleaned JSON file saved successfully!\n",
      "Processing: sources/ecommerce/data_sentence\n",
      "Combined JSON file saved successfully!\n",
      "Cleaned JSON file saved successfully!\n",
      "Processing: sources/healthcare/data_sentence\n",
      "Combined JSON file saved successfully!\n",
      "Cleaned JSON file saved successfully!\n",
      "Processing: sources/finance/data_sentence\n",
      "Combined JSON file saved successfully!\n",
      "Cleaned JSON file saved successfully!\n",
      "Processing: sources/tourism/data_sentence\n",
      "Combined JSON file saved successfully!\n",
      "Cleaned JSON file saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "base_dir = \"sources\"\n",
    "\n",
    "# Sort the dictionary by DataFrame length in ascending order\n",
    "names = ['education','ecommerce','healthcare','finance','tourism'] #\n",
    "\n",
    "# Loop through each DataFrame in sorted order\n",
    "for name in names:\n",
    "    # Construct the checkpoint file path dynamically\n",
    "    checkpoint_path = f\"{base_dir}/{name}/data_sentence\"\n",
    "    if name == 'finance':\n",
    "        primary_id = 'Customer_ID'\n",
    "    elif name == 'tourism':\n",
    "        primary_id = 'Trip ID'\n",
    "    elif name == 'healthcare':\n",
    "        primary_id = 'Index'\n",
    "    elif name == 'ecommerce':\n",
    "        primary_id = 'index'\n",
    "    elif name == 'education':\n",
    "        primary_id = 'index'\n",
    "\n",
    "    print(f\"Processing: {checkpoint_path}\")\n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    # df = pd.read_csv(checkpoint_path)\n",
    "    \n",
    "    # dataframe_to_json(df,checkpoint_path,primary_id)\n",
    "    combine_jsons(checkpoint_path)\n",
    "    clean_combined_jsons(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_datasets.ipynb\t\t      generate_mimic_dataset.ipynb\n",
      "generate_mimic_cleaned_dataset.ipynb  sources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 files. Cleaning and merging...\n",
      "File: tourism/data_sentence_combined_cleaned.json contains 28 valid entries.\n",
      "File: education/data_sentence_combined_cleaned.json contains 24 valid entries.\n",
      "File: healthcare/data_sentence_combined_cleaned.json contains 52 valid entries.\n",
      "File: ecommerce/data_sentence_combined_cleaned.json contains 33 valid entries.\n",
      "File: finance/data_sentence_combined_cleaned.json contains 36 valid entries.\n",
      "Successfully merged 173 entries into merged_dataset.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "def find_json_files(root_folder, target_filename):\n",
    "    \"\"\"Recursively find all files named target_filename in root_folder.\"\"\"\n",
    "    json_files = []\n",
    "    \n",
    "    for root, _, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            if file == target_filename:\n",
    "                file_path = os.path.join(root, file)\n",
    "                json_files.append(file_path)\n",
    "\n",
    "    return json_files\n",
    "\n",
    "def clean_data(data):\n",
    "    \"\"\"Remove entries with empty 'text', 'entities', and 'key_value', and clean empty keys in 'key_value'.\"\"\"\n",
    "    cleaned = []\n",
    "    for entry in data:\n",
    "        if (\n",
    "            entry.get(\"entities\") == [] and\n",
    "            entry.get(\"key_value\") == {}\n",
    "        ):\n",
    "            continue  # Skip unwanted entry\n",
    "\n",
    "        if \"key_value\" in entry:\n",
    "            # Remove empty key_value entries\n",
    "            entry[\"key_value\"] = {k: v for k, v in entry[\"key_value\"].items() if v}\n",
    "\n",
    "        cleaned.append(entry)\n",
    "    return cleaned\n",
    "\n",
    "def merge_and_shuffle_json_files(json_files, root_folder):\n",
    "    \"\"\"Merge and shuffle all lists of dictionaries from found JSON files after cleaning.\"\"\"\n",
    "    combined_data = []\n",
    "    \n",
    "    for file in json_files:\n",
    "        relative_path = os.path.relpath(file, root_folder)\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    data = clean_data(data)\n",
    "                    print(f\"File: {relative_path} contains {len(data)} valid entries.\")\n",
    "                    combined_data.extend(data)\n",
    "                else:\n",
    "                    print(f\"Warning: {relative_path} does not contain a list.\")\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error: Could not decode {relative_path}\")\n",
    "\n",
    "    random.shuffle(combined_data)\n",
    "    return combined_data\n",
    "\n",
    "# Settings\n",
    "root_folder = \"sources\"\n",
    "target_filename = \"data_sentence_combined_cleaned.json\"\n",
    "\n",
    "# Run\n",
    "json_files = find_json_files(root_folder, target_filename)\n",
    "\n",
    "if not json_files:\n",
    "    print(\"No matching JSON files found.\")\n",
    "\n",
    "print(f\"Found {len(json_files)} files. Cleaning and merging...\")\n",
    "\n",
    "combined_data = merge_and_shuffle_json_files(json_files, root_folder)\n",
    "\n",
    "output_file = \"merged_dataset.json\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(combined_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Successfully merged {len(combined_data)} entries into {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned 6489 entries from: sources/education/data_sentence_combined_cleaned.json\n",
      "Cleaned 249773 entries from: sources/healthcare/data_sentence_combined_cleaned.json\n",
      "Cleaned 89433 entries from: sources/ecommerce/data_sentence_combined_cleaned.json\n",
      "Cleaned 1364 entries from: sources/finance/data_sentence_combined_cleaned.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def remove_empty_key_value_entries(root_dir):\n",
    "    \"\"\"Remove entries with empty 'key_value' dict from all JSON files in subdirectories.\"\"\"\n",
    "    for root, _, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        data = json.load(f)\n",
    "\n",
    "                    if isinstance(data, list):\n",
    "                        cleaned_data = [\n",
    "                            entry for entry in data\n",
    "                            if not (isinstance(entry, dict) and entry.get(\"key_value\") == {})\n",
    "                        ]\n",
    "\n",
    "                        if len(cleaned_data) != len(data):\n",
    "                            print(f\"Cleaned {len(data) - len(cleaned_data)} entries from: {file_path}\")\n",
    "\n",
    "                        # Save back the cleaned data\n",
    "                        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                            json.dump(cleaned_data, f, indent=4, ensure_ascii=False)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Change to your root directory\n",
    "root_directory = \"sources\"\n",
    "remove_empty_key_value_entries(root_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (agpr)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
